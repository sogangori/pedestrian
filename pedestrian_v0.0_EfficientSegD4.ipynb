{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## add segmentation from Aeroscapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_datasets as tfds\n",
    "import pandas as pd\n",
    "\n",
    "from PIL import Image\n",
    "from PIL import ImageFilter\n",
    "import ast\n",
    "import json\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://towardsdatascience.com/building-a-resnet-in-keras-e8f1322a49ba\n",
    "from tensorflow import Tensor\n",
    "from tensorflow.keras.layers import Input, Conv2D, ReLU, BatchNormalization,\\\n",
    "                                    Add, AveragePooling2D, Flatten, Dense, MaxPool2D\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/tensorflow/addons/blob/v0.11.2/tensorflow_addons/image/__init__.py\n",
    "from tensorflow_addons.image.color_ops import sharpness\n",
    "from tensorflow_addons.image.filters import gaussian_filter2d\n",
    "from tensorflow_addons.image.dense_image_warp import dense_image_warp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.__version__, tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\" #CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['bg', 'person', 'bicycle', 'lean', 'car']\n",
    "\n",
    "padded_image_shape = (1024, 1920)\n",
    "anchor_k = 6\n",
    "num_classes = 30\n",
    "num_classes_real = len(class_names)\n",
    "max_data_m = 1700#0#0\n",
    "use_zoom_up_data = False\n",
    "use_pedestrian = False\n",
    "use_visdrone = False\n",
    "use_Aeroscapes = True\n",
    "level_start = 4\n",
    "level_end = 7\n",
    "l1 = 1e-8 #1e-9\n",
    "activation = 'swish'#'selu' is not converted to tflite\n",
    "kernel_init = tf.initializers.he_normal()\n",
    "edgecolors = np.random.rand(num_classes, 3) \n",
    "edgecolors = np.minimum(edgecolors+0.1, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_pedestrian = '/home/mvlab/Downloads/dataset/통영/'\n",
    "forder_P_DESTRE = '/home/mvlab/Downloads/dataset/P-DESTRE/'\n",
    "folder_weather = '/home/sogangori/Downloads/dataset/weather/'\n",
    "\n",
    "names = ['fn','cname','id', 'x0', 'y0', 'w', 'h']\n",
    "edgecolors = np.random.rand(num_classes, 3) \n",
    "edgecolors = np.minimum(edgecolors+0.1, 1.0)\n",
    "path_weight = \"weight/pedestrian_efficientDet-D4\"\n",
    "\n",
    "os.path.isdir(path_pedestrian), os.path.isdir(forder_P_DESTRE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths_avi = glob(path_pedestrian+'*.avi')\n",
    "paths_txt = glob(path_pedestrian+'*/*.txt')#42, 43\n",
    "paths_img = glob(path_pedestrian+'*/*')\n",
    "len(paths_avi), len(paths_txt), len(paths_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(paths_txt), paths_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_weather_effect = os.path.isdir(folder_weather)\n",
    "use_weather_effect, folder_weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_images = []\n",
    "if use_weather_effect:\n",
    "    path_weathers = glob(folder_weather + '*.*')\n",
    "    if len(path_weathers)>0:\n",
    "        for path_weather in path_weathers:            \n",
    "            image = Image.open(path_weather) \n",
    "            weather_images.append(image)            \n",
    "            plt.imshow(image)\n",
    "            plt.show()\n",
    "    else:\n",
    "        use_weather_effect = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### video label load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list_df_TongYoung = []\n",
    "\n",
    "for path_txt in paths_txt:\n",
    "    df = pd.read_csv(path_txt, header=None, names=names)    \n",
    "    index_last_sep = path_txt.rindex(os.sep)\n",
    "    path_self_folder = path_txt[:index_last_sep+1]\n",
    "    annot_file_name = path_txt.split(os.sep)[-1]\n",
    "    print(path_self_folder, annot_file_name, df.shape)\n",
    "    for i in range(len(df)):\n",
    "        file_name = df.loc[i, 'fn']\n",
    "        \n",
    "        if '.' not in str(file_name):            \n",
    "            if i==0:\n",
    "                print('file_name', file_name)\n",
    "                \n",
    "            file_name = '0' + str(file_name) + '.jpg'            \n",
    "        \n",
    "        full_path = path_self_folder + file_name        \n",
    "        df.loc[i, 'exist'] = os.path.isfile(full_path)\n",
    "        df.loc[i, 'path'] = full_path        \n",
    "    \n",
    "    exist = df['exist'].astype(int)\n",
    "    print(exist.mean())\n",
    "    df = df[exist > 0].drop(columns='exist')\n",
    "    if len(df) > 0:\n",
    "        list_df_TongYoung.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_TongYoung = pd.concat(list_df_TongYoung, axis=0)\n",
    "df_TongYoung.shape\n",
    "#3566, 4315, 4644, 6052, 7843, 8652, 10312, 11479, 12985, 14859, 15419, 15844, 16383, 18117, 20206, 21863, 82051"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_TongYoung.cname.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_TongYoung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_TongYoung['cname'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_TongYoung['cname'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_img = df_TongYoung\n",
    "df_img.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aeroscapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_aeroscapes = ['bg', 'person', 'bike', 'car', 'drone', 'boat', 'animal', 'obstacle', 'construction', 'vegetation', 'road', 'sky']\n",
    "class_aeroscapes += ['water']\n",
    "seg_k = len(class_aeroscapes)\n",
    "seg_k, class_aeroscapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_aeroscapes = '/home/mvlab/Downloads/dataset/aeroscapes/'\n",
    "folder_aeroscapes_rgb = folder_aeroscapes + 'JPEGImages/'\n",
    "folder_aeroscapes_seg = folder_aeroscapes + 'SegmentationClass/'\n",
    "\n",
    "os.path.isdir(folder_aeroscapes), os.path.isdir(folder_aeroscapes_rgb), os.path.isdir(folder_aeroscapes_seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "paths_aeroscapes_rgb = glob(folder_aeroscapes_rgb + '*.jpg')\n",
    "paths_aeroscapes_seg = glob(folder_aeroscapes_seg + '*.png')\n",
    "\n",
    "paths_aeroscapes_rgb = np.sort(paths_aeroscapes_rgb)\n",
    "paths_aeroscapes_seg = np.sort(paths_aeroscapes_seg)\n",
    "\n",
    "len(paths_aeroscapes_rgb), len(paths_aeroscapes_seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_Tongyeong = '/home/mvlab/Downloads/dataset/통영 segment/'\n",
    "folder_Tongyeong_rgb = folder_Tongyeong + 'JPEGImages/'\n",
    "folder_Tongyeong_seg = folder_Tongyeong + 'SegmentationClass/'\n",
    "\n",
    "os.path.isdir(folder_Tongyeong), os.path.isdir(folder_Tongyeong_rgb), os.path.isdir(folder_Tongyeong_seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths_Tongyeong_rgb = glob(folder_Tongyeong_rgb + '*.*')\n",
    "paths_Tongyeong_seg = glob(folder_Tongyeong_seg + '*.png')\n",
    "\n",
    "paths_Tongyeong_rgb = np.sort(paths_Tongyeong_rgb)\n",
    "paths_Tongyeong_seg = np.sort(paths_Tongyeong_seg)\n",
    "\n",
    "len(paths_Tongyeong_rgb), len(paths_Tongyeong_seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(paths_Tongyeong_rgb), len(paths_Tongyeong_rgb), paths_Tongyeong_rgb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths_aeroscapes_rgb = np.concatenate((paths_Tongyeong_rgb, paths_aeroscapes_rgb))\n",
    "paths_aeroscapes_seg = np.concatenate((paths_Tongyeong_seg, paths_aeroscapes_seg))\n",
    "\n",
    "len(paths_aeroscapes_rgb), len(paths_aeroscapes_seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_img = Image.open(paths_aeroscapes_rgb[0])\n",
    "sample_img#.resize((200, 100), Image.NEAREST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_img.size, padded_image_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_seg = np.array(Image.open(paths_aeroscapes_seg[0]))\n",
    "print('sample_seg', np.min(sample_seg), np.max(sample_seg), np.unique(sample_seg))\n",
    "plt.imshow(sample_seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_rgbs = np.concatenate((np.array(sample_img), np.expand_dims(np.array(sample_seg), -1)), -1)\n",
    "sample_rgbs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "resized_img = np.array(tf.image.resize(sample_rgbs, (500, 500)), np.uint8)\n",
    "print('type', type(resized_img), resized_img.dtype, np.max(resized_img))\n",
    "plt.imshow(resized_img[:, :, :3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_Aeroscapes(folder_aeroscapes_rgb, folder_aeroscapes_seg, stride=1, max_data_m=max_data_m):\n",
    "    input_list, bbox_list, segment_list, path_list = [], [], [], []\n",
    "        \n",
    "    if not os.path.isdir(folder_aeroscapes_rgb):        \n",
    "        print('not exist Aeroscapes folder', folder_aeroscapes_seg)\n",
    "        return \n",
    "            \n",
    "    #paths_aeroscapes_rgb = glob(folder_aeroscapes_rgb + '*.jpg')\n",
    "    #paths_aeroscapes_seg = glob(folder_aeroscapes_seg + '*.png')\n",
    "\n",
    "    #paths_aeroscapes_rgb = np.sort(paths_aeroscapes_rgb)\n",
    "    #paths_aeroscapes_seg = np.sort(paths_aeroscapes_seg)\n",
    "    \n",
    "    print('len images', len(paths_aeroscapes_rgb), len(paths_aeroscapes_seg))\n",
    "    \n",
    "    resize_h, resize_w = padded_image_shape\n",
    "    for i in range(0, len(paths_aeroscapes_rgb), stride):        \n",
    "        img_rgb = Image.open(paths_aeroscapes_rgb[i])\n",
    "        img_seg = Image.open(paths_aeroscapes_seg[i])\n",
    "        \n",
    "        img_rgb_resized = img_rgb.resize((resize_w, resize_h))\n",
    "        img_seg_resized = img_seg.resize((resize_w, resize_h), Image.NEAREST)\n",
    "        dummy_box = np.array([[0, 0, 200, 200, 0, 0, 0, 0]])\n",
    "        #np.stack((x0, y0, x0 + w, y0 + h, cls, zero, zero, zero), axis=1)\n",
    "        input_list.append(np.array(img_rgb_resized))\n",
    "        bbox_list.append(dummy_box)\n",
    "        segment_list.append(np.array(img_seg_resized))\n",
    "        path_list.append(paths_aeroscapes_rgb)\n",
    "        \n",
    "        if len(input_list) % 500==0:\n",
    "            print('len', len(input_list), len(bbox_list), len(segment_list), len(path_list))\n",
    "        if len(input_list) >= max_data_m:\n",
    "            break\n",
    "    \n",
    "    print('len', len(input_list), len(bbox_list), len(segment_list), len(path_list))\n",
    "    return input_list, bbox_list, segment_list, path_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_Aeroscapes:\n",
    "    input_list_aero, bbox_list_aero, segment_list_aero, path_list_aero = load_Aeroscapes(folder_aeroscapes_rgb, folder_aeroscapes_seg, stride=1, max_data_m=max_data_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_Aeroscapes:\n",
    "    plt.imshow(segment_list_aero[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P-DESTRE load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PDESTRE_columns = ['frame', 'ID', 'x', 'y', 'w', 'h', 'head', 'yaw', 'pitch', 'roll',          \n",
    "          'gender', 'age', 'height', 'body volume', 'ethnicity', 'hair color', 'hairstyle', 'beard', 'mustache', 'glasses', \n",
    "           'head accessories', 'upper cloth', 'lower cloth', 'feet', 'accessories', 'action']\n",
    "len(PDESTRE_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glob(forder_P_DESTRE + '*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#glob(forder_P_DESTRE + 'annotation/*.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_dataframe(forder_P_DESTRE):\n",
    "    list_df = []\n",
    "    \n",
    "    list_annotation_path = glob(forder_P_DESTRE + 'annotation/*.txt')\n",
    "    for i in range(len(list_annotation_path)):\n",
    "        path_anno = list_annotation_path[i]\n",
    "        anno_file_name = path_anno.split(os.sep)[-1]\n",
    "        anno_file_name = anno_file_name[:-4]\n",
    "        df = pd.read_csv(path_anno, header=None, names=PDESTRE_columns)\n",
    "        df['video'] = anno_file_name\n",
    "        list_df.append(df)\n",
    "        #print(i, anno_file_name, df.shape)\n",
    "    \n",
    "    df_all = pd.concat(list_df, axis=0)\n",
    "    df_all = df_all.reset_index()\n",
    "    return df_all \n",
    "\n",
    "df_P_DESTRE = get_dataframe(forder_P_DESTRE)    \n",
    "df_P_DESTRE.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_P_DESTRE.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_P_DESTRE.iloc[:, 11:-2].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ax = df_P_DESTRE.iloc[:, 11:-2].hist(figsize=(10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_P_DESTRE['ID'].hist() #2:unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_P_DESTRE['gender'].hist() #2:unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_P_DESTRE['body volume'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_P_DESTRE['action'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_P_DESTRE['upper cloth'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_P_DESTRE['gender_'] = df_P_DESTRE['gender'].max() - df_P_DESTRE['gender']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_P_DESTRE['body volume_'] = df_P_DESTRE['body volume'].max() - df_P_DESTRE['body volume']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_P_DESTRE['upper cloth_'] = df_P_DESTRE['upper cloth'].max() - df_P_DESTRE['upper cloth']\n",
    "df_P_DESTRE['upper cloth'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_k = 2\n",
    "body_volumn_k = 3\n",
    "upper_cloth_k = 12\n",
    "label_attribute_k = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_P_DESTRE['ID'].max(), df_P_DESTRE['ID'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#num_classes_real = df_P_DESTRE['ID'].max() + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str.format('%#05d' % 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = []\n",
    "for fr in df_P_DESTRE['frame'].values:\n",
    "    file_names.append(str.format('%#05d.jpg' % fr))\n",
    "    \n",
    "df_P_DESTRE['file_name'] = file_names\n",
    "df_P_DESTRE['path'] = forder_P_DESTRE + 'videos/' + df_P_DESTRE['video'] + os.sep + df_P_DESTRE['file_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_P_DESTRE.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove not exist file\n",
    "is_exist_file = []\n",
    "for path_pdestre in df_P_DESTRE['path']:\n",
    "    is_exist_file.append(os.path.isfile(path_pdestre))\n",
    "\n",
    "np.mean(is_exist_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_P_DESTRE_exist = df_P_DESTRE[is_exist_file]\n",
    "df_P_DESTRE_exist.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_P_DESTRE_cut = df_P_DESTRE_exist[\n",
    "    ['ID', 'ID', 'x', 'y', 'w', 'h', 'path','gender_','body volume_','upper cloth_']]\n",
    "df_P_DESTRE_cut.columns = [\n",
    "    'cname', 'id', 'x0', 'y0', 'w', 'h', 'path','gender_','body volume_','upper cloth_']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_P_DESTRE_cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond = df_P_DESTRE_cut['id']==-1\n",
    "cond.sum(), len(cond)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_P_DESTRE_cut['id'][cond] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_img.shape, df_P_DESTRE_cut.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_img.columns, df_P_DESTRE_cut.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_img['id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_img.drop(columns='fn')\n",
    "df['id']= 1 #finetune\n",
    "df['gender_'] = 0\n",
    "df['body volume_'] = 0\n",
    "df['upper cloth_'] = 0\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum().sum(), df['path'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['path'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names, class_names.index('person')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def parsing_annotation(df, is_PEDESTRE=False):\n",
    "    annotation = dict()\n",
    "    for i in range(len(df)):\n",
    "      \n",
    "        row = df.iloc[i].values\n",
    "        cname, iden, x0, y0, w, h, path, gender, body_volume, upper_cloth = row\n",
    "        x1 = x0 + w\n",
    "        y1 = y0 + h\n",
    "        if i%10000==0:\n",
    "            print(i, row)\n",
    "            \n",
    "        cls = 1\n",
    "        if cname=='bike':\n",
    "            cname = 'bicycle'\n",
    "        if cname in class_names:            \n",
    "            cls = class_names.index(cname)\n",
    "        \n",
    "        if is_PEDESTRE:\n",
    "            cls = iden # land \n",
    "            cls = iden * 0 + 1\n",
    "            \n",
    "        bbox = np.array([x0, y0, x1, y1, cls, gender, body_volume, upper_cloth]).reshape((1, -1))        \n",
    "\n",
    "        path_image = path#finetune\n",
    "        if not os.path.isfile(path_image):\n",
    "            print('not exist', path_image)\n",
    "            continue\n",
    "            \n",
    "        if path_image in annotation.keys():\n",
    "            pre_bbox = annotation[path_image]\n",
    "            new_bbox = np.concatenate((pre_bbox, bbox), axis=0)\n",
    "            #cls_bbox = np.stack(cls_bbox, 0)#.reshape([-1, 6])\n",
    "            #annotation[path_image].extend(new_bbox)\n",
    "            annotation[path_image] = new_bbox\n",
    "        else:\n",
    "            annotation[path_image] = bbox        \n",
    "    return annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape, df_P_DESTRE_cut.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "annotation = parsing_annotation(df)\n",
    "annotation_PDESTRE = parsing_annotation(df_P_DESTRE_cut, is_PEDESTRE=True)\n",
    "\n",
    "len(annotation), len(annotation_PDESTRE) #basic:1530"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VisDrone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visdrone_categories = ['bg','pedestrian', 'person', 'car', 'van', 'bus', 'truck', 'motor', 'bicycle', 'awning-tricycle', 'tricycle','empty_0','empty_1']\n",
    "path_visDrone = '/home/mvlab/Downloads/dataset/VisDrone2019/VisDrone2019-DET-train/'\n",
    "path_visDrone_annotation = path_visDrone + 'annotations/'\n",
    "path_visDrone_image = path_visDrone + 'images/'\n",
    "os.path.isdir(path_visDrone_annotation), os.path.isdir(path_visDrone_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_visdrone_data(path_visDrone, max_data_m, pedestrian_only=True):    \n",
    "    annotation = dict()\n",
    "    path_visDrone_annotation = path_visDrone + 'annotations/'\n",
    "    path_visDrone_image = path_visDrone + 'images/'\n",
    "    os.path.isdir(path_visDrone_annotation), os.path.isdir(path_visDrone_image)\n",
    "    list_annotation = glob(path_visDrone_annotation+'*.*')\n",
    "    list_image_path = glob(path_visDrone_image+'*.*')\n",
    "    \n",
    "    print('len', len(list_annotation), len(list_image_path), list_annotation[0])\n",
    "    \n",
    "    for i in range(len(list_annotation)):\n",
    "        path_annotation = list_annotation[i]\n",
    "        \n",
    "        df = pd.read_csv(path_annotation, header=None)\n",
    "                \n",
    "        cls = np.array(df.iloc[:, 5])\n",
    "        bbox_xywh = np.array(df.iloc[:, :4])        \n",
    "        \n",
    "        file_name_annotation = path_annotation.split('/')[-1].split('.')[0]\n",
    "        file_name_image = path_visDrone_image+file_name_annotation+'.jpg'\n",
    "        \n",
    "        if not os.path.isfile(file_name_image):\n",
    "            print('not exist', file_name_image)\n",
    "            continue\n",
    "                \n",
    "        if pedestrian_only:            \n",
    "            human_mask = np.logical_and(cls > 0, cls < 3)            \n",
    "            car_mask = cls > 2.5\n",
    "            is_human_contain = np.any(human_mask)\n",
    "            human_count = np.sum(human_mask)\n",
    "            if not is_human_contain or human_count < 20:#30:651\n",
    "                continue\n",
    "            \n",
    "            cls = np.where(human_mask, 1.0, 0.0)\n",
    "            cls = np.where(car_mask, 4.0, cls)#new            \n",
    "            use_mask = np.logical_or(human_mask, car_mask)\n",
    "            cls = cls[use_mask]\n",
    "            #print('cls', cls)\n",
    "            bbox_xywh = bbox_xywh[use_mask]            \n",
    "                      \n",
    "        x0 = bbox_xywh[:, 0]\n",
    "        y0 = bbox_xywh[:, 1]\n",
    "        w = bbox_xywh[:, 2]\n",
    "        h = bbox_xywh[:, 3]        \n",
    "        \n",
    "        #annotation[file_name_image] = np.stack((x0, y0, x0 + w, y0 + h, cls), axis=1)\n",
    "        zero = np.zeros_like(w)\n",
    "        annotation[file_name_image] = np.stack((x0, y0, x0 + w, y0 + h, cls, zero, zero, zero), axis=1)\n",
    "        \n",
    "        if len(file_name_image)>max_data_m:\n",
    "            break\n",
    "        if i%100==0:\n",
    "            print(len(list_annotation), i, file_name_annotation, len(file_name_image))\n",
    "    return annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if use_visdrone:\n",
    "    annotation_visdrone = load_visdrone_data(path_visDrone, max_data_m)\n",
    "    print(len(annotation_visdrone))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(annotation), len(annotation_PDESTRE)#, len(annotation_visdrone)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### image load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_bbox_image(image, boxes):\n",
    "    img_objects = []\n",
    "    image = np.array(image)\n",
    "    for box in boxes:        \n",
    "        box = box.astype(np.int)\n",
    "        x1, y1, x2, y2 = box\n",
    "        w = x2 - x1\n",
    "        h = y2 - y1        \n",
    "        crop_image_arr = image[y1:y2, x1:x2]\n",
    "        ch, cw, cc = crop_image_arr.shape\n",
    "        if ch>1 and cw>1:\n",
    "            img_objects.append(crop_image_arr)\n",
    "        else:\n",
    "            print('crop_bbox_image', x2-x1, y2-y1, 'crop_image_arr.shape', crop_image_arr.shape)\n",
    "        \n",
    "    return img_objects\n",
    "    \n",
    "    \n",
    "def attach_crop_image(image, boxes, max_crop=200):\n",
    "        \n",
    "    crop_bbox_arr = crop_bbox_image(image, np.array(boxes)[:max_crop])\n",
    "    bbox_k = len(crop_bbox_arr)\n",
    "    max_col = 30\n",
    "    \n",
    "    if bbox_k > 0:\n",
    "        img_h, img_w, img_c = image.shape\n",
    "        object_img_w = img_w//bbox_k        \n",
    "        resize_h = img_h // 8\n",
    "        resize_w = img_w // bbox_k  \n",
    "        resize_w = min(max(resize_w, img_w//max_col), img_w//8)\n",
    "        \n",
    "        footer_h = resize_h * (1 + (bbox_k-1)//max_col)\n",
    "        footer = np.zeros((footer_h, img_w, img_c), np.uint8)\n",
    "        \n",
    "        for i in range(min(bbox_k, max_crop)):\n",
    "            crop_arr = crop_bbox_arr[i]\n",
    "            crop_img = Image.fromarray(crop_arr)                \n",
    "            crop_img = crop_img.resize((resize_w, resize_h))\n",
    "            crop_arr_resized = np.array(crop_img)\n",
    "            offset_y = (i//max_col) * resize_h\n",
    "            offset_x = (i%max_col) * resize_w\n",
    "            footer[offset_y:offset_y+resize_h, offset_x:offset_x+resize_w] = crop_arr_resized\n",
    "\n",
    "        seperate_line = np.zeros_like(footer[:2])\n",
    "        image = np.concatenate((image, seperate_line, footer), axis=0)    \n",
    "    return image    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_detections_simple(\n",
    "    image, boxes, classes, figsize=(12, 12), linewidth=1, color=[0, 0, 1]\n",
    "):\n",
    "    \"\"\"Visualize Detections\"\"\"\n",
    "    image = np.array(image, dtype=np.uint8)    \n",
    "    \n",
    "    img_h = image.shape[0]\n",
    "    img_w = image.shape[1]\n",
    "    \n",
    "    if image.ndim > 2:\n",
    "        image = attach_crop_image(image, boxes, max_crop=100)\n",
    "    \n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(image)\n",
    "    ax = plt.gca()\n",
    "    boxes_width = boxes[:, 2] - boxes[:, 0]\n",
    "    boxes_height = boxes[:, 3] - boxes[:, 1]\n",
    "    box_min_width = np.min(boxes_width)\n",
    "    box_max_width = np.max(boxes_width)\n",
    "    box_min_height = np.min(boxes_height)\n",
    "    box_max_height = np.max(boxes_height)\n",
    "    title = str.format('(%dx%d) %d box, width:%d ~ %d, height:%d ~ %d' \n",
    "                       %(img_h, img_w, len(boxes), box_min_width, box_max_width, box_min_height, box_max_height))\n",
    "    plt.title(title)\n",
    "    for box, cls in zip(boxes, classes):\n",
    "        x1, y1, x2, y2 = box        \n",
    "        w, h = x2 - x1, y2 - y1\n",
    "        \n",
    "        color = edgecolors[min(len(edgecolors)-1,int(cls))]\n",
    "        patch = plt.Rectangle(\n",
    "            [x1, y1], w, h, fill=False, edgecolor=color, linewidth=linewidth\n",
    "        )\n",
    "        ax.add_patch(patch)\n",
    "        if len(boxes) < 170:\n",
    "            #score_txt = class_names[int(cls)]\n",
    "            score_txt = str(int(cls))\n",
    "            ax.text(x1, y1, score_txt, bbox={\"facecolor\": [1,1,0], \"alpha\": 0.4}, clip_box=ax.clipbox, clip_on=True,)\n",
    "        \n",
    "    plt.show()\n",
    "    return ax\n",
    "\n",
    "def display_data(X, BBOX, stride=1):\n",
    "    for i in range(len(X)):\n",
    "        if i%stride==0:\n",
    "            img_arr = X[i]\n",
    "            sample_box = BBOX[i]            \n",
    "            bbox = sample_box[:, :4]\n",
    "            label = sample_box[:, 4]\n",
    "            \n",
    "            if img_arr.ndim < 2:\n",
    "                print('img_arr.ndim < 2')\n",
    "                continue\n",
    "            \n",
    "            h = img_arr.shape[0]\n",
    "            w = img_arr.shape[1]\n",
    "            scale = np.array((w, h, w, h))\n",
    "            scale = np.reshape(scale, (1, 4))\n",
    "            bbox_norm = bbox.astype(np.float) * scale.astype(np.float)\n",
    "            #print('bbox_norm', bbox, bbox_norm)\n",
    "            print(i, np.unique(label))\n",
    "            ax = visualize_detections_simple(img_arr,bbox_norm,label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array((3)).ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_xy(annotation, rescale=1, stride=1, size_cut=False):\n",
    "    input_list = []\n",
    "    bbox_list = []\n",
    "    path_list = []\n",
    "    i = 0\n",
    "    #np.random.rand() #미리 만들어놔\n",
    "    for path_image in annotation:\n",
    "        i+=1\n",
    "        if stride!=1 and np.random.randint(1, 1+stride)%stride!=0:\n",
    "            continue\n",
    "            \n",
    "        cls_bbox = annotation[path_image]                \n",
    "        bbox = np.array(cls_bbox[:, :4])\n",
    "        attribute = cls_bbox[:, 4:]\n",
    "\n",
    "        img = Image.open(path_image)    \n",
    "        scale = np.array((img.width, img.height, img.width, img.height))\n",
    "        scale = np.reshape(scale, (1, 4))\n",
    "\n",
    "        if rescale!=1:\n",
    "            img = img.resize((img.width//rescale, img.height//rescale))\n",
    "\n",
    "        img_arr = np.array(img)\n",
    "        try:\n",
    "            std_v = np.std(img_arr)\n",
    "            if std_v < 3:\n",
    "                print('std_v', std_v)\n",
    "                continue\n",
    "        except:\n",
    "            print('error', path_image)\n",
    "            continue\n",
    "        \n",
    "        box_width = bbox[:, 3] - bbox[:, 1]\n",
    "        box_height = bbox[:, 3] - bbox[:, 1]\n",
    "        \n",
    "        if np.min(box_width) < 1 or np.min(box_height) < 1:\n",
    "            print('box_size < 1', box_width, box_height)#check\n",
    "            continue\n",
    "            \n",
    "        bbox_norm = bbox.astype(np.float) / scale.astype(np.float)\n",
    "        cls_bbox_norm = np.concatenate((bbox_norm, attribute), axis=1)\n",
    "\n",
    "        if size_cut:                \n",
    "            box_height = cls_bbox_norm[:, 3] - cls_bbox_norm[:, 1]\n",
    "            box_height_max = np.max(box_height)\n",
    "            if box_height_max < 0.05 or box_height_max > 0.2:\n",
    "                continue\n",
    "\n",
    "        input_list.append(img_arr)\n",
    "        bbox_list.append(cls_bbox_norm)\n",
    "        path_list.append(path_image)\n",
    "        if len(input_list)%100==0:        \n",
    "            print(len(annotation), i, len(input_list))   \n",
    "        if len(input_list) > max_data_m:\n",
    "            break       \n",
    "\n",
    "    print(len(input_list), len(bbox_list))\n",
    "    return input_list, bbox_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(annotation), max_data_m, len(annotation) // max_data_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "stride = max(1, len(annotation) // max_data_m)\n",
    "print(len(annotation), stride)#6258, 7165, 7467, 7956, 9428, 9889, 10194, 10569, 11603, 11750\n",
    "input_list, bbox_list = load_xy(annotation, stride=stride)\n",
    "#error /home/mvlab/Downloads/dataset/통영/도천2/01748.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(input_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbbox = np.concatenate(bbox_list, 0)\n",
    "print(cbbox.shape, np.max(cbbox[:, 4])) # basic:1736 + P-DESTRE > 9757\n",
    "h = plt.hist(cbbox[:, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display_data(input_list, bbox_list, stride=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if use_pedestrian:\n",
    "    input_list_PDESTRE, bbox_list_PDESTRE = load_xy(annotation_PDESTRE, rescale=4, stride=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if use_visdrone:\n",
    "    list_visdrone_x, list_visdrone_y = load_xy(annotation_visdrone, size_cut=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_visdrone:\n",
    "    cbbox = np.concatenate(list_visdrone_y, 0)\n",
    "    print(cbbox.shape, np.max(cbbox[:, 4])) # basic:1736 + P-DESTRE > 9757\n",
    "    h = plt.hist(cbbox[:, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(input_list)#, len(input_list_PDESTRE)#, len(list_visdrone_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "list_visdrone_x, list_visdrone_y = load_visdrone_data(path_visDrone)\n",
    "print(len(list_visdrone_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#max_data_m, np.unique(np.concatenate(list_visdrone_y, 0)[:, -1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "display_data(list_visdrone_x, list_visdrone_y, stride=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbbox = np.concatenate(bbox_list, 0)\n",
    "print(cbbox.shape, np.max(cbbox[:, 4])) # basic:1736 + P-DESTRE > 9757\n",
    "h = plt.hist(cbbox[:, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "h = plt.hist(cbbox[:, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbbox = np.concatenate(bbox_list, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "box_w = cbbox[:, 2] - cbbox[:, 0]\n",
    "box_h = cbbox[:, 3] - cbbox[:, 1]\n",
    "h = plt.hist(box_h, label='h')\n",
    "h = plt.hist(box_w, label='w')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(box_w, box_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbbox.shape, 1/9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(a<2, 10, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tile_object_biggest_center(X, BBOX, scope=0.5, min_cls=1):\n",
    "    \n",
    "    crop_xs = []\n",
    "    crop_bboxs = []\n",
    "    for i in range(len(X)):\n",
    "        x = X[i]\n",
    "        img_h, img_w, img_c = x.shape\n",
    "        bbox = BBOX[i].copy()\n",
    "        #print('len', len(x), len(bbox), x.shape, bbox.shape)\n",
    "        \n",
    "        x0 = bbox[:, 0]\n",
    "        x1 = bbox[:, 2]\n",
    "        cls = bbox[:, 4]\n",
    "        \n",
    "        if not np.any(cls - 0.1 > min_cls):\n",
    "            continue            \n",
    "    \n",
    "        box_size = (bbox[:, 2] - bbox[:, 0]) * (bbox[:, 3] - bbox[:, 1])\n",
    "        box_max_index = np.argmax(box_size)\n",
    "        if np.max(cls) > 1:\n",
    "            box_max_index = np.argmax(cls)\n",
    "            \n",
    "        #print('box_max_index', box_max_index)\n",
    "        box_center = bbox[box_max_index]\n",
    "        #print('box_center', box_center)\n",
    "                    \n",
    "        cx = (box_center[0] + box_center[2])/2        \n",
    "\n",
    "        if cx < scope:\n",
    "            tx0 = np.maximum(0, cx - scope/2)\n",
    "            tx1 = tx0 + scope\n",
    "        elif cx > 1 - scope:\n",
    "            tx1 = np.minimum(1.0, cx + scope/2)\n",
    "            tx0 = tx1 - scope\n",
    "        else:\n",
    "            tx0 = cx - scope/2\n",
    "            tx1 = cx + scope/2\n",
    "        \n",
    "        epsilon = 1e-8 \n",
    "        bbox[:, 0] = np.where(np.logical_and(x0 < tx0, x1 < tx1) , tx0, x0)        \n",
    "        bbox[:, 2] = np.where(np.logical_and(x1 > tx1, x0 > tx0) , tx1, x1)                \n",
    "        cond = (bbox[:, 2] - bbox[:, 0]) > epsilon\n",
    "        \n",
    "        if not np.any(cond):\n",
    "            print('no valid center box', bbox)\n",
    "            continue\n",
    "            \n",
    "        bbox = bbox[cond]\n",
    "        cls = bbox[:, 4]\n",
    "        attributes = bbox[:, 4:]\n",
    "        \n",
    "        x0 = bbox[:, 0]\n",
    "        y0 = bbox[:, 1]\n",
    "        x1 = bbox[:, 2]\n",
    "        y1 = bbox[:, 3]\n",
    "        tbox = np.stack(((x0 - tx0)/scope, y0, (x1 - tx0)/scope, y1), axis=1)            \n",
    "        tbox = np.concatenate((tbox, attributes), -1)\n",
    "\n",
    "        img_x0 = int(tx0 * img_w)\n",
    "        img_x1 = img_x0 + int(img_w*scope)\n",
    "        timg = x[:, img_x0:img_x1]\n",
    "\n",
    "        img = Image.fromarray(timg)\n",
    "        dst_w = int(padded_image_shape[1]*scope)\n",
    "        img_resized = img.resize((dst_w, padded_image_shape[0]))\n",
    "        arr_resized = np.array(img_resized)            \n",
    "\n",
    "        crop_xs.append(arr_resized)\n",
    "        crop_bboxs.append(tbox)\n",
    "        \n",
    "    return crop_xs, crop_bboxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def attach_tiled_data(X, Y, col=2, row = 1):\n",
    "    \n",
    "    m = len(X)\n",
    "    use_m = (m // (row * col)) * (row * col)    \n",
    "    print('use_m', use_m)\n",
    "    X = np.array(X[:use_m])\n",
    "    Y = np.array(Y[:use_m])\n",
    "    \n",
    "    x_split = np.split(X, col, axis=0)\n",
    "    y_split = np.split(Y, col, axis=0)\n",
    "        \n",
    "    print('y_split', X[0].shape,  x_split[0].shape, x_split[1].shape, y_split[0].shape, y_split[1].shape)\n",
    "        \n",
    "    stride_i = len(x_split[0])\n",
    "    scale_x = 1.0 / col\n",
    "    \n",
    "    list_Y = []    \n",
    "    for i in range(stride_i):\n",
    "        list_y = []\n",
    "                \n",
    "        for j in range(col):\n",
    "            y = Y[i + stride_i * j]\n",
    "            y_box = y[:, :4]\n",
    "            y_attr = y[:, 4:]\n",
    "            x0, y0, x1, y1 = np.split(y_box, 4, -1) \n",
    "            \n",
    "            x0 = x0 * scale_x + j * scale_x\n",
    "            x1 = x1 * scale_x + j * scale_x\n",
    "\n",
    "            y_box = np.concatenate((x0, y0, x1, y1), -1)\n",
    "            new_y = np.concatenate((y_box, y_attr), axis=-1)\n",
    "            \n",
    "            list_y.append(new_y)\n",
    "        \n",
    "        list_Y.append(np.concatenate(list_y, axis=0))\n",
    "       \n",
    "    \n",
    "    x_concat = np.concatenate(x_split, axis=2)\n",
    "    \n",
    "    print('x_concat', x_concat.shape, 'type y',type(y_split))\n",
    "        \n",
    "    return list(x_concat), list_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attach_tiled_data_row(X, Y, row=2):\n",
    "    \n",
    "    m = len(X)\n",
    "    use_m = (m // row) * row    \n",
    "    print('use_m', use_m)\n",
    "    X = np.array(X[:use_m])\n",
    "    Y = np.array(Y[:use_m])\n",
    "    \n",
    "    x_split = np.split(X, row, axis=0)\n",
    "    y_split = np.split(Y, row, axis=0)\n",
    "    \n",
    "    print('len()', len(x_split), len(y_split), len(x_split[0]))\n",
    "    print('y_split', x_split[0].shape, x_split[1].shape, y_split[0].shape, y_split[1].shape)\n",
    "        \n",
    "    stride_i = len(x_split[0])\n",
    "    scale_y = 1.0 / row\n",
    "    \n",
    "    list_Y = []    \n",
    "    for i in range(stride_i):\n",
    "        list_y = []\n",
    "                \n",
    "        for j in range(row):\n",
    "            y = Y[i + stride_i * j]\n",
    "            y_box = y[:, :4]\n",
    "            y_attr = y[:, 4:]\n",
    "            x0, y0, x1, y1 = np.split(y_box, 4, -1) \n",
    "            \n",
    "            y0 = y0 * scale_y + j * scale_y\n",
    "            y1 = y1 * scale_y + j * scale_y\n",
    "\n",
    "            y_box = np.concatenate((x0, y0, x1, y1), -1)\n",
    "            new_y = np.concatenate((y_box, y_attr), axis=-1)\n",
    "            \n",
    "            list_y.append(new_y)\n",
    "        \n",
    "        list_Y.append(np.concatenate(list_y, axis=0))\n",
    "       \n",
    "    \n",
    "    x_concat = np.concatenate(x_split, axis=1)\n",
    "    \n",
    "    print('x_concat', x_concat.shape, 'type y',type(y_split))\n",
    "        \n",
    "    return list(x_concat), list_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bg_paths():\n",
    "    list_bg_jpg = glob(folder_water_bg + '*')\n",
    "    list_bg_jpg0 = glob(folder_water_bg[:-1] + '0/*')\n",
    "    print(len(list_bg_jpg), len(list_bg_jpg0))\n",
    "    return list_bg_jpg0[1::2]#+ list_bg_jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def box_convert_cxy(box):\n",
    "    y0, x0, y1, x1 = np.split(box, 4, axis=-1)    \n",
    "    return np.concatenate(( (y0+y1)/2, (x0+x1)/2 ), axis=1)\n",
    "\n",
    "def box_swap_xy(box):\n",
    "    y0, x0, y1, x1 = np.split(box, 4, axis=-1)    \n",
    "    return np.concatenate((x0, y0, x1, y1), axis=1)\n",
    "\n",
    "def box_convert_to_xywh(boxes):\n",
    "    return np.concatenate(\n",
    "        [(boxes[..., :2] + boxes[..., 2:]) / 2.0, boxes[..., 2:] - boxes[..., :2]], axis=-1,)\n",
    "\n",
    "def box_convert_to_corners(boxes):    \n",
    "    return np.concatenate(\n",
    "        [boxes[..., :2] - boxes[..., 2:] / 2.0, boxes[..., :2] + boxes[..., 2:] / 2.0], axis=-1,)\n",
    "\n",
    "def angle_to_radian(angle):\n",
    "    return angle * np.pi/180\n",
    "\n",
    "def rotate_images(X, angle):\n",
    "    rotate_X = []\n",
    "    for i in range(len(X)):        \n",
    "        x = X[i]\n",
    "        img_h = x.shape[0]\n",
    "        img_w = x.shape[1]\n",
    "        img = Image.fromarray(x)                \n",
    "        img_rotated = img.rotate(angle)\n",
    "        rotate_X.append(np.array(img_rotated))\n",
    "\n",
    "    return rotate_X\n",
    "\n",
    "def rotate_box(box, angle, img_h, img_w):\n",
    "    box_xywh = box_convert_to_xywh(box)\n",
    "    box_xy = box_xywh[:, :2] \n",
    "    box_wh = box_xywh[:, 2:] \n",
    "    box_uv = (np.reshape(box_xy, [-1, 2]) - 0.5) * 2\n",
    "\n",
    "    scale_mat = np.array([1, 0, 0, 1.0*img_h/img_w]).reshape((2,2))\n",
    "    scale_mat_rev = np.array([1, 0, 0, 1.0*img_w/img_h]).reshape((2,2))\n",
    "    \n",
    "    radian = angle_to_radian(angle)        \n",
    "    rotate_mat = np.array([np.cos(radian), -np.sin(radian), np.sin(radian), np.cos(radian)])        \n",
    "    rotate_mat = np.reshape(rotate_mat, (2, 2))\n",
    "    box_uv_trans = np.matmul(box_uv, scale_mat)\n",
    "    box_uv_trans = np.matmul(box_uv_trans, rotate_mat)\n",
    "    box_uv_trans = np.matmul(box_uv_trans, scale_mat_rev)\n",
    "    box_trans = (box_uv_trans + 1)/2\n",
    "    box_trans_xy = np.reshape(box_trans, [-1, 2])\n",
    "    box_trans_xywh = np.concatenate((box_trans_xy, box_wh), axis=1)\n",
    "    box_trans = box_convert_to_corners(box_trans_xywh)\n",
    "    return box_trans\n",
    "\n",
    "def gen_rotate_data(X, BBOX, angle):\n",
    "    rotate_xs = []\n",
    "    rotate_bboxs = []\n",
    "    m = len(X)\n",
    "    for i in range(m):        \n",
    "        x = X[i]        \n",
    "        bbox = BBOX[i]       \n",
    "        attributes = bbox[:, 4:]        \n",
    "        box = bbox[:, :4]        \n",
    "        if np.random.rand() > 0.5:\n",
    "                angle = -angle\n",
    "     \n",
    "        img_h, img_w, img_c = x.shape\n",
    "        box_trans = rotate_box(box, angle, img_h, img_w)\n",
    "        #if np.min(box_trans)<0 or np.max(box_trans)>1: continue               \n",
    "        \n",
    "        bbox_trans = np.concatenate((box_trans, attributes), -1)\n",
    "        img = Image.fromarray(x)\n",
    "        img_rotated = img.rotate(angle)\n",
    "        #plt.imshow(img_rotated)\n",
    "        rotate_xs.append(np.array(img_rotated))\n",
    "        rotate_bboxs.append(bbox_trans)\n",
    "\n",
    "    return rotate_xs, rotate_bboxs        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "display_data(input_list, bbox_list, stride=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "m = len(input_list)\n",
    "print('m', m)\n",
    "s = 1\n",
    "input_list_train = input_list[::s]\n",
    "input_list_test = input_list[1::2]\n",
    "bbox_list_train = bbox_list[::s]\n",
    "bbox_list_test = bbox_list[1::2]\n",
    "print('bbox_list_train', len(bbox_list), len(bbox_list_train), len(bbox_list_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(input_list_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attach_col = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 1\n",
    "crop_xs, crop_bboxs = tile_object_biggest_center(input_list_train[::s], bbox_list_train[::s], scope=1/attach_col, min_cls=0)\n",
    "print('crop_xs', len(crop_xs), len(crop_bboxs), crop_xs[0].shape, crop_bboxs[0])\n",
    "#display_data(crop_xs, crop_bboxs, stride=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for box in crop_bboxs:\n",
    "    w = box[:, 2] - box[:, 0]\n",
    "    w_min = np.min(w)\n",
    "    if w_min < 0.01:\n",
    "        print(w_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_important_class_resample = False\n",
    "is_rotate_attach = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_important_class_resample:    \n",
    "    crop_xs_i, crop_bboxs_i = tile_object_biggest_center(input_list_train[::s], bbox_list_train[::s], scope=1/attach_col, min_cls=2)\n",
    "    print('crop_xs_i', len(crop_xs_i), len(crop_bboxs_i))\n",
    "    rotate_xs_i0, rotate_bboxs_i0 = gen_rotate_data(crop_xs_i, crop_bboxs_i, angle=0.5)\n",
    "    \n",
    "    print('rotate_xs', len(rotate_xs_i0), len(rotate_bboxs_i0))\n",
    "    crop_xs.extend(rotate_xs_i0)\n",
    "    crop_bboxs.extend(rotate_bboxs_i0)    \n",
    "    print(len(crop_xs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(crop_xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rotate_xs0, rotate_bboxs0 = gen_rotate_data(crop_xs[:2], crop_bboxs[:2], angle=31.5)\n",
    "display_data(rotate_xs0, rotate_bboxs0, stride=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rotate_xs, rotate_bboxs = gen_rotate_data(crop_xs, crop_bboxs, angle=0.5)    \n",
    "\n",
    "if is_rotate_attach:    \n",
    "    rotate_xs.extend(crop_xs)\n",
    "    rotate_bboxs.extend(crop_bboxs)\n",
    "\n",
    "    print('rotate_xs', len(rotate_xs), len(rotate_bboxs))\n",
    "    print('crop_xs', len(crop_xs), len(rotate_xs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_permutation(lst, p):\n",
    "    return [lst[x] for x in p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rotate_ind = np.arange(len(rotate_bboxs))\n",
    "np.random.shuffle(rotate_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rotate_xs = apply_permutation(rotate_xs, rotate_ind)\n",
    "rotate_bboxs = apply_permutation(rotate_bboxs, rotate_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attach_x, attach_bbox = attach_tiled_data(rotate_xs, rotate_bboxs, col=attach_col)\n",
    "len(attach_x), len(attach_bbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_data(attach_x, attach_bbox, stride=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "insert_stride = len(input_list_train)//len(attach_x)\n",
    "\n",
    "for i in range(len(attach_x)):\n",
    "    input_list_train.insert(i*insert_stride, attach_x.pop())\n",
    "    bbox_list_train.insert(i*insert_stride, attach_bbox.pop())\n",
    "    \n",
    "print('bbox_list_train', len(input_list_train), len(bbox_list_train), len(bbox_list_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input_list_train.extend(rotate_xs)\n",
    "bbox_list_train.extend(rotate_bboxs)\n",
    "len(input_list_train), len(rotate_xs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "attach_x, attach_bbox = attach_tiled_data(rotate_xs, rotate_bboxs, row=1, col=2)\n",
    "len(attach_x), len(attach_bbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_pedestrian:\n",
    "    display_data(input_list_PDESTRE, bbox_list_PDESTRE, stride=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "display_data(list_visdrone_x, list_visdrone_y, stride=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_pedestrian:    \n",
    "    print(len(input_list_PDESTRE), len(bbox_list_PDESTRE), input_list_PDESTRE[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rotate_xs.extend(crop_xs)\n",
    "rotate_bboxs.extend(crop_bboxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if use_pedestrian:    \n",
    "    m = len(input_list_PDESTRE)\n",
    "    for i in range(m):\n",
    "        x = input_list_PDESTRE[i]\n",
    "        y = bbox_list_PDESTRE[i]\n",
    "        if not x.shape==(540, 960, 3):\n",
    "            print(i, x.shape)\n",
    "            if x.shape[1]<500:\n",
    "                print(x.shape)\n",
    "                input_list_PDESTRE.pop(i)\n",
    "                bbox_list_PDESTRE.pop(i)\n",
    "        m = len(input_list_PDESTRE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('crop_xs', len(crop_xs), len(rotate_xs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_pedestrian:\n",
    "    display_data(input_list_PDESTRE, bbox_list_PDESTRE, stride=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_pedestrian:\n",
    "    attach_xs, attach_bboxs = attach_tiled_data(np.stack(input_list_PDESTRE, 0), bbox_list_PDESTRE, col=2)\n",
    "    \n",
    "    print(len(attach_xs), len(attach_bboxs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_pedestrian:\n",
    "    print(len(attach_xs), len(attach_bboxs))\n",
    "    display_data(attach_xs, attach_bboxs, stride=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_pedestrian:\n",
    "    attach_x_row, attach_bbox_row = attach_tiled_data_row(attach_xs, attach_bboxs, row=2)  \n",
    "    print(len(attach_x_row), len(attach_bbox_row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_pedestrian:\n",
    "    display_data(attach_x_row, attach_bbox_row, stride=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_pedestrian:\n",
    "    print('attach_x', len(attach_x), 'attach_x_row', len(attach_x_row))\n",
    "    attach_x.extend(attach_x_row)\n",
    "    attach_bbox.extend(attach_bbox_row)\n",
    "    print('attach_x', len(attach_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_pedestrian:\n",
    "    insert_stride = len(attach_x)//len(attach_x_row)\n",
    "\n",
    "    for i in range(len(attach_x_row)):\n",
    "        attach_x.insert(i*insert_stride, attach_x_row.pop())\n",
    "        attach_bbox.insert(i*insert_stride, attach_bbox_row.pop())\n",
    "\n",
    "    print('bbox_list_train', len(attach_x), len(attach_x_row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insert_stride = len(input_list_train)//len(attach_x)\n",
    "print('insert_stride', insert_stride)\n",
    "\n",
    "for i in range(len(attach_x)):\n",
    "    input_list_train.insert(i*insert_stride, attach_x.pop())\n",
    "    bbox_list_train.insert(i*insert_stride, attach_bbox.pop())\n",
    "\n",
    "print('bbox_list_train', len(input_list_train), len(attach_x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input_list_train = attach_x\n",
    "bbox_list_train = attach_bbox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "attach_xs.extend(attach_x)\n",
    "attach_bboxs.extend(attach_bbox)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "len(attach_xs), len(attach_bboxs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "display_data(attach_xs, attach_bboxs, stride=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input_list_train.extend(attach_xs)\n",
    "bbox_list_train.extend(attach_bboxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if use_visdrone:\n",
    "    display_data(list_visdrone_x, list_visdrone_y, stride=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_visdrone:\n",
    "    \n",
    "    insert_stride = len(input_list_train)//len(list_visdrone_x)\n",
    "    print('insert_stride', insert_stride)\n",
    "\n",
    "    for i in range(len(list_visdrone_x)):\n",
    "        input_list_train.insert(i*insert_stride, list_visdrone_x.pop())\n",
    "        bbox_list_train.insert(i*insert_stride, list_visdrone_y.pop())\n",
    "\n",
    "    print('bbox_list_train', len(input_list_train), len(list_visdrone_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('len', len(input_list_train), len(input_list_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(input_list_aero), len(bbox_list_aero), len(segment_list_aero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display_data(segment_list_aero, bbox_list_aero, stride=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add segmentation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(2, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_list_train = []\n",
    "\n",
    "if len(input_list_train)>len(input_list_aero):\n",
    "    insert_stride = max(2, len(input_list_train)//len(input_list_aero))\n",
    "else:\n",
    "    insert_stride = 2\n",
    "\n",
    "    print('insert_stride', insert_stride)\n",
    "\n",
    "for i in range(len(input_list_train)):\n",
    "    seg_list_train.append(np.array([0]))\n",
    "\n",
    "for i in range(len(input_list_aero)):\n",
    "    input_list_train.insert(i*insert_stride, input_list_aero.pop())\n",
    "    bbox_list_train.insert(i*insert_stride, bbox_list_aero.pop())    \n",
    "    seg_list_train.insert(i*insert_stride, segment_list_aero.pop())    \n",
    "\n",
    "print('len', len(input_list_train), len(bbox_list_train), len(seg_list_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_data(input_list_test, bbox_list_test, stride=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display_data(input_list_train, bbox_list_train, stride=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display_data(seg_list_train, bbox_list_train, stride=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swap_xy(boxes):\n",
    "    \"\"\"Swaps order the of x and y coordinates of the boxes.\n",
    "    Arguments:\n",
    "      boxes: A tensor with shape `(num_boxes, 4)` representing bounding boxes.\n",
    "    Returns:\n",
    "      swapped boxes with shape same as that of boxes.\n",
    "    \"\"\"\n",
    "    return tf.stack([boxes[:, 1], boxes[:, 0], boxes[:, 3], boxes[:, 2]], axis=-1)\n",
    "\n",
    "\n",
    "def convert_to_xywh(boxes):\n",
    "    \"\"\"Changes the box format to center, width and height.\n",
    "    Arguments:\n",
    "      boxes: A tensor of rank 2 or higher with a shape of `(..., num_boxes, 4)`\n",
    "        representing bounding boxes where each box is of the format\n",
    "        `[xmin, ymin, xmax, ymax]`.\n",
    "    Returns:\n",
    "      converted boxes with shape same as that of boxes.\n",
    "    \"\"\"\n",
    "    return tf.concat(\n",
    "        [(boxes[..., :2] + boxes[..., 2:]) / 2.0, boxes[..., 2:] - boxes[..., :2]],\n",
    "        axis=-1,\n",
    "    )\n",
    "\n",
    "\n",
    "def convert_to_corners(boxes):\n",
    "    \"\"\"Changes the box format to corner coordinates\n",
    "    Arguments:\n",
    "      boxes: A tensor of rank 2 or higher with a shape of `(..., num_boxes, 4)`\n",
    "        representing bounding boxes where each box is of the format\n",
    "        `[x, y, width, height]`.\n",
    "    Returns:\n",
    "      converted boxes with shape same as that of boxes.\n",
    "    \"\"\"\n",
    "    return tf.concat(\n",
    "        [boxes[..., :2] - boxes[..., 2:] / 2.0, boxes[..., :2] + boxes[..., 2:] / 2.0],\n",
    "        axis=-1,\n",
    "    )\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "## Computing pairwise Intersection Over Union (IOU)\n",
    "As we will see later in the example, we would be assigning ground truth boxes\n",
    "to anchor boxes based on the extent of overlapping. This will require us to\n",
    "calculate the Intersection Over Union (IOU) between all the anchor\n",
    "boxes and ground truth boxes pairs.\n",
    "\"\"\"\n",
    "\n",
    "def compute_iou(boxes1, boxes2):#compute_iou(anchor_boxes, gt_boxes)\n",
    "    \"\"\"Computes pairwise IOU matrix for given two sets of boxes\n",
    "    Arguments:\n",
    "      boxes1: A tensor with shape `(N, 4)` representing bounding boxes\n",
    "        where each box is of the format `[x, y, width, height]`.\n",
    "        boxes2: A tensor with shape `(M, 4)` representing bounding boxes\n",
    "        where each box is of the format `[x, y, width, height]`.\n",
    "    Returns:\n",
    "      pairwise IOU matrix with shape `(N, M)`, where the value at ith row\n",
    "        jth column holds the IOU between ith box and jth box from\n",
    "        boxes1 and boxes2 respectively.\n",
    "    \"\"\"\n",
    "    boxes1_corners = convert_to_corners(boxes1)\n",
    "    boxes2_corners = convert_to_corners(boxes2)\n",
    "    lu = tf.maximum(boxes1_corners[:, None, :2], boxes2_corners[:, :2])\n",
    "    rd = tf.minimum(boxes1_corners[:, None, 2:], boxes2_corners[:, 2:])\n",
    "    intersection = tf.maximum(0.0, rd - lu)\n",
    "    intersection_area = intersection[:, :, 0] * intersection[:, :, 1]\n",
    "    boxes1_area = boxes1[:, 2] * boxes1[:, 3]\n",
    "    boxes2_area = boxes2[:, 2] * boxes2[:, 3]\n",
    "    union_area = tf.maximum(\n",
    "        boxes1_area[:, None] + boxes2_area - intersection_area, 1e-8\n",
    "    )\n",
    "    return tf.clip_by_value(intersection_area / union_area, 0.0, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "level_end - level_start, anchor_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "## Implementing Anchor generator\n",
    "Anchor boxes are fixed sized boxes that the model uses to predict the bounding\n",
    "box for an object. It does this by regressing the offset between the location\n",
    "of the object's center and the center of an anchor box, and then uses the width\n",
    "and height of the anchor box to predict a relative scale of the object. In the\n",
    "case of RetinaNet, each location on a given feature map has nine anchor boxes\n",
    "(at three scales and three ratios).\n",
    "\"\"\"\n",
    "class AnchorBox:\n",
    "    \"\"\"Generates anchor boxes.\n",
    "    This class has operations to generate anchor boxes for feature maps at\n",
    "    strides `[8, 16, 32, 64, 128]`. Where each anchor each box is of the\n",
    "    format `[x, y, width, height]`.\n",
    "    Attributes:\n",
    "      aspect_ratios: A list of float values representing the aspect ratios of\n",
    "        the anchor boxes at each location on the feature map\n",
    "      scales: A list of float values representing the scale of the anchor boxes\n",
    "        at each location on the feature map.\n",
    "      num_anchors: The number of anchor boxes at each location on feature map\n",
    "      areas: A list of float values representing the areas of the anchor\n",
    "        boxes for each feature map in the feature pyramid.\n",
    "      strides: A list of float value representing the strides for each feature\n",
    "        map in the feature pyramid.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.level_start = level_start\n",
    "        self.level_end = level_end\n",
    "        \n",
    "        if anchor_k==9:\n",
    "            self.aspect_ratios = [0.5, 1.0, 2.0]        \n",
    "            self.scales = [2 ** x for x in [0, 1 / 3, 2 / 3]]\n",
    "        elif anchor_k==6:\n",
    "            self.aspect_ratios = [0.5, 1.0] #area / aspect_ratios. = height\n",
    "            self.scales = [2 ** x for x in [0, 1 / 3, 2 / 3]]        \n",
    "        else:\n",
    "            self.aspect_ratios = [1.0]#area / ratio = height\n",
    "            self.scales = [2 ** x for x in [0]]\n",
    "                \n",
    "        self._num_anchors = len(self.aspect_ratios) * len(self.scales)\n",
    "        self._strides = [2 ** i for i in range(self.level_start, self.level_end)]\n",
    "        self._areas = [x ** 2 for x in [32.0, 64.0, 128.0, 196.0, 256.0]]                        \n",
    "        self._areas = self._areas[:level_end - level_start]\n",
    "        \n",
    "        self._anchor_dims = self._compute_dims()\n",
    "\n",
    "    def _compute_dims(self):\n",
    "        \"\"\"Computes anchor box dimensions for all ratios and scales at all levels\n",
    "        of the feature pyramid.\n",
    "        \"\"\"\n",
    "        anchor_dims_all = []\n",
    "        for area in self._areas:\n",
    "            anchor_dims = []\n",
    "            for ratio in self.aspect_ratios:\n",
    "                anchor_height = tf.math.sqrt(area / ratio)\n",
    "                anchor_width = area / anchor_height\n",
    "                dims = tf.reshape(\n",
    "                    tf.stack([anchor_width, anchor_height], axis=-1), [1, 1, 2]\n",
    "                )\n",
    "                for scale in self.scales:\n",
    "                    anchor_dims.append(scale * dims)\n",
    "            anchor_dims_all.append(tf.stack(anchor_dims, axis=-2))\n",
    "        return anchor_dims_all\n",
    "\n",
    "    def _get_anchors(self, feature_height, feature_width, level):\n",
    "        \"\"\"Generates anchor boxes for a given feature map size and level\n",
    "        Arguments:\n",
    "          feature_height: An integer representing the height of the feature map.\n",
    "          feature_width: An integer representing the width of the feature map.\n",
    "          level: An integer representing the level of the feature map in the\n",
    "            feature pyramid.\n",
    "        Returns:\n",
    "          anchor boxes with the shape\n",
    "          `(feature_height * feature_width * num_anchors, 4)`\n",
    "        \"\"\"\n",
    "        rx = tf.range(feature_width, dtype=tf.float32) + 0.5\n",
    "        ry = tf.range(feature_height, dtype=tf.float32) + 0.5\n",
    "        centers = tf.stack(tf.meshgrid(rx, ry), axis=-1) * self._strides[level - self.level_start]\n",
    "        centers = tf.expand_dims(centers, axis=-2)\n",
    "        centers = tf.tile(centers, [1, 1, self._num_anchors, 1])\n",
    "        dims = tf.tile(\n",
    "            self._anchor_dims[level - self.level_start], [feature_height, feature_width, 1, 1]\n",
    "        )\n",
    "        anchors = tf.concat([centers, dims], axis=-1)\n",
    "        return tf.reshape(\n",
    "            anchors, [feature_height * feature_width * self._num_anchors, 4]\n",
    "        )\n",
    "\n",
    "    def get_anchors(self, image_height, image_width):\n",
    "        \"\"\"Generates anchor boxes for all the feature maps of the feature pyramid.\n",
    "        Arguments:\n",
    "          image_height: Height of the input image.\n",
    "          image_width: Width of the input image.\n",
    "        Returns:\n",
    "          anchor boxes for all the feature maps, stacked as a single tensor\n",
    "            with shape `(total_anchors, 4)`\n",
    "        \"\"\"\n",
    "        anchors = [\n",
    "            self._get_anchors(\n",
    "                tf.math.ceil(image_height / 2 ** i),\n",
    "                tf.math.ceil(image_width / 2 ** i),\n",
    "                i,\n",
    "            )\n",
    "            for i in range(self.level_start, self.level_end)\n",
    "        ]\n",
    "        return tf.concat(anchors, axis=0)\n",
    "    \n",
    "    def get_anchors_check(self, image_height, image_width):\n",
    "        \"\"\"Generates anchor boxes for all the feature maps of the feature pyramid.\n",
    "        Arguments:\n",
    "          image_height: Height of the input image.\n",
    "          image_width: Width of the input image.\n",
    "        Returns:\n",
    "          anchor boxes for all the feature maps, stacked as a single tensor\n",
    "            with shape `(total_anchors, 4)`\n",
    "        \"\"\"\n",
    "        anchors = [\n",
    "            self._get_anchors(\n",
    "                tf.math.ceil(image_height / 2 ** i),\n",
    "                tf.math.ceil(image_width / 2 ** i),\n",
    "                i,\n",
    "            )\n",
    "            for i in range(self.level_start, self.level_end)\n",
    "        ]\n",
    "        return anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "level_start, level_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_check = AnchorBox()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "anchors = anchor_check.get_anchors_check(128,128)\n",
    "for anchor in anchors:\n",
    "    print(anchor.shape, anchor[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_image_shape, 128*8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## Preprocessing data\n",
    "Preprocessing the images involves two steps:\n",
    "- Resizing the image: Images are resized such that the shortest size is equal\n",
    "to 800 px, after resizing if the longest side of the image exceeds 1333 px,\n",
    "the image is resized such that the longest size is now capped at 1333 px.\n",
    "- Applying augmentation: Random scale jittering  and random horizontal flipping\n",
    "are the only augmentations applied to the images.\n",
    "Along with the images, bounding boxes are rescaled and flipped if required.\n",
    "\"\"\"\n",
    "\n",
    "def random_flip_horizontal(image, boxes):\n",
    "    \"\"\"Flips image and boxes horizontally with 50% chance\n",
    "    Arguments:\n",
    "      image: A 3-D tensor of shape `(height, width, channels)` representing an\n",
    "        image.\n",
    "      boxes: A tensor with shape `(num_boxes, 4)` representing bounding boxes,\n",
    "        having normalized coordinates.\n",
    "    Returns:\n",
    "      Randomly flipped image and boxes\n",
    "    \"\"\"\n",
    "    if tf.random.uniform(()) > 0.5:\n",
    "        image = tf.image.flip_left_right(image)\n",
    "        boxes = tf.stack([1 - boxes[:, 2], boxes[:, 1], 1 - boxes[:, 0], boxes[:, 3]], axis=-1)\n",
    "   \n",
    "    return image, boxes\n",
    "\n",
    "\n",
    "def resize_and_pad_image_seg(\n",
    "    image, segmap, min_side=1024.0, max_side=11333.0, jitter=[128*8, 128*8+1], stride=128.0):\n",
    "   \n",
    "    image_shape = tf.cast(tf.shape(image)[:2], dtype=tf.float32)\n",
    "    \n",
    "    ratio = min_side / tf.reduce_min(image_shape)\n",
    "    \n",
    "    image_shape = ratio * image_shape\n",
    "    image = tf.image.resize(image, tf.cast(image_shape, dtype=tf.int32))\n",
    "    image = tf.image.pad_to_bounding_box(image, 0, 0, padded_image_shape[0], padded_image_shape[1]) \n",
    "    \n",
    "    segmap_exp = tf.expand_dims(segmap, -1)\n",
    "    segmap_exp = tf.image.resize(segmap_exp, tf.cast(image_shape, dtype=tf.int32), 'nearest')\n",
    "    segmap_exp = tf.image.pad_to_bounding_box(segmap_exp, 0, 0, padded_image_shape[0], padded_image_shape[1])        \n",
    "    segmap = tf.squeeze(segmap_exp, -1)\n",
    "            \n",
    "    return image, segmap, image_shape, ratio\n",
    "\n",
    "\n",
    "def resize_and_pad_image(\n",
    "    image, min_side=1024.0, max_side=11333.0, jitter=[128*8, 128*8+1], stride=128.0):\n",
    "   \n",
    "    image_shape = tf.cast(tf.shape(image)[:2], dtype=tf.float32)\n",
    "    \n",
    "    ratio = min_side / tf.reduce_min(image_shape)\n",
    "    \n",
    "    image_shape = ratio * image_shape\n",
    "    image = tf.image.resize(image, tf.cast(image_shape, dtype=tf.int32))\n",
    "    image = tf.image.pad_to_bounding_box(image, 0, 0, padded_image_shape[0], padded_image_shape[1]) \n",
    "   \n",
    "    return image, image_shape, ratio\n",
    "\n",
    "\n",
    "def resize_and_pad_image_bbox(\n",
    "    image, bbox, mask_obj=None, min_side=1024.0, max_side=1024.0*4, jitter=[128*7+32, 128*8-32], stride=128.0\n",
    "):\n",
    "    #image, min_side=800.0, max_side=1333.0, jitter=[640, 1024], stride=128.0\n",
    "    \"\"\"Resizes and pads image while preserving aspect ratio.\n",
    "    1. Resizes images so that the shorter side is equal to `min_side`\n",
    "    2. If the longer side is greater than `max_side`, then resize the image\n",
    "      with longer side equal to `max_side`\n",
    "    3. Pad with zeros on right and bottom to make the image shape divisible by\n",
    "    `stride`\n",
    "    Arguments:\n",
    "      image: A 3-D tensor of shape `(height, width, channels)` representing an\n",
    "        image.\n",
    "      min_side: The shorter side of the image is resized to this value, if\n",
    "        `jitter` is set to None.\n",
    "      max_side: If the longer side of the image exceeds this value after\n",
    "        resizing, the image is resized such that the longer side now equals to\n",
    "        this value.\n",
    "      jitter: A list of floats containing minimum and maximum size for scale\n",
    "        jittering. If available, the shorter side of the image will be\n",
    "        resized to a random value in this range.\n",
    "      stride: The stride of the smallest feature map in the feature pyramid.\n",
    "        Can be calculated using `image_size / feature_map_size`.\n",
    "    Returns:\n",
    "      image: Resized and padded image.\n",
    "      image_shape: Shape of the image before padding.\n",
    "      ratio: The scaling factor used to resize the image\n",
    "    \"\"\"\n",
    "    image_shape = tf.cast(tf.shape(image)[:2], dtype=tf.float32)\n",
    "    if jitter is not None:\n",
    "        min_side = tf.random.uniform((), jitter[0], jitter[1], dtype=tf.float32)\n",
    "    ratio = min_side / tf.reduce_min(image_shape)\n",
    "    if ratio * tf.reduce_max(image_shape) > max_side:\n",
    "        ratio = max_side / tf.reduce_max(image_shape)\n",
    "    image_shape = ratio * image_shape\n",
    "    ratio_jitter = tf.random.uniform(tf.shape(image_shape), -32, 32, dtype=tf.float32)\n",
    "    image_shape += ratio_jitter      \n",
    "    image = tf.image.resize(image, tf.cast(image_shape, dtype=tf.int32))\n",
    "    if mask_obj!=None:\n",
    "        mask_obj = tf.image.resize(mask_obj, tf.cast(image_shape, dtype=tf.int32))\n",
    "    padded_image_shape = tf.cast(\n",
    "        tf.math.ceil(image_shape / stride) * stride, dtype=tf.int32\n",
    "    )\n",
    "    image = tf.image.pad_to_bounding_box(image, 0, 0, padded_image_shape[0], padded_image_shape[1])\n",
    "    if mask_obj!=None:\n",
    "        mask_obj = tf.image.pad_to_bounding_box(mask_obj, 0, 0, padded_image_shape[0], padded_image_shape[1])        \n",
    "    padded_image_shape = tf.cast(padded_image_shape, tf.float32)              \n",
    "    pad_ratio = tf.cast(image_shape, tf.float32) / padded_image_shape\n",
    "    bbox_padded = tf.stack(\n",
    "        [\n",
    "            bbox[:, 0] * pad_ratio[1],\n",
    "            bbox[:, 1] * pad_ratio[0],\n",
    "            bbox[:, 2] * pad_ratio[1],\n",
    "            bbox[:, 3] * pad_ratio[0],\n",
    "        ],\n",
    "        axis=-1,\n",
    "    )\n",
    "    if mask_obj!=None:\n",
    "        return image, padded_image_shape, ratio, bbox_padded, mask_obj    \n",
    "    return image, padded_image_shape, ratio, bbox_padded\n",
    "\n",
    "\n",
    "def unnormalize_box(bbox, image_shape):\n",
    "    bbox = tf.cast(bbox, tf.float32)\n",
    "    bbox = tf.stack(\n",
    "        [\n",
    "            bbox[:, 0] * image_shape[1],\n",
    "            bbox[:, 1] * image_shape[0],\n",
    "            bbox[:, 2] * image_shape[1],\n",
    "            bbox[:, 3] * image_shape[0],\n",
    "        ],\n",
    "        axis=-1,\n",
    "    )\n",
    "    bbox = convert_to_xywh(bbox)    \n",
    "    return bbox    \n",
    "\n",
    "\n",
    "def preprocess_data(image, segmap, cls_bbox):\n",
    "    \"\"\"Applies preprocessing step to a single sample\n",
    "    Arguments:\n",
    "      sample: A dict representing a single training sample.\n",
    "    Returns:\n",
    "      image: Resized and padded image with random horizontal flipping applied.\n",
    "      bbox: Bounding boxes with the shape `(num_objects, 4)` where each box is\n",
    "        of the format `[x, y, width, height]`.\n",
    "      class_id: An tensor representing the class id of the objects, having\n",
    "        shape `(num_objects,)`.\n",
    "    \"\"\"\n",
    "     \n",
    "    bbox = cls_bbox[:, :4]    \n",
    "    attribute = tf.cast(cls_bbox[:, 4:], dtype=tf.int32)\n",
    "\n",
    "    rgbs = tf.concat((image, tf.expand_dims(segmap, -1)), -1)\n",
    "    rgbs, bbox = random_flip_horizontal(rgbs, bbox)        \n",
    "    #image, image_shape, _, bbox = resize_and_pad_image_bbox(image, bbox)        \n",
    "    \n",
    "    rgbs, image_shape, _ = resize_and_pad_image(rgbs)\n",
    "    bbox = unnormalize_box(bbox, image_shape)\n",
    "    \n",
    "    return rgbs, bbox, attribute\n",
    "\n",
    "def preprocess_test_data(image, segmap, cls_bbox):         \n",
    "    bbox = cls_bbox[:, :4]    \n",
    "    attribute = tf.cast(cls_bbox[:, 4:], dtype=tf.int32)\n",
    "    rgbs = tf.concat((image, tf.expand_dims(segmap, -1)), -1)\n",
    "    \n",
    "    rgbs, image_shape, _ = resize_and_pad_image(rgbs)\n",
    "    bbox = unnormalize_box(bbox, image_shape)    \n",
    "    \n",
    "    return rgbs, bbox, attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_weather_effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_color_augment(x):\n",
    "    if tf.random.uniform(()) < -0.5:\n",
    "        x_max = tf.reduce_max(x, [1, 2], True)\n",
    "        x = x_max - x\n",
    "    if tf.random.uniform(()) < 0.1:\n",
    "        r, g, b = tf.split(x, 3, axis=-1)\n",
    "        x = tf.concat((r, b, g), -1)\n",
    "    elif tf.random.uniform(()) < -0.2:\n",
    "        r, g, b = tf.split(x, 3, axis=-1)\n",
    "        x = tf.concat((b, r, g), -1)\n",
    "    if tf.random.uniform(()) < 0.2:\n",
    "        x = tf.image.random_hue(x, 0.08)\n",
    "        x = tf.image.random_saturation(x, 0.6, 1.6)\n",
    "    if tf.random.uniform(()) < 0.2:\n",
    "        x = tf.image.random_brightness(x, 0.05)\n",
    "        x = tf.image.random_contrast(x, 0.7, 1.3)\n",
    "    if tf.random.uniform(()) < -0.2:\n",
    "        gray = tf.image.rgb_to_grayscale(x)\n",
    "        x = tf.concat((gray, gray, gray), -1)        \n",
    "    if tf.random.uniform(()) < -0.2:\n",
    "        noise = tf.random.normal(tf.shape(x), stddev=tf.pow(tf.reduce_mean(x), 0.3))\n",
    "        x += noise\n",
    "    if tf.random.uniform(()) < 0.2:\n",
    "        x = gaussian_filter2d(x, filter_shape=tuple(np.random.randint(1, 10, (2))), sigma=10)\n",
    "        #x = gaussian_filter2d(x, filter_shape=np.random.randint(3, 10, (2)), sigma=10)\n",
    "    if tf.random.uniform(()) < 0.2:        \n",
    "        x = sharpness(x, factor=10)\n",
    "    if tf.random.uniform(()) < 0.2:        \n",
    "        if use_weather_effect:\n",
    "            weather_k = len(weather_images)                            \n",
    "            h = tf.shape(x)[1]\n",
    "            w = tf.shape(x)[2]\n",
    "            weather_image = weather_images[np.random.randint(weather_k)]\n",
    "            weather_image = tf.image.resize(weather_image, tf.cast((h, w), dtype=tf.int32))\n",
    "            weather_image = tf.expand_dims(weather_image, 0)\n",
    "            x = (x // 3) * 2 + weather_image//3\n",
    "            \n",
    "    #x = tf.image.random_jpeg_quality(x, 0, 1.0)\n",
    "    #x = tf.clip_by_value(x, 0, 1)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## Encoding labels\n",
    "The raw labels, consisting of bounding boxes and class ids need to be\n",
    "transformed into targets for training. This transformation consists of\n",
    "the following steps:\n",
    "- Generating anchor boxes for the given image dimensions\n",
    "- Assigning ground truth boxes to the anchor boxes\n",
    "- The anchor boxes that are not assigned any objects, are either assigned the\n",
    "background class or ignored depending on the IOU\n",
    "- Generating the classification and regression targets using anchor boxes\n",
    "\"\"\"\n",
    "\n",
    "class LabelEncoder:\n",
    "    \"\"\"Transforms the raw labels into targets for training.\n",
    "    This class has operations to generate targets for a batch of samples which\n",
    "    is made up of the input images, bounding boxes for the objects present and\n",
    "    their class ids.\n",
    "    Attributes:\n",
    "      anchor_box: Anchor box generator to encode the bounding boxes.\n",
    "      box_variance: The scaling factors used to scale the bounding box targets.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self._anchor_box = AnchorBox()\n",
    "        self._box_variance = tf.convert_to_tensor(\n",
    "            [0.1, 0.1, 0.2, 0.2], dtype=tf.float32\n",
    "        )    \n",
    "    \n",
    "    def _compute_box_target(self, anchor_boxes, matched_gt_boxes):\n",
    "        \"\"\"Transforms the ground truth boxes into targets for training\"\"\"\n",
    "        box_target = tf.concat(\n",
    "            [\n",
    "                (matched_gt_boxes[:, :2] - anchor_boxes[:, :2]) / anchor_boxes[:, 2:],\n",
    "                tf.math.log(matched_gt_boxes[:, 2:] / anchor_boxes[:, 2:]),\n",
    "            ],\n",
    "            axis=-1,\n",
    "        )\n",
    "        box_target = box_target / self._box_variance\n",
    "        return box_target\n",
    "    \n",
    "    def _compute_box_width(self, anchor_boxes, matched_gt_boxes):\n",
    "        \"\"\"Transforms the ground truth boxes into targets for training\"\"\"\n",
    "        box_w = matched_gt_boxes[:, 3] * tf.ones_like(anchor_boxes[:, 0])\n",
    "        box_w_norm = 1 + tf.pow(box_w / tf.reduce_max(box_w), 3)\n",
    "        return box_w_norm\n",
    "    \n",
    "    \n",
    "    def _match_anchor_boxes(\n",
    "        self, anchor_boxes, gt_boxes, match_iou=0.35, ignore_iou=0.1\n",
    "    ):\n",
    "        iou_matrix = compute_iou(anchor_boxes, gt_boxes)\n",
    "        max_iou = tf.reduce_max(iou_matrix, axis=1)#from anchor to object-box        \n",
    "        matched_gt_idx = tf.argmax(iou_matrix, axis=1)    \n",
    "        positive_mask = tf.greater_equal(max_iou, match_iou)# not only this, but also need max iou cell\n",
    "        \n",
    "        positive_proposal_mask = tf.greater_equal(iou_matrix, match_iou)\n",
    "        positive_mask = tf.reduce_any(positive_proposal_mask, axis=1)\n",
    "        \n",
    "        negative_mask = tf.less(max_iou, ignore_iou)\n",
    "        \n",
    "        max_iou_anchor = tf.reduce_max(iou_matrix, axis=0, keepdims=True) \n",
    "        max_iou_anchor_mask = tf.greater_equal(iou_matrix, max_iou_anchor)\n",
    "        positive_max_mask = tf.reduce_any(max_iou_anchor_mask, axis=1)\n",
    "        positive_mask = tf.logical_or(positive_mask, positive_max_mask)#new      \n",
    "        \n",
    "        negative_mask = tf.logical_and(negative_mask, tf.logical_not(positive_mask))\n",
    "        ignore_mask = tf.logical_not(tf.logical_or(positive_mask, negative_mask))        \n",
    "        \n",
    "        return (\n",
    "            matched_gt_idx,            \n",
    "            tf.cast(positive_mask, dtype=tf.float32),\n",
    "            tf.cast(positive_max_mask, dtype=tf.float32),            \n",
    "            tf.cast(ignore_mask, dtype=tf.float32),\n",
    "        )\n",
    "\n",
    "    def _encode_sample(self, image_shape, gt_boxes, box_label):\n",
    "        \"\"\"Creates box and classification targets for a single sample\"\"\"\n",
    "        \n",
    "        anchor_boxes = self._anchor_box.get_anchors(image_shape[1], image_shape[2])\n",
    "        box_label = tf.cast(box_label, dtype=tf.float32)\n",
    "        \n",
    "        bx, by, bw, bh = tf.split(gt_boxes, 4, -1)#finetune xywh original size\n",
    "        \n",
    "        bw = tf.squeeze(bw * bh, -1)\n",
    "        bw = tf.sqrt(bw)\n",
    "        \n",
    "        cls_ids, cls_gender, cls_body, upper_cloth = tf.split(box_label, 4, -1)\n",
    "        cls_ids = tf.squeeze(cls_ids, -1)\n",
    "        cls_gender = tf.squeeze(cls_gender, -1)\n",
    "        cls_body = tf.squeeze(cls_body, -1)\n",
    "        upper_cloth = tf.squeeze(upper_cloth, -1)\n",
    "        \n",
    "        matched_gt_idx, positive_mask, positive_max_mask, ignore_mask = self._match_anchor_boxes(\n",
    "            anchor_boxes, gt_boxes\n",
    "        )\n",
    "        matched_gt_boxes = tf.gather(gt_boxes, matched_gt_idx)\n",
    "        matched_gt_boxes_size = tf.reduce_prod(matched_gt_boxes[:, 2:], 1)\n",
    "        matched_gt_boxes_size = tf.sqrt(matched_gt_boxes_size)        \n",
    "        \n",
    "        box_target = self._compute_box_target(anchor_boxes, matched_gt_boxes)    \n",
    "        box_target_w = self._compute_box_width(anchor_boxes, matched_gt_boxes)\n",
    "        matched_bw = tf.gather(bw, matched_gt_idx)\n",
    "        matched_gt_cls_ids = tf.gather(cls_ids, matched_gt_idx)\n",
    "        matched_gt_gender = tf.gather(cls_gender, matched_gt_idx)\n",
    "        matched_cls_body = tf.gather(cls_body, matched_gt_idx)\n",
    "        matched_upper_cloth = tf.gather(upper_cloth, matched_gt_idx)\n",
    "        \n",
    "        cls_target = tf.where(tf.not_equal(positive_mask, 1.0), 0.0, matched_gt_cls_ids)        \n",
    "        cls_target = tf.where(tf.equal(ignore_mask, 1.0), -1.0, cls_target)\n",
    "        #bw_thresh = tf.minimum(tf.reduce_min(matched_bw)+1, 10)\n",
    "        #cls_target = tf.where(matched_bw < bw_thresh, -1.0, cls_target) #finetune\n",
    "        \n",
    "        attribute_target = tf.stack((cls_target, box_target_w, matched_gt_gender, matched_cls_body, matched_upper_cloth), -1)\n",
    "        targets = tf.concat([box_target, attribute_target], axis=-1)        \n",
    "        return targets\n",
    "    \n",
    "    def encode_batch(self, rgbs, gt_boxes, box_label):\n",
    "        \"\"\"Creates box and classification targets for a batch\"\"\"\n",
    "        \n",
    "        images_shape = tf.shape(rgbs)\n",
    "        batch_size = images_shape[0]\n",
    "       \n",
    "        labels = tf.TensorArray(dtype=tf.float32, size=batch_size, dynamic_size=True)\n",
    "        for i in range(batch_size):\n",
    "            label = self._encode_sample(images_shape, gt_boxes[i], box_label[i])\n",
    "            labels = labels.write(i, label)\n",
    "        \n",
    "        \n",
    "        batch_images = tf.cast(rgbs[:,:,:,:3], tf.float32)\n",
    "        segmap = tf.cast(rgbs[:,:,:,-1], tf.int32)\n",
    "        labels = labels.stack()\n",
    "                \n",
    "        return batch_images, {\"segment\": segmap, \"detect\": labels}#dual\n",
    "        #return batch_images, segmap, labels#single\n",
    "    \n",
    "    def encode_batch_train(self, rgbs, gt_boxes, cls):\n",
    "        \"\"\"Creates box and classification targets for a batch\"\"\"\n",
    "        batch_images = rgbs[:,:,:,:3]\n",
    "        segmap = rgbs[:,:,:,-1:]\n",
    "        batch_images = image_color_augment(batch_images)#finetune        \n",
    "        rgbs = tf.concat((batch_images, segmap), -1)\n",
    "        return self.encode_batch(rgbs, gt_boxes, cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BifeaturePyramidNet(c345):\n",
    "    filters = 128\n",
    "    a2 = c345[0]\n",
    "    a3 = c345[1]\n",
    "    a4 = c345[2]\n",
    "    a5 = c345[3]\n",
    "    \n",
    "    regulizer  = tf.keras.regularizers.L2(l1)\n",
    "   \n",
    "    a2_0 = Conv2D(filters, 1, 1, \"same\", groups=1, activation=activation, kernel_regularizer=regulizer, name='a2_0')(a2)    \n",
    "    a33 = Conv2D(filters*2, 1, 1, \"same\", groups=1, activation=activation, kernel_regularizer=regulizer, name='a33')(a3)\n",
    "    a44 = Conv2D(filters*2, 1, 1, \"same\", groups=1, activation=activation, kernel_regularizer=regulizer, name='a44')(a4)\n",
    "    a55 = Conv2D(filters*2, 1, 1, \"same\", groups=1, activation=activation, kernel_regularizer=regulizer, name='a55')(a5)\n",
    "    a66 = Conv2D(filters*2, 3, 2, \"same\", groups=1, activation=activation, kernel_regularizer=regulizer, name='a66')(a5)\n",
    "    \n",
    "    a3_0, a3_1 = tf.split(a33, 2, -1)\n",
    "    a4_0, a4_1 = tf.split(a44, 2, -1)\n",
    "    a5_0, a5_1 = tf.split(a55, 2, -1)\n",
    "    a6_0, a6_1 = tf.split(a66, 2, -1)\n",
    "    \n",
    "    b6 = a6_0\n",
    "    \n",
    "    a6_up = keras.layers.UpSampling2D(2)(a6_1)    \n",
    "    b5 = keras.layers.Add()([a5_0, a6_up])  \n",
    "        \n",
    "    a5_up = keras.layers.UpSampling2D(2)(a5_1)    \n",
    "    b4 = keras.layers.Add()([a4_0, a5_up])  \n",
    "    \n",
    "    b4_up = keras.layers.UpSampling2D(2)(b4)\n",
    "    b3 = keras.layers.Add()([a3_0, b4_up])  \n",
    "    \n",
    "    b3_up = keras.layers.UpSampling2D(2)(b3)\n",
    "    b2 = keras.layers.Add()([a2_0, b3_up])\n",
    "    \n",
    "    b2_down = Conv2D(filters, 3, 2, \"same\", groups=1, activation=activation, kernel_regularizer=regulizer, name='b2_d')(b2)\n",
    "    b3_1 = Conv2D(filters, 1, 1, \"same\", groups=1, activation=activation, kernel_regularizer=regulizer, name='b3_1')(b3)    \n",
    "    c3 = keras.layers.Add()([a3_1, b3_1, b2_down])\n",
    "    \n",
    "    c3_down = Conv2D(filters, 3, 2, \"same\", groups=1, activation=activation, kernel_regularizer=regulizer, name='c3_d')(c3)\n",
    "    b4_1 = Conv2D(filters, 1, 1, \"same\", groups=1, activation=activation, kernel_regularizer=regulizer, name='b4_1')(b4)    \n",
    "    c4 = keras.layers.Add()([a4_1, b4_1, c3_down])    \n",
    "    \n",
    "    c4_down = Conv2D(filters, 3, 2, \"same\", groups=1, activation=activation, kernel_regularizer=regulizer, name='c4_d')(c4)\n",
    "    b5_1 = Conv2D(filters, 1, 1, \"same\", groups=1, activation=activation, kernel_regularizer=regulizer, name='b5_1')(b5)    \n",
    "    c5 = keras.layers.Add()([a5_1, b5_1, c4_down])    \n",
    "    \n",
    "    c5_down = Conv2D(filters, 3, 2, \"same\", groups=1, activation=activation, kernel_regularizer=regulizer, name='c5_d')(c5)\n",
    "    b6_1 = Conv2D(filters, 1, 1, \"same\", groups=1, activation=activation, kernel_regularizer=regulizer, name='b6_1')(b6)    \n",
    "    c6 = keras.layers.Add()([a6_1, b6_1, c5_down])\n",
    "    \n",
    "    return b2, c3, c4, c5, c6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape=(3, 3, 2))  # 18   \n",
    "outputs = Conv2D(10, 3)(inputs)# 18 * 10 + 10 = 190\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "backbone = keras.applications.EfficientNetB3(include_top=False, input_shape=[64, 64, 3])\n",
    "backbone.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_backbone():\n",
    "    backbone = keras.applications.EfficientNetB4(include_top=False, input_shape=[None, None, 3])\n",
    "    c2_output, c3_output, c4_output, c5_output = [\n",
    "        backbone.get_layer(layer_name).output\n",
    "        for layer_name in [\"block2d_add\", \"block3d_add\", \"block5f_add\", \"block6c_add\"]]#block5c_add, block6d_add\n",
    "    #c4_output = (c4_output + c4a_output[:, :, :, :80])/2\n",
    "    return keras.Model(\n",
    "        inputs=[backbone.inputs], outputs=[c2_output, c3_output, c4_output, c5_output]\n",
    "    )\n",
    "\n",
    "\n",
    "#D0 for layer_name in [\"block2b_add\", \"block3b_add\", \"block5c_add\", \"block6d_add\"]]\n",
    "#D7 for layer_name in [\"block2f_add\", \"block3g_add\", \"block5j_add\", \"block6d_add\"]]\n",
    "#input                           (None, 64, 64, 3)   \n",
    "#block2b_add (Add)               (None, 16, 16, 24) \n",
    "#block3b_add (Add)               (None, 8, 8, 40)    \n",
    "#block4c_add (Add)               (None, 4, 4, 80)\n",
    "#block5c_add (Add)               (None, 4, 4, 112) \n",
    "#block6d_add (Add)               (None, 2, 2, 192)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def get_backbone():\n",
    "    backbone = keras.applications.MobileNetV2(include_top=False, input_shape=[None, None, 3])\n",
    "    c3_output, c4_output, c5_output = [\n",
    "        backbone.get_layer(layer_name).output\n",
    "        for layer_name in [\"block_6_expand_relu\", \"block_13_expand_relu\", \"out_relu\"]]\n",
    "    return keras.Model(\n",
    "        inputs=[backbone.inputs], outputs=[c3_output, c4_output, c5_output]\n",
    "    )\n",
    "backbone = get_backbone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coordinate_map_uv(h, w):\n",
    "    #return (6, 18, 256)\n",
    "    x = tf.range(0.5, w, 1) / tf.cast(w, tf.float32) * 2.0 -1\n",
    "    y = tf.range(0.5, h, 1) / tf.cast(h, tf.float32) * 2.0 -1    \n",
    "    X, Y = tf.meshgrid(x, y)\n",
    "    xy = tf.stack((X, Y), -1)\n",
    "    xy = tf.expand_dims(xy, axis=0)   \n",
    "    return xy \n",
    "\n",
    "def coordinate_map(h, w):\n",
    "    #return (6, 18, 256)    \n",
    "    x = tf.range(0.0, w, 1) / tf.cast(w, tf.float32) * 2.0 -1\n",
    "    y = tf.linspace(0, 1, h)\n",
    "    y = tf.cast(y, tf.float32)\n",
    "    x = tf.zeros_like(x)#finetune\n",
    "    X, Y = tf.meshgrid(x, y)\n",
    "    xy = tf.stack((X, Y), -1)\n",
    "    xy = tf.expand_dims(xy, axis=0)   \n",
    "    return xy \n",
    "\n",
    "def add_map(net):\n",
    "    shape = tf.shape(net)\n",
    "    map_norm = coordinate_map(shape[1], shape[2])        \n",
    "    map_square = tf.sqrt(map_norm)\n",
    "    net = tf.concat((map_norm + net[:, :, :, :2], map_square + net[:, :, :, 2:4], net[:, :, :, 4:]), -1)    \n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.sqrt(coordinate_map(8, 3))[0,:,:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_pixel(net):\n",
    "    # net # (m, h, w, c)    \n",
    "    n0, n1, n2, n3 = tf.split(net, 4, -1)\n",
    "    n01 = tf.stack((n0, n1), 3) # (m, h, w, 2, c)\n",
    "    n23 = tf.stack((n2, n3), 3) # (m, h, w, 2, c)\n",
    "\n",
    "    n01234 = tf.stack((n01, n23), 2)# (m, h, 2, w, 2, c)\n",
    "    out = tf.reshape(n01234, [-1])#seg\n",
    "    return out\n",
    "\n",
    "\n",
    "def shuffle_pixel_2x(net, h, w, c):\n",
    "    # net # (m, h, w, c)\n",
    "    #n0, n1 = tf.split(net, 2, -1)\n",
    "    n0, n1, n2, n3 = tf.split(net, 4, -1)\n",
    "    n01 = tf.stack((n0, n1), 3) # (m, h, w, 2, c)\n",
    "    n23 = tf.stack((n2, n3), 3) # (m, h, w, 2, c)\n",
    "\n",
    "    n01234 = tf.stack((n01, n23), 2)# (m, h, 2, w, 2, c)\n",
    "    out = tf.reshape(n01234, [-1, h*2, w*2, c//4])\n",
    "    return out\n",
    "\n",
    "\n",
    "def shuffle_pixel_3x(net, h, w):\n",
    "    # net # (m, h, w, c)\n",
    "    #n0, n1 = tf.split(net, 2, -1)\n",
    "    n0, n1, n2, n3, n4, n5, n6, n7, n8 = tf.split(net, 9, -1)\n",
    "    r0 = tf.stack((n0, n4, n1), 3) # (m, h, w, 2, c)\n",
    "    r1 = tf.stack((n5, n6, n7), 3) # (m, h, w, 2, c)\n",
    "    r2 = tf.stack((n2, n8, n3), 3) # (m, h, w, 2, c)\n",
    "\n",
    "    r = tf.stack((r0, r1, r2), 2)# (m, h, 2, w, 2, c)\n",
    "    out = tf.reshape(r, [-1, h*3, w*3, 1])\n",
    "    return out\n",
    "\n",
    "\n",
    "def shuffle_pixel_4x(net):\n",
    "    # net # (m, h, w, c)\n",
    "    #n0, n1 = tf.split(net, 2, -1)\n",
    "    net_split = tf.split(net, 16, -1)\n",
    "    r0 = tf.stack(net_split[0:4], 3) # (m, h, w, 2, c)\n",
    "    r1 = tf.stack(net_split[4:8], 3) # (m, h, w, 2, c)\n",
    "    r2 = tf.stack(net_split[8:12], 3)\n",
    "    r3 = tf.stack(net_split[12:16], 3)\n",
    "\n",
    "    r = tf.stack((r0, r1, r2, r3), 2)# (m, h, 4, w, 2, c)\n",
    "    #out = tf.reshape(r, [-1, h*4, w*4, 1])\n",
    "    out = tf.reshape(r, [-1])\n",
    "    return out\n",
    "\n",
    "\n",
    "def shuffle_pixel_5x(net, h, w, c=1):\n",
    "    # net # (m, h, w, c)\n",
    "    k = 5\n",
    "    net_split = tf.split(net, k*k, -1)\n",
    "    r0 = tf.stack(net_split[k * 0:k * 1], 3)\n",
    "    r1 = tf.stack(net_split[k * 1:k * 2], 3)\n",
    "    r2 = tf.stack(net_split[k * 2:k * 3], 3)\n",
    "    r3 = tf.stack(net_split[k * 3:k * 4], 3)\n",
    "    r4 = tf.stack(net_split[k * 4:k * 5], 3)\n",
    "\n",
    "    r = tf.stack((r0, r1, r2, r3, r4), 2)\n",
    "    out = tf.reshape(r, [-1, h*k, w*k, c])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_segmap_basic(inputs, p3456):\n",
    "    p2 = p3456[0][:,:,:,:seg_k * 1]\n",
    "    p3 = p3456[1][:,:,:,:seg_k * 1]\n",
    "    p4 = p3456[2][:,:,:,:seg_k]\n",
    "    p5 = p3456[3][:,:,:,:seg_k]\n",
    "    p6 = p3456[4][:,:,:,:seg_k]\n",
    "    \n",
    "    #p2 = shuffle_pixel_2x(p2, tf.shape(inputs)[1]//4, tf.shape(inputs)[2]//4, seg_k*4)\n",
    "    #p3 = shuffle_pixel_2x(p3, tf.shape(inputs)[1]//8, tf.shape(inputs)[2]//8, seg_k*4)\n",
    "    \n",
    "    dst_size = (tf.shape(inputs)[1], tf.shape(inputs)[2])\n",
    "    p2 = tf.image.resize(p2, dst_size)\n",
    "    p3 = tf.image.resize(p3, dst_size)\n",
    "    p4 = tf.image.resize(p4, dst_size)\n",
    "    p5 = tf.image.resize(p5, dst_size)\n",
    "    p6 = tf.image.resize(p6, dst_size)\n",
    "    seg_prob = tf.nn.softmax(p2 + p3 + p4 + p5 + p6)\n",
    "    return seg_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_segmap(inputs, p3456):\n",
    "    p2 = p3456[0][:,:,:,:seg_k * 4]\n",
    "    p3 = p3456[1][:,:,:,:seg_k * 4]\n",
    "    p4 = p3456[2][:,:,:,:seg_k * 4]\n",
    "    p5 = p3456[3][:,:,:,:seg_k * 4]\n",
    "    p6 = p3456[4][:,:,:,:seg_k * 4]\n",
    "    \n",
    "    p2 = shuffle_pixel_2x(p2, tf.shape(inputs)[1]//4, tf.shape(inputs)[2]//4, seg_k*4)\n",
    "    p3 = shuffle_pixel_2x(p3, tf.shape(inputs)[1]//8, tf.shape(inputs)[2]//8, seg_k*4)\n",
    "    p4 = shuffle_pixel_2x(p4, tf.shape(inputs)[1]//16, tf.shape(inputs)[2]//16, seg_k*4)\n",
    "    p5 = shuffle_pixel_2x(p5, tf.shape(inputs)[1]//32, tf.shape(inputs)[2]//32, seg_k*4)\n",
    "    p6 = shuffle_pixel_2x(p6, tf.shape(inputs)[1]//64, tf.shape(inputs)[2]//64, seg_k*4)\n",
    "    \n",
    "    dst_size = (tf.shape(inputs)[1], tf.shape(inputs)[2])\n",
    "    p2 = tf.image.resize(p2, dst_size)\n",
    "    p3 = tf.image.resize(p3, dst_size)\n",
    "    p4 = tf.image.resize(p4, dst_size)\n",
    "    p5 = tf.image.resize(p5, dst_size)\n",
    "    p6 = tf.image.resize(p6, dst_size)\n",
    "    seg_prob = tf.nn.softmax(p2 + p3 + p4 + p5 + p6)\n",
    "    return seg_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createRetinaNet(num_classes, anchor_k, is_train=False):\n",
    "    prior_probability = tf.constant_initializer(-np.log((1 - 0.01) / 0.01))\n",
    "    inputs = Input(shape=(None, None, 3))       \n",
    "    \n",
    "    backbone = get_backbone()\n",
    "    backbone.trainable = True #finetune\n",
    "    nets_3 = backbone(inputs, training=is_train)            \n",
    "    p23456 = BifeaturePyramidNet(nets_3)    \n",
    "    \n",
    "    cls_outputs = []\n",
    "    box_outputs = []\n",
    "    \n",
    "    kernel_init = tf.initializers.he_normal()\n",
    "    regulizer = tf.keras.regularizers.L2(l1)\n",
    "    \n",
    "    filters = 5 + num_classes\n",
    "    conv_h0 = keras.layers.Conv2D(anchor_k * filters, 3, padding=\"same\", kernel_initializer=kernel_init, bias_initializer=prior_probability, kernel_regularizer=regulizer, name='head_0')   \n",
    "    conv_h1 = keras.layers.Conv2D(anchor_k * filters, 3, padding=\"same\", kernel_initializer=kernel_init, bias_initializer=prior_probability, kernel_regularizer=regulizer, name='head_1')   \n",
    "    conv_h2 = keras.layers.Conv2D(anchor_k * filters, 3, padding=\"same\", kernel_initializer=kernel_init, bias_initializer=prior_probability, kernel_regularizer=regulizer, name='head_2')\n",
    "    conv_kernels = [0, 0, conv_h0, conv_h1, conv_h2]\n",
    "    \n",
    "    drop = keras.layers.Dropout(0.00)\n",
    "    N = tf.shape(nets_3[0])[0]\n",
    "    \n",
    "    cbox_outputs = []    \n",
    "    feature_add_map = []\n",
    "    for i in range(len(p23456)):                     \n",
    "        feature_add_map.append(add_map(p23456[i]))\n",
    "        \n",
    "    for i in range(2, len(p23456)):            \n",
    "        feature = drop(feature_add_map[i])        \n",
    "        feature = add_map(feature)\n",
    "        conv_kernel = conv_kernels[i]\n",
    "        cls_out = conv_kernel(feature)        \n",
    "        cbox_out = tf.reshape(cls_out, [N, -1, filters])\n",
    "        cbox_outputs.append(cbox_out[:,:,:])\n",
    "  \n",
    "    bbox = tf.concat(cbox_outputs, axis=1)\n",
    "    segmap = get_segmap(inputs, p23456)\n",
    "    \n",
    "    outputs = {'detect':bbox, 'segment':segmap}#[outputs, h_mask]\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)#dual    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _decode_box_predictions(anchor_boxes, box_predictions):\n",
    "    _box_variance = tf.convert_to_tensor([0.1, 0.1, 0.2, 0.2], dtype=tf.float32)\n",
    "    boxes = box_predictions * _box_variance\n",
    "    boxes = tf.concat(\n",
    "        [\n",
    "            boxes[:, :, :2] * anchor_boxes[:, :, 2:] + anchor_boxes[:, :, :2],\n",
    "            tf.math.exp(boxes[:, :, 2:]) * anchor_boxes[:, :, 2:],\n",
    "        ],\n",
    "        axis=-1,\n",
    "    )\n",
    "    boxes_transformed = convert_to_corners(boxes)\n",
    "    return boxes_transformed\n",
    "\n",
    "def decodePredictions(images, predictions, \n",
    "                      num_classes=num_classes,\n",
    "                      confidence_threshold=0.5,\n",
    "                      nms_iou_threshold=0.2,\n",
    "                      max_detections_per_class=1000,\n",
    "                      max_detections=1500,\n",
    "                      box_variance=[0.1, 0.1, 0.2, 0.2]):\n",
    "    \n",
    "    _anchor_box = AnchorBox()\n",
    "        \n",
    "    image_shape = tf.cast(tf.shape(images), dtype=tf.float32)\n",
    "    image_h = padded_image_shape[0]\n",
    "    image_w = padded_image_shape[1]\n",
    "    anchor_boxes = _anchor_box.get_anchors(image_shape[1], image_shape[2])#free size    \n",
    "    #anchor_boxes = _anchor_box.get_anchors(image_h, image_w)\n",
    "    box_predictions = predictions[:, :, :4]\n",
    "    objectness = tf.nn.sigmoid(predictions[:, :, 4:5])\n",
    "    cls_score = predictions[:, :, 5:5+num_classes_real]\n",
    "    cls_prob = tf.nn.softmax(cls_score)\n",
    "    #cls_prob_obj = 1.0 - cls_prob[:, :, 0]\n",
    "    #objectness = tf.sqrt(objectness * (1.0 - cls_prob[:, :, 0]))\n",
    "    cls_prob_max = tf.reduce_max(cls_prob, -1)\n",
    "    \n",
    "    cls = tf.argmax(cls_score, -1)\n",
    "    cls = tf.cast(cls, tf.float32)\n",
    "    \n",
    "    boxes = _decode_box_predictions(anchor_boxes[None, ...], box_predictions)\n",
    "    boxes_2d = tf.reshape(boxes, [-1, 4])    \n",
    "    scores = tf.reshape(objectness, [-1, 1])#new\n",
    "    \n",
    "    cls = tf.reshape(cls, [-1, 1])\n",
    "    ccbox = tf.concat((cls, scores, boxes_2d), -1)\n",
    "    \n",
    "    selected_indices, selected_scores = tf.image.non_max_suppression_with_scores(    \n",
    "        ccbox[:, 2:],\n",
    "        ccbox[:, 1],        \n",
    "        max_detections,\n",
    "        nms_iou_threshold,\n",
    "        confidence_threshold,        \n",
    "    )\n",
    "    output = tf.gather(ccbox, selected_indices)\n",
    "    output = tf.boolean_mask(output, output[:, 0] < 4) # Freeze\n",
    "    #cls_h = detected_box[:, 0].astype(np.int)\n",
    "    #scores = detected_box[:, 1]\n",
    "    #box = detected_box[:, 2:]\n",
    "    return output   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenderLoss(tf.losses.Loss):\n",
    "    \"\"\"Implements Focal loss\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(GenderLoss, self).__init__(\n",
    "            reduction=\"none\", name=\"GenderLoss\"\n",
    "        )\n",
    "        self._alpha = 0.5\n",
    "        self._gamma = 1.0\n",
    "        \n",
    "    def call(self, y_cls, y_pred):\n",
    "        #y : 0:unknown, 1, 2        \n",
    "        y_positive = tf.cast(y_cls > 0, tf.float32)\n",
    "        y_cls = tf.cast(y_cls - 1, dtype=tf.int32)\n",
    "        y_cls = tf.maximum(y_cls, 0)\n",
    "        y_hot = tf.one_hot(y_cls, depth=2, dtype=tf.float32,)\n",
    "        \n",
    "        pt = tf.nn.sigmoid(y_pred)        \n",
    "        pt = tf.clip_by_value(pt, 1e-7, 1.0 - 1e-7)\n",
    "                \n",
    "        loss_p = - (1.0 - self._alpha) * tf.pow(1.0 - pt, self._gamma) * y_positive * tf.math.log(pt)        \n",
    "        loss_f = - self._alpha * tf.pow(pt, self._gamma) * (1 - y_positive) * tf.math.log(1 - pt)\n",
    "        loss_obj = y_positive * (loss_p + loss_f)\n",
    "        return loss_obj\n",
    "\n",
    "class BodyLoss(tf.losses.Loss):\n",
    "    def __init__(self):\n",
    "        super(BodyLoss, self).__init__(reduction=\"none\", name=\"BodyLoss\")\n",
    "        self._alpha = 0.5\n",
    "        self._gamma = 3.0\n",
    "\n",
    "    def call(self, y_cls, y_pred):\n",
    "        #y : 0:unknown, 1:thin, 2:medium, 3:fat         \n",
    "        y_positive = tf.cast(y_cls > 0, tf.float32)\n",
    "        y_cls = tf.cast(y_cls - 1, dtype=tf.int32)\n",
    "        y_cls = tf.maximum(y_cls, 0)\n",
    "        y_hot = tf.one_hot(y_cls, depth=body_volumn_k, dtype=tf.float32,)\n",
    "\n",
    "        pt = tf.nn.softmax(y_pred)        \n",
    "        pt = tf.clip_by_value(pt, 1e-7, 1.0 - 1e-7)\n",
    "\n",
    "        loss_p = - (1.0 - self._alpha) * tf.pow(1.0 - pt, self._gamma) * y_hot * tf.math.log(pt)        \n",
    "        loss_f = - self._alpha * tf.pow(pt, self._gamma) * (1 - y_hot) * tf.math.log(1 - pt)\n",
    "        loss_obj = y_positive * tf.reduce_sum(loss_p + loss_f, -1)\n",
    "        return loss_obj\n",
    "\n",
    "class UpperClothLoss(tf.losses.Loss):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(UpperClothLoss, self).__init__(reduction=\"none\", name=\"UpperClothLoss\")\n",
    "        self._alpha = 0.5\n",
    "        self._gamma = 3.0\n",
    "\n",
    "    def call(self, y_cls, y_pred):\n",
    "        #y : 0:unknown, 1:thin, 2:medium, 3:fat         \n",
    "        y_positive = tf.cast(y_cls > 0, tf.float32)        \n",
    "        y_cls = tf.cast(y_cls - 1, dtype=tf.int32)\n",
    "        y_cls = tf.maximum(y_cls, 0)\n",
    "        y_hot = tf.one_hot(y_cls, depth=upper_cloth_k, dtype=tf.float32,)\n",
    "\n",
    "        pt = tf.nn.softmax(y_pred)        \n",
    "        pt = tf.clip_by_value(pt, 1e-7, 1.0 - 1e-7)\n",
    "\n",
    "        loss_p = - (1.0 - self._alpha) * tf.pow(1.0 - pt, self._gamma) * y_hot * tf.math.log(pt)        \n",
    "        loss_f = - self._alpha * tf.pow(pt, self._gamma) * (1 - y_hot) * tf.math.log(1 - pt)\n",
    "        loss_obj = y_positive * tf.reduce_sum(loss_p + loss_f, -1)\n",
    "        return loss_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetinaNetBoxLoss(tf.losses.Loss):\n",
    "    \"\"\"Implements Smooth L1 loss\"\"\"\n",
    "\n",
    "    def __init__(self, delta):\n",
    "        super(RetinaNetBoxLoss, self).__init__(\n",
    "            reduction=\"none\", name=\"RetinaNetBoxLoss\"\n",
    "        )\n",
    "        self._delta = delta\n",
    "\n",
    "    def call(self, y_true, y_pred):    \n",
    "        \n",
    "        difference = y_true - y_pred\n",
    "        absolute_difference = tf.abs(difference)        \n",
    "        loss = tf.where(\n",
    "            tf.less(absolute_difference, self._delta),\n",
    "            0.5 * (difference ** 2),\n",
    "            absolute_difference - 0.5,\n",
    "        )\n",
    "        loss = tf.where(loss < 0.02, 0.0, loss)#new marginal loss        \n",
    "        return tf.reduce_sum(loss, axis=-1)\n",
    "\n",
    "\n",
    "class RetinaNetClassificationLoss(tf.losses.Loss):\n",
    "    \"\"\"Implements Focal loss\"\"\"\n",
    "\n",
    "    def __init__(self, alpha, gamma, num_classes):\n",
    "        super(RetinaNetClassificationLoss, self).__init__(\n",
    "            reduction=\"none\", name=\"RetinaNetClassificationLoss\"\n",
    "        )\n",
    "        self._alpha = alpha\n",
    "        self._gamma = gamma\n",
    "        self._num_classes = num_classes\n",
    "        print('RetinaNetClassificationLoss', num_classes)\n",
    "        \n",
    "    def call(self, y_cls, y_pred):\n",
    "        y_cls_int = tf.cast(y_cls, dtype=tf.int32)\n",
    "        y_hot = tf.one_hot(y_cls_int, depth=self._num_classes, dtype=tf.float32,)\n",
    "        \n",
    "        y_positive = tf.cast(y_cls > 0, tf.float32)#finetune, 1:unknown\n",
    "        y_positive_identity = tf.cast(y_cls > 1, tf.float32)# 1:unknown\n",
    "        \n",
    "        obj_score = tf.identity(y_pred[:, :, 0], name='obj_score')\n",
    "        cls_score = y_pred[:, :, 1:1+self._num_classes]\n",
    "        h_cls_num = tf.argmax(cls_score, -1)\n",
    "        \n",
    "        pt = tf.nn.sigmoid(obj_score)        \n",
    "        pt = tf.clip_by_value(pt, 1e-7, 1.0 - 1e-7)\n",
    "        \n",
    "        car_exist_sample = tf.reduce_any(y_cls > 3, axis=1, keepdims=True)\n",
    "        k_positive = tf.reduce_sum(tf.cast(y_cls > 0, tf.float32), -1, True)\n",
    "        mask_ignore_cond0 = tf.logical_and(y_cls_int == 0, pt >= 0.5)\n",
    "        mask_ignore_cond1 = tf.logical_and(h_cls_num > 3, k_positive < 60)\n",
    "        mask_ignore_loss = tf.logical_and(mask_ignore_cond0, mask_ignore_cond1)\n",
    "        mask_ignore_loss = tf.logical_and(tf.logical_not(car_exist_sample), mask_ignore_loss)\n",
    "                        \n",
    "        loss_p = - (1.0 - self._alpha) * tf.pow(1.0 - pt, self._gamma-0.5) * y_positive * tf.math.log(pt)        \n",
    "        loss_f = - self._alpha * tf.pow(pt, self._gamma-0.5) * (1 - y_positive) * tf.math.log(1 - pt)\n",
    "        loss_obj = loss_p + loss_f \n",
    "      \n",
    "        cls_pt = tf.nn.softmax(cls_score)        \n",
    "        cls_pt = tf.clip_by_value(cls_pt, 1e-7, 1.0 - 1e-7)\n",
    "        loss_cls_p = - tf.pow(1.0 - cls_pt, self._gamma) * y_hot * tf.math.log(cls_pt)\n",
    "        loss_cls_f = - tf.pow(cls_pt, self._gamma) * (1 - y_hot) * tf.math.log(1 - cls_pt)\n",
    "        loss_cls = tf.reduce_sum(loss_cls_p + loss_cls_f, axis=-1)\n",
    "        #loss_cls_no_label_car = tf.reduce_sum(-tf.math.log(cls_pt[:, :, 1:4]), -1)\n",
    "        loss_cls_no_label_car = tf.reduce_sum(cls_pt[:, :, 1:4], -1)\n",
    "                        \n",
    "        normalizer = tf.reduce_sum(y_positive_identity, axis=-1, keepdims=True)\n",
    "        loss_cls = tf.math.divide_no_nan(loss_cls, normalizer)        \n",
    "        y_cls_positive = tf.boolean_mask(y_cls, y_cls > 0)\n",
    "        is_various_cls_exist = tf.cast(tf.math.reduce_std(y_cls_positive) > 0, tf.float32)        \n",
    "                        \n",
    "        loss = loss_obj + y_positive * is_various_cls_exist * loss_cls * 10\n",
    "        #loss = loss_obj + y_positive * loss_cls\n",
    "        loss_uncertain_car = cls_pt[:, :, 1] + cls_pt[:, :, 3]\n",
    "        loss = tf.where(mask_ignore_loss, loss_uncertain_car, loss)                \n",
    "        \n",
    "        loss_bg_cls = tf.cast(y_cls_int == 0, tf.float32) * is_various_cls_exist * cls_pt[:, :, 1]\n",
    "        loss += loss_bg_cls#try\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_k, body_volumn_k, upper_cloth_k, num_classes_real, (gender_k+body_volumn_k+ upper_cloth_k+num_classes_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetinaNetLoss(tf.losses.Loss):\n",
    "    \"\"\"Wrapper to combine both the losses\"\"\"\n",
    "\n",
    "    def __init__(self, num_classes=80, alpha=0.3, gamma=3.0, delta=1.0):#alpha=0.25\n",
    "        super(RetinaNetLoss, self).__init__(reduction=\"auto\", name=\"RetinaNetLoss\")\n",
    "        self._clf_loss = RetinaNetClassificationLoss(alpha, gamma, num_classes_real)\n",
    "        self._box_loss = RetinaNetBoxLoss(delta)\n",
    "        self._gender_loss = GenderLoss()\n",
    "        self._body_loss = BodyLoss()\n",
    "        self._upper_cloth_loss = UpperClothLoss()\n",
    "        self._num_classes = num_classes\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        # y_pred : tf.concat([box_outputs, cls_outputs], axis=-1)\n",
    "        #y_pred = tf.cast(y_pred, dtype=tf.float32)\n",
    "        \n",
    "        y_box = y_true[:, :, :4]\n",
    "        y_cls = y_true[:, :, 4]\n",
    "        y_width_norm = y_true[:, :, 5]#1~2\n",
    "        y_gender = y_true[:, :, 6]\n",
    "        y_body = y_true[:, :, 7]\n",
    "        y_upper_cloth = y_true[:, :, 8]\n",
    "        \n",
    "        h_box = y_pred[:, :, :4]\n",
    "        h_obj = tf.nn.sigmoid(y_pred[:, :, 4])        \n",
    "        h_cls = y_pred[:, :, 4:4+1+num_classes_real]        \n",
    "        h_gender = y_pred[:, :, -1]#k2\n",
    "        h_body = y_pred[:, :, -body_volumn_k-1:-1]#k3\n",
    "        h_upper_cloth = y_pred[:, :, -upper_cloth_k-body_volumn_k-1:-body_volumn_k-1]\n",
    "      \n",
    "        positive_mask = tf.greater(y_cls, 0.0)\n",
    "        ignore_mask = tf.less(y_cls, 0.0)\n",
    "        \n",
    "        clf_loss = self._clf_loss(y_cls, h_cls)\n",
    "        box_loss = self._box_loss(y_box, h_box) \n",
    "        if False:\n",
    "            gender_loss = self._gender_loss(y_gender, h_gender)\n",
    "            body_loss = self._body_loss(y_body, h_body)\n",
    "            upper_cloth_loss = self._upper_cloth_loss(y_upper_cloth, h_upper_cloth)\n",
    "            pass\n",
    "        \n",
    "        attribute_loss = box_loss\n",
    "        if False:\n",
    "            attribute_loss = box_loss + .1 * (gender_loss + body_loss + upper_cloth_loss)\n",
    "            pass\n",
    "                \n",
    "        clf_loss = tf.where(ignore_mask, 0.0, clf_loss)        \n",
    "        attribute_loss = tf.where(positive_mask, attribute_loss, 0.0)\n",
    "                \n",
    "        positive_mask = tf.cast(positive_mask, tf.float32)        \n",
    "        normalizer = tf.sqrt(tf.reduce_sum(positive_mask, -1))+1\n",
    "                \n",
    "        loss = y_width_norm * (1 + tf.cast(y_cls > 1, tf.float32)) * (clf_loss + attribute_loss)\n",
    "        loss = loss * tf.cast(tf.reduce_any(y_cls > 0, 1, True) ,tf.float32)\n",
    "        #loss = tf.where(tf.reduce_any(y_cls > 0, axis=1), loss, loss*0)\n",
    "        \n",
    "        loss = tf.math.divide_no_nan(tf.reduce_sum(loss, axis=-1), normalizer) \n",
    "        \n",
    "        #print('Base Loss', loss)\n",
    "        loss = tf.reduce_mean(loss)\n",
    "        return loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_aeroscapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_person = tf.one_hot(class_aeroscapes.index('person'), seg_k)\n",
    "weight_bike = tf.one_hot(class_aeroscapes.index('bike'), seg_k)        \n",
    "weight_cls = 1 + 2 * weight_person + weight_bike\n",
    "weight_cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "h = 7\n",
    "sample_person_mask = np.zeros((1, h, h, 1), np.float32)\n",
    "filter_mask = np.ones((3, 3, 1), np.float32)\n",
    "sample_person_mask[0, 3, 3] = 1\n",
    "dilations = [1, 1, 1, 1]\n",
    "morp_out = tf.nn.dilation2d(sample_person_mask, filter_mask, strides=[1,1,1,1], padding='SAME', data_format='NHWC', dilations=dilations)\n",
    "tf.reshape(morp_out, [h, h])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_aeroscapes.index('person'), class_aeroscapes.index('water')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentLoss(tf.losses.Loss):\n",
    "   \n",
    "    def __init__(self, k):\n",
    "        super(SegmentLoss, self).__init__(reduction=\"none\", name=\"SegmentLoss\")\n",
    "        self.k = k \n",
    "        self._gamma = 2.0\n",
    "        weight_person = tf.one_hot(class_aeroscapes.index('person'), seg_k)\n",
    "        weight_bike = tf.one_hot(class_aeroscapes.index('bike'), seg_k)\n",
    "        weight_car = tf.one_hot(class_aeroscapes.index('car'), seg_k)\n",
    "        weight_obstacle = tf.one_hot(class_aeroscapes.index('obstacle'), seg_k)\n",
    "        \n",
    "        weight_cls = 1 + 5 * weight_person + 2 * weight_bike + 1 * weight_car + weight_obstacle\n",
    "        self.weight_cls = tf.reshape(weight_cls, [-1, 1, 1, seg_k])\n",
    "        \n",
    "        self.filter = np.ones((15, 15, 1), np.float32)        \n",
    "        self.dilations = [1, 1, 1, 1]\n",
    "        self.person_index = class_aeroscapes.index('person')\n",
    "        \n",
    "    def call(self, y, h):\n",
    "        \n",
    "        y_hot = tf.one_hot(y, self.k)\n",
    "        cls_pt = tf.clip_by_value(h, 1e-6, 1.0 - 1e-6)\n",
    "        loss_p = - tf.pow(1.0 - cls_pt, self._gamma) * y_hot * tf.math.log(cls_pt)\n",
    "        loss_f = - tf.pow(cls_pt, self._gamma) * (1 - y_hot) * tf.math.log(1 - cls_pt)\n",
    "        loss_pf = loss_p + loss_f\n",
    "        loss = tf.reduce_sum(self.weight_cls * loss_pf, axis=-1)\n",
    "        \n",
    "        binary_person = tf.cast(tf.equal(y, self.person_index), tf.float32)\n",
    "        binary_person = tf.expand_dims(binary_person, -1)\n",
    "        dilate_person = tf.nn.dilation2d(binary_person, self.filter, strides=[1,1,1,1], padding='SAME', data_format='NHWC', dilations=dilations)\n",
    "        \n",
    "        loss *= tf.square(tf.squeeze(dilate_person, -1))\n",
    "        \n",
    "        #loss = tf.where(y > 0, loss, 0.0)\n",
    "        loss = tf.reduce_mean(loss, [1, 2])\n",
    "        cond = tf.reduce_any(y > 0, axis=[1, 2])\n",
    "        index_water = class_aeroscapes.index('water')\n",
    "        cond_water = tf.reduce_any(y >= index_water, axis=[1, 2])\n",
    "        \n",
    "        loss = tf.where(cond, loss, 0.0)\n",
    "        loss = tf.where(cond_water, 2 * loss, loss)\n",
    "        #print('Segment Loss', loss)\n",
    "        loss = tf.reduce_mean(loss)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(y_true, y_pred):    \n",
    "    y_cls = tf.cast(y_true[:, :, 4], tf.int32)\n",
    "    y_positive = tf.cast(y_cls > 0, tf.int32)\n",
    "    y_bg = tf.cast(tf.abs(y_cls)==0, tf.int32)\n",
    "    h_score = y_pred[:, :, 4]\n",
    "    h_prob = tf.nn.sigmoid(h_score)    \n",
    "    h_postive = tf.cast(tf.round(h_prob), tf.int32)\n",
    "    \n",
    "    true_positives = tf.cast(tf.logical_and(y_cls > 0, h_postive>0), tf.float32)\n",
    "    false_negative = y_positive * (1 - h_postive)\n",
    "                \n",
    "    tp = tf.reduce_sum(true_positives, axis=1)# + 0.01\n",
    "    fn = tf.reduce_sum(false_negative, axis=1)\n",
    "    tp = tf.cast(tp, tf.float32)\n",
    "    fn = tf.cast(fn, tf.float32)\n",
    "    \n",
    "    rec = tp / (tp + fn + 1e-8)\n",
    "    return rec\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    \n",
    "    y_cls_symbol = tf.cast(y_true[:, :, 4], dtype=tf.int32)    \n",
    "    y_cls_symbol = tf.cast(y_cls_symbol != 0, tf.int32)\n",
    "    h_obj_prob = tf.nn.sigmoid(y_pred[:, :, 4])\n",
    "    h_cls_symbol = tf.round(h_obj_prob)    \n",
    "    h_cls_symbol = tf.cast(h_cls_symbol, tf.int32)\n",
    "    \n",
    "    true_positives = y_cls_symbol * h_cls_symbol\n",
    "    false_positive = (1 - y_cls_symbol) * h_cls_symbol\n",
    "    \n",
    "    ones = tf.ones_like(true_positives)\n",
    "    zeeros = tf.zeros_like(true_positives)\n",
    "    true_positives = tf.cast(tf.equal(true_positives, ones), tf.float32)\n",
    "    false_positive = tf.cast(tf.equal(false_positive, ones), tf.float32)\n",
    "    \n",
    "    tp = tf.reduce_sum(true_positives, axis=1)# + 0.01\n",
    "    fp = tf.reduce_sum(false_positive, axis=1)\n",
    "    tp = tf.cast(tp, tf.float32)\n",
    "    fp = tf.cast(fp, tf.float32)\n",
    "    prec = tp / (tp + fp + 1e-8)\n",
    "    return prec\n",
    "\n",
    "def acc(y_true, y_pred):    \n",
    "    y_cls = tf.cast(y_true[:, :, 4], tf.int32)\n",
    "    y_positive = y_cls > 0    \n",
    "    h_cls = tf.math.argmax(y_pred[:, :, 5:5+num_classes_real], -1, output_type=tf.int32)        \n",
    "    acc = tf.boolean_mask(tf.equal(y_cls, h_cls), y_positive)    \n",
    "    #acc = tf.equal(y_cls, h_cls)\n",
    "    return acc\n",
    "\n",
    "def seg_acc(y, h):\n",
    "    y = tf.cast(y, tf.int32)\n",
    "    h_cls = tf.math.argmax(h, -1, output_type=tf.int32)        \n",
    "    acc = tf.equal(y, h_cls)\n",
    "    return acc\n",
    "\n",
    "def gender_acc(y_true, y_pred):    \n",
    "    y_cls = tf.cast(y_true[:, :, 4], tf.int32)\n",
    "    y_gender = tf.cast(y_true[:, :, 5], tf.int32)\n",
    "    y_positive = tf.logical_and(y_cls > 0, y_gender > 0)\n",
    "    \n",
    "    y_gender = tf.maximum(y_gender - 1, 0)    \n",
    "    h_prob = tf.nn.sigmoid(y_pred[:, :, -1])\n",
    "    h = tf.cast(tf.round(h_prob), tf.int32)    \n",
    "    acc = tf.boolean_mask(tf.equal(y_gender, h), y_positive)        \n",
    "    return acc\n",
    "\n",
    "def body_acc(y_true, y_pred):    \n",
    "    y_cls = tf.cast(y_true[:, :, 4], tf.int32)\n",
    "    y_gender = tf.cast(y_true[:, :, 6], tf.int32)\n",
    "    y_positive = tf.logical_and(y_cls > 0, y_gender > 0)\n",
    "    \n",
    "    y_gender = tf.maximum(y_gender - 1, 0)    \n",
    "    h_prob = tf.nn.softmax(y_pred[:, :, -3-1:-1])\n",
    "    h = tf.cast(tf.argmax(h_prob, -1), tf.int32)    \n",
    "    acc = tf.boolean_mask(tf.equal(y_gender, h), y_positive)        \n",
    "    return acc\n",
    "\n",
    "def up_cloth_acc(y_true, y_pred):    \n",
    "    y_cls = tf.cast(y_true[:, :, 4], tf.int32)\n",
    "    y_gender = tf.cast(y_true[:, :, 7], tf.int32)\n",
    "    y_positive = tf.logical_and(y_cls > 0, y_gender > 0)\n",
    "    \n",
    "    y_gender = tf.maximum(y_gender - 1, 0)\n",
    "    \n",
    "    h_score = y_pred[:, :, -upper_cloth_k-body_volumn_k-1:-body_volumn_k-1]    \n",
    "    h_prob = tf.nn.softmax(h_score)\n",
    "    h = tf.cast(tf.argmax(h_prob, -1), tf.int32)    \n",
    "    acc = tf.boolean_mask(tf.equal(y_gender, h), y_positive)        \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "anchor_k = len(label_encoder._anchor_box.aspect_ratios)*len(label_encoder._anchor_box.scales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bbox_list_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(bbox_list_train), np.concatenate(bbox_list_train, 0).shape\n",
    "cls = np.concatenate(bbox_list_train, 0)[:, 4]\n",
    "print('cls', np.unique(cls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "is_rotate_aug = True\n",
    "def generator():    \n",
    "    for i in range(0, (len(input_list_train)//2)*2):\n",
    "        x = input_list_train[i]\n",
    "        y_box = bbox_list_train[i]\n",
    "        y_seg = seg_list_train[i]\n",
    "        \n",
    "        if y_seg.ndim < 2:\n",
    "            y_seg = np.zeros_like(x[:, :, 0], np.uint8)\n",
    "        if is_rotate_aug:            \n",
    "            angle = 2 * (np.random.rand() - 0.5) #0~1 > -0.5~0.5\n",
    "            if np.abs(angle) > 0.1:                \n",
    "                x_rotated = Image.fromarray(x).rotate(angle)\n",
    "                y_seg_rotated = Image.fromarray(y_seg).rotate(angle, Image.NEAREST)\n",
    "                x = np.array(x_rotated)\n",
    "                y_seg = np.array(y_seg_rotated)\n",
    "\n",
    "                attributes = y_box[:, 4:]        \n",
    "                box = y_box[:, :4]\n",
    "                img_h, img_w, img_c = x.shape\n",
    "                box_trans = rotate_box(box, angle, img_h, img_w)            \n",
    "                y_box = np.concatenate((box_trans, attributes), -1)\n",
    "            \n",
    "        if y_box.shape[1] < label_attribute_k:\n",
    "            z = np.zeros_like(y_box[:, :label_attribute_k - y_box.shape[1]])\n",
    "            y_box = np.concatenate((y_box, z), -1)\n",
    "        yield (x, y_seg, y_box)\n",
    "\n",
    "def generator_test():    \n",
    "    for i in range(len(input_list_test)):\n",
    "        x = input_list_test[i]\n",
    "        y_box = bbox_list_test[i]\n",
    "        y_seg = np.zeros_like(x[:, :, 0], np.uint8)\n",
    "        if y_box.shape[1] < label_attribute_k:\n",
    "            z = np.zeros_like(y_box[:, :label_attribute_k - y_box.shape[1]])\n",
    "            y_box = np.concatenate((y_box, z), -1)\n",
    "        yield (x, y_seg, y_box)\n",
    "\n",
    "dataset = tf.data.Dataset.from_generator(\n",
    "    generator, \n",
    "    output_types=(tf.uint8, tf.uint8, tf.float32), \n",
    "    output_shapes=(tf.TensorShape([None, None, 3]), tf.TensorShape([None, None]), tf.TensorShape([None, label_attribute_k])))\n",
    "dataset_test = tf.data.Dataset.from_generator(\n",
    "    generator_test, \n",
    "    output_types=(tf.uint8, tf.uint8, tf.float32), \n",
    "    output_shapes=(tf.TensorShape([None, None, 3]), tf.TensorShape([None, None]), tf.TensorShape([None, label_attribute_k])))\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "for example in tfds.as_numpy(dataset):\n",
    "    image = example[0]\n",
    "    seg = example[1]\n",
    "    bbox = example[2]\n",
    "    \n",
    "    print(image.dtype, image.shape, seg.shape, bbox.shape, bbox[0])\n",
    "    plt.imshow(seg)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2# * 5 #finetune 1 or 4\n",
    "autotune = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_dataset = dataset.map(preprocess_data, num_parallel_calls=autotune)\n",
    "#train_dataset = train_dataset.shuffle(8 * batch_size)\n",
    "train_dataset = train_dataset.padded_batch(batch_size=batch_size, padding_values=(0.0, 1e-8, -1), drop_remainder=False)\n",
    "#train_dataset = train_dataset.padded_batch(batch_size=batch_size)\n",
    "train_dataset = train_dataset.map(label_encoder.encode_batch_train, num_parallel_calls=autotune)\n",
    "train_dataset = train_dataset.prefetch(autotune)\n",
    "\n",
    "#output_map (2, 1024, 1920, 3) (2, 1024, 1920, 1) (2, 60480, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = dataset_test.map(preprocess_test_data, num_parallel_calls=autotune)\n",
    "val_dataset = val_dataset.padded_batch(batch_size=batch_size, padding_values=(0.0, 1e-8, -1), drop_remainder=False)\n",
    "#val_dataset = val_dataset.padded_batch(batch_size=batch_size)\n",
    "val_dataset = val_dataset.map(label_encoder.encode_batch, num_parallel_calls=autotune)\n",
    "val_dataset = val_dataset.prefetch(autotune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=3, linewidth=200)\n",
    "image_height, image_width = padded_image_shape\n",
    "\n",
    "img_check = 0\n",
    "for image, output_map in train_dataset:\n",
    "    cbbox = output_map[\"detect\"]\n",
    "    segmap = output_map[\"segment\"]    \n",
    "    img_check+=1\n",
    "    if img_check < 3:\n",
    "        continue\n",
    "    print('output_map', image.shape, segmap.shape, cbbox.shape)\n",
    "    \n",
    "    bbox = cbbox[:, :, :4]\n",
    "    cls_gt = cbbox[:,:,4]\n",
    "    img_m, image_height, image_width, image_ch = image.shape\n",
    "    anchor_feature_size = [(np.ceil(image_height / 2 ** i), np.ceil(image_width / 2 ** i)) \n",
    "                           for i in range(level_start, level_end)]\n",
    "    print('anchor_feature_size', anchor_feature_size)    \n",
    "    m = len(cbbox)    \n",
    "    positive_count = np.sum(cls_gt > 0)\n",
    "    print('cbbox', cbbox.shape)\n",
    "    print('cls_sum',np.sum(cls_gt < 0.0), np.sum(cls_gt == 0.0), \n",
    "          np.sum(cls_gt == 1.0), np.sum(cls_gt > 1.0))\n",
    "    print('cls_mean',np.mean(cls_gt < 0.0), np.mean(cls_gt == 0.0), \n",
    "          np.mean(cls_gt == 1.0), np.mean(cls_gt > 0.0))\n",
    "    print('shape',image.shape, cbbox.shape,'unique', np.unique(cls_gt))\n",
    "    print('anchor_feature_size', anchor_feature_size)\n",
    "    offset = 0\n",
    "    positive_maps = []\n",
    "    for anchor_feature_size_1 in anchor_feature_size:        \n",
    "        fm_h, fm_w = anchor_feature_size_1\n",
    "        fm_h = int(fm_h)\n",
    "        fm_w = int(fm_w)        \n",
    "        fm_wh = int(fm_h * fm_w * anchor_k)\n",
    "        cbbox_anchor = cbbox[:, offset:offset+fm_wh, 4]\n",
    "        cbbox_anchor = np.reshape(cbbox_anchor, [m, fm_h, fm_w, anchor_k])\n",
    "        coount_m1 = np.count_nonzero(cbbox_anchor==-1)\n",
    "        coount_0 = np.count_nonzero(cbbox_anchor==0)\n",
    "        coount_1 = np.count_nonzero(cbbox_anchor==1)\n",
    "        coount_1_over = np.count_nonzero(cbbox_anchor>1)\n",
    "        positive_ratio = np.mean(cbbox_anchor>0)\n",
    "        positive_maps.append(cbbox_anchor>0)\n",
    "        print('cbbox_anchor', cbbox_anchor.shape, coount_m1, coount_0, coount_1, coount_1_over, 'ratio', positive_ratio)\n",
    "        sample_0_cbbox = cbbox_anchor[0]\n",
    "        sample_0_cbbox_sum = np.max(sample_0_cbbox, -1).astype(np.int)       \n",
    "      \n",
    "        offset += fm_wh\n",
    "        if False:            \n",
    "            file_name = str(fm_h)+ '_' + str(fm_w)+ '.txt'\n",
    "            np.savetxt(file_name,sample_0_cbbox_sum, fmt='%d',delimiter='')\n",
    "    img_check = image\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(np.max(positive_maps[0][0], -1))\n",
    "plt.title(str(positive_maps[0].shape)+ str(np.mean(positive_maps[0][0]))+ ' ' + str(np.sum(positive_maps[0][0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmap0 = np.array(Image.fromarray(np.max(positive_maps[0][0],-1)).resize((image_width, image_height)))\n",
    "pmap1 = np.array(Image.fromarray(np.max(positive_maps[1][0],-1)).resize((image_width, image_height)))\n",
    "pmap2 = np.array(Image.fromarray(np.max(positive_maps[2][0],-1)).resize((image_width, image_height)))\n",
    "#pmap3 = np.array(Image.fromarray(np.max(positive_maps[3][0],-1)).resize((image_width, image_height)))\n",
    "#pmap4 = np.array(Image.fromarray(np.max(positive_maps[4][0],-1)).resize((image_width, image_height)))\n",
    "pmap0 = pmap0.astype(np.uint8)\n",
    "pmap1 = pmap1.astype(np.uint8)\n",
    "pmap2 = pmap2.astype(np.uint8)\n",
    "pmap3 = 0#pmap3.astype(np.uint8)\n",
    "pmap4 = 0#pmap4.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pmap_with_img = np.array(img_check)[0]#*255\n",
    "pmap_with_img = pmap_with_img.astype(np.uint8)\n",
    "pmap_add = np.expand_dims(pmap0+pmap1+pmap2+pmap3+pmap4, -1)\n",
    "pmap = (pmap_add>0).astype(np.uint8)*255\n",
    "mix_rgb = np.concatenate((pmap, pmap_with_img[:,:,1:]),-1)\n",
    "plt.figure(figsize=(14,14))\n",
    "plt.imshow(mix_rgb)\n",
    "plt.title(str(np.mean(pmap_add)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_weight():   \n",
    "    weights_dir = path_weight#\"data\"\n",
    "    #latest_checkpoint = tf.train.latest_checkpoint(weights_dir)\n",
    "    latest_checkpoint = weights_dir \n",
    "    print('latest_checkpoint', latest_checkpoint)\n",
    "    model.load_weights(weights_dir,by_name=False, skip_mismatch=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes, num_classes_real, len(class_names), seg_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = tf.distribute.MirroredStrategy()\n",
    "#strategy = tf.distribute.OneDeviceStrategy(device='/gpu:0')\n",
    "print(\"Number of devices: {}\".format(strategy.num_replicas_in_sync))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes, use_pedestrian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#optimizer = tf.optimizers.SGD(learning_rate=1e-5, momentum=0.1, clipvalue=5.)#warm up clipvalue=10. !\n",
    "\n",
    "with strategy.scope():\n",
    "    optimizer = tf.optimizers.SGD(learning_rate=1e-3, momentum=0.1)\n",
    "    loss_detect = RetinaNetLoss(num_classes)\n",
    "    loss_seg = SegmentLoss(seg_k)\n",
    "    model = createRetinaNet(num_classes, anchor_k)\n",
    "    metrics_detect = [recall, precision, acc]#, gender_acc, body_acc, up_cloth_acc]\n",
    "    losses = {\"detect\": loss_detect, 'segment': loss_seg} #dual\n",
    "    metrics = {\"detect\": metrics_detect, 'segment': seg_acc} \n",
    "    loss_weights = {\"detect\": 1, 'segment': 10} \n",
    "    if use_pedestrian and False:\n",
    "        metrics = [recall, precision, acc, gender_acc, body_acc, up_cloth_acc]\n",
    "    model.compile(optimizer=optimizer, loss=losses, metrics=metrics, loss_weights=loss_weights)\n",
    "    callbacks_list = [\n",
    "        tf.keras.callbacks.ModelCheckpoint(\n",
    "            filepath=path_weight,\n",
    "            monitor=\"loss\",\n",
    "            save_best_only=False,\n",
    "            save_weights_only=True,\n",
    "            verbose=0,\n",
    "            save_freq=200\n",
    "        )\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()#8,682,381"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    load_weight()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(input_list_train), len(input_list_test))#990 2857, 1170 3128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "resnet-34  81ms/step, resnet-24  53ms/step\n",
    "efficientB0  :      63ms/step - loss: 1.5916 - recall: 0.9155 - precision: 0.9330 - accuracy: 0.8751\n",
    "eff-D7 Freeze:     182ms/step - loss: 2.1849 - recall: 0.9470 - precision: 0.9660 - accuracy: 0.9055 - flip_accuracy: 0.0052\n",
    "eff-D2 finetu:  74s 74ms/step - loss: 4.5865 - recall: 0.8719 - precision: 0.8843 - acc: 0.9773\n",
    "eff-D3 finetu:  74s 74ms/step - loss: 1.5638 - recall: 0.8632 - precision: 0.8382 - acc: 0.8657\n",
    "        7s 75ms/step - loss: 3.3875 - recall: 0.8686 - precision: 0.8915 - acc: 0.8226       \n",
    "       76s 76ms/step - loss: 1.9485 - recall: 0.9110 - precision: 0.7519 - acc: 0.9157 - gender_acc: 0.0000e+00 - body_acc: 0.0000e+00 - up_cloth_acc: 0.0000e+00\n",
    "       75s 75ms/step - loss: 1.6430 - recall: 0.9019 - precision: 0.8985 - acc: 0.9158\n",
    "       47s 95ms/step - loss: 6.2153 - recall: 0.8751 - precision: 0.7636 - acc: 0.6846 - gender_acc: 0.0000e+00 - body_acc: 0.0000e+00 - up_cloth_acc: 0.0000e+00\n",
    "eff-D4 finetu:  92s 92ms/step - loss: 5.1396 - recall: 0.9585 - precision: 0.9536 - acc: 0.9400                                \n",
    "                59s 118ms/step - loss: 6.1633 - tf_op_layer_concat_6_loss: 6.1633 - tf_op_layer_Softmax_loss: 0.0000e+00 - tf_op_layer_concat_6_recall: 0.8807 - tf_op_layer_concat_6_precision: 0.9260 - tf_op_layer_concat_6_acc: 0.8668 - tf_op_layer_Softmax_seg_acc: 0.1728\n",
    "                49s 130ms/step - loss: 6.9062 - tf_op_layer_concat_16_loss: 6.9062 - tf_op_layer_Softmax_loss: 0.0000e+00 - tf_op_layer_concat_16_recall: 0.9372 - tf_op_layer_concat_16_precision: 0.9568 - tf_op_layer_concat_16_acc: 0.8979 - tf_op_layer_Softmax_seg_acc: 0.0090\n",
    "shuffle_ 2_2x:  49s 131ms/step - loss: 3.6201 - tf_op_layer_concat_16_loss: 3.6201 - tf_op_layer_Softmax_loss: 0.0000e+00 - tf_op_layer_concat_16_recall: 0.9478 - tf_op_layer_concat_16_precision: 0.9652 - tf_op_layer_concat_16_acc: 0.9236 - tf_op_layer_Softmax_seg_acc: 0.1304\n",
    "''' \n",
    "out = model.evaluate(val_dataset.take(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "hist = model.fit(train_dataset, epochs=epochs,  callbacks=callbacks_list, verbose=1)\n",
    "'''\n",
    "effD2 Freeze:32s 474ms/step - loss: 14.7472 - recall: 0.4458 - precision: 0.6697 - accuracy: 0.0049\n",
    "effD2 fine :269s 544ms/step - loss: 9.3350 - recall: 0.9176 - precision: 0.9638 - acc: 0.9910\n",
    "effD3 Freeze : 171s 546ms/step - loss:15.9819 - recall: 0.8651 - precision: 0.6972 - acc: 0.9518 - gender_acc: 0.5927 - body_acc: 0.5747 - up_cloth_acc: 0.3175\n",
    "        fine :1239s 521ms/step - loss: 2.2392 - recall: 0.8902 - precision: 0.9115 - acc: 0.9087\n",
    "effD4 Freeze : 123s 796ms/step - loss: 53.2804 - recall: 0.5714 - precision: 0.6422 - acc: 0.4137 - gender_acc: 0.5843 - body_acc: 0.5230 - up_cloth_acc: 0.2131\n",
    "      fine   :1536s 651ms/step - loss: 5.4101 - recall: 0.9451 - precision: 0.9601 - acc: 0.8175              \n",
    "suppre bgcls  1829/Unknown - 1224s 669ms/step - loss: 25.7655 - recall: 0.9261 - precision: 0.9402 - acc: 0.7992\n",
    "effD4 seg    : 2405s 718ms/step - loss: 2.4981 - tf_op_layer_concat_27_loss: 1.8903 - tf_op_layer_Softmax_5_loss: 0.2026 - tf_op_layer_concat_27_recall: 0.4977 - tf_op_layer_concat_27_precision: 0.5013 - tf_op_layer_concat_27_acc: 0.9326 - tf_op_layer_Softmax_5_seg_acc: 0.4095\n",
    "               2216s 724ms/step - loss: 2.9644 - tf_op_layer_concat_6_loss: 2.2235 - tf_op_layer_Softmax_loss: 0.2470 - tf_op_layer_concat_6_recall: 0.4438 - tf_op_layer_concat_6_precision: 0.4496 - tf_op_layer_concat_6_acc: 0.9164 - tf_op_layer_Softmax_seg_acc: 0.4521\n",
    "               2295s 749ms/step - loss: 2.2662 - tf_op_layer_concat_6_loss: 1.9251 - tf_op_layer_Softmax_loss: 0.1137 - tf_op_layer_concat_6_recall: 0.4484 - tf_op_layer_concat_6_precision: 0.4535 - tf_op_layer_concat_6_acc: 0.9265 - tf_op_layer_Softmax_seg_acc: 0.5297\n",
    "               2056s 740ms/step - loss: 1.1527 - tf_op_layer_concat_6_loss: 0.9402 - tf_op_layer_Softmax_loss: 0.0212 - tf_op_layer_concat_6_recall: 0.4042 - tf_op_layer_concat_6_precision: 0.4061 - tf_op_layer_concat_6_acc: 0.9523 - tf_op_layer_Softmax_seg_acc: 0.4421\n",
    "               1423s 781ms/step - loss: 5.8642 - tf_op_layer_concat_16_loss: 5.8005 - tf_op_layer_Softmax_loss: 0.0064 - tf_op_layer_concat_16_recall: 0.4751 - tf_op_layer_concat_16_precision: 0.4916 - tf_op_layer_concat_16_acc: 0.8297 - tf_op_layer_Softmax_seg_acc: 0.2005\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.save_weights(path_weight)\n",
    "path_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = tf.keras.Input(shape=[None, None, 3], name=\"image\")\n",
    "model_out = model(image, training=False)\n",
    "print('predictions', type(model_out), model_out.keys())\n",
    "predictions = model_out['detect']\n",
    "segment_h = model_out['segment']\n",
    "segment_cls = tf.argmax(segment_h, -1)\n",
    "\n",
    "detections = decodePredictions(image, predictions, confidence_threshold=0.5, nms_iou_threshold=0.2)\n",
    "inference_model = tf.keras.Model(inputs=image, outputs=[detections, segment_cls])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_rotate_aug = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def visualize_detections(\n",
    "    image, boxes, classes, scores, figsize=(12, 10), linewidth=1, color=[0, 0, 1], \n",
    "    boxes_gt=None):\n",
    "    \n",
    "    image = np.array(image, dtype=np.uint8)\n",
    "    image = attach_crop_image(image, boxes, max_crop=200)        \n",
    "    \n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(image)\n",
    "    ax = plt.gca()        \n",
    "   \n",
    "    if boxes_gt is not None:\n",
    "        for box in boxes_gt:        \n",
    "            x1, y1, x2, y2 = box\n",
    "            w, h = x2 - x1, y2 - y1\n",
    "            patch = plt.Rectangle(\n",
    "                [x1, y1], w, h, fill=False, edgecolor=[0,1,0], linewidth=2\n",
    "            )\n",
    "            ax.add_patch(patch)\n",
    "            \n",
    "    for box, cls, score in zip(boxes, classes, scores):        \n",
    "        x1, y1, x2, y2 = box\n",
    "        w, h = x2 - x1, y2 - y1\n",
    "        \n",
    "        if cls == 3:\n",
    "            color = [1, 0, 0]\n",
    "        elif cls == 1:\n",
    "            color = [1, 1, 1]\n",
    "        else:\n",
    "            color = [0, 0, 1]\n",
    "            \n",
    "        color_text = color#edgecolors[cls]\n",
    "        \n",
    "        patch = plt.Rectangle(\n",
    "            [x1, y1], w, h, fill=False, edgecolor=color, linewidth=linewidth\n",
    "        )\n",
    "        ax.add_patch(patch)\n",
    "        if len(boxes) < 100:\n",
    "            score_txt = str.format('(%d)%.2f' %(cls, score))\n",
    "            ax.text(x1, y1, score_txt, bbox={\"facecolor\": color_text, \"alpha\": 0.4}, clip_box=ax.clipbox, clip_on=True,)\n",
    "          \n",
    "    plt.show()\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_test(test_datas, step=1, is_rgb_show=True):\n",
    "    i = 0\n",
    "    for image, segmap, cbbox in test_datas: \n",
    "        \n",
    "        if i%step==0:                        \n",
    "            bbox_annotation = cbbox\n",
    "            scale = np.array(image.shape[:2])[::-1]\n",
    "            scale = np.reshape(scale, [1, 2])\n",
    "            scale = np.concatenate((scale, scale), 1)\n",
    "            gt_bbox = bbox_annotation[ :, :4] * scale\n",
    "           \n",
    "            input_image, _, ratio = resize_and_pad_image(image, jitter=None)\n",
    "            input_image = tf.expand_dims(input_image, axis=0)\n",
    "            #input_image = tf.cast(input_image, tf.uint8)\n",
    "            model_out = inference_model.predict(input_image)\n",
    "            print('detected_box', type(model_out))\n",
    "            detected_box = model_out[0]            \n",
    "            seg_h = (model_out[1][0]).astype(np.uint8)      \n",
    "                        \n",
    "            print(input_image.shape, seg_h.shape, np.mean(seg_h), 'detected_box', detected_box.shape, scale, 'ratio',ratio)\n",
    "            #(1, 1024, 1920, 3) detected_box (7, 6) [[1920 1080 1920 1080]] ratio tf.Tensor(0.94814813, shape=(), dtype=float32)\n",
    "            cls_h = detected_box[:, 0].astype(np.int)\n",
    "            scores = detected_box[:, 1]\n",
    "            box = detected_box[:, 2:] / ratio\n",
    "            if not is_rgb_show:\n",
    "                if False:\n",
    "                    seg_person = (seg_h==1).astype(np.float32)\n",
    "                    seg_h_exp = np.expand_dims(seg_person, 0)\n",
    "                    seg_h_exp = np.expand_dims(seg_h_exp, -1)\n",
    "                    filter_mask = np.ones((15, 15, 1), np.float32)\n",
    "                    morp_out = tf.nn.dilation2d(seg_h_exp, filter_mask, strides=[1,1,1,1], padding='SAME', data_format='NHWC', dilations=dilations)\n",
    "                    seg_h = morp_out[0, :, :, 0]\n",
    "                plt.imshow(seg_h)\n",
    "                plt.show()\n",
    "            else:\n",
    "                visualize_detections(\n",
    "                    image,\n",
    "                    box,\n",
    "                    cls_h,\n",
    "                    scores,\n",
    "                    boxes_gt=gt_bbox\n",
    "                )\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "check_test(dataset_test, step=100, is_rgb_show=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "check_test(dataset_test, step=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "check_test(dataset_test, step=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "check_test(dataset, step=100, is_rgb_show=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "check_test(dataset, step=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#random indexing for train\n",
    "check_test(dataset, step=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_wrong(X, Y, annot):\n",
    "    i=0\n",
    "    for ann in annot:\n",
    "        image, y = X[i], Y[i]\n",
    "        #print('ann', ann)            \n",
    "        i+=1\n",
    "        \n",
    "        scale = np.array(image.shape[:2])[::-1]\n",
    "        scale = np.reshape(scale, [1, 2])\n",
    "        scale = np.concatenate((scale, scale), 1)\n",
    "        gt_bbox = y[ :, :4] * scale\n",
    "        #gt_bbox = y[:, 1:1+4]\n",
    "        input_image, _, ratio = resize_and_pad_image(image, jitter=None)\n",
    "        input_image = tf.expand_dims(input_image, axis=0)\n",
    "        #input_image = tf.cast(input_image, tf.uint8)\n",
    "        detected_box = inference_model.predict(input_image)\n",
    "        if len(detected_box)>len(y):\n",
    "            print('ann', ann)\n",
    "            print('detected_box', detected_box.shape, 'gt', len(y))\n",
    "            #(1, 1024, 1920, 3) detected_box (7, 6) [[1920 1080 1920 1080]] ratio tf.Tensor(0.94814813, shape=(), dtype=float32)\n",
    "            cls_h = detected_box[:, 0].astype(np.int)\n",
    "            scores = detected_box[:, 1]\n",
    "            box = detected_box[:, 2:] / ratio\n",
    "\n",
    "            visualize_detections(\n",
    "                image,\n",
    "                box,\n",
    "                cls_h,\n",
    "                scores,\n",
    "                boxes_gt=gt_bbox\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "check_wrong(input_list, bbox_list, annotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_pad_32x_arr(image_arr):\n",
    "    #print('image_arr', image_arr.shape, image_arr.dtype)\n",
    "    stride = 32\n",
    "    img_h = image_arr.shape[0]\n",
    "    img_w = image_arr.shape[1]\n",
    "    img_c = image_arr.shape[2]\n",
    "    #print('add_pad_32x', image_arr.shape, img_h, img_w)\n",
    "    pad_h = (stride - (img_h % stride)) % stride\n",
    "    pad_w = (stride - (img_w % stride)) % stride\n",
    "    padded_h = img_h + pad_h\n",
    "    padded_w = img_w + pad_w\n",
    "    #print('pad_h', pad_h, 'pad_w', pad_w)\n",
    "    image_padded = np.zeros((padded_h, padded_w, img_c), dtype=np.uint8)\n",
    "    image_padded[:img_h, :img_w] = image_arr\n",
    "        \n",
    "    return image_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_bg(image, is_display=True, is_save=False, save_path=''):\n",
    "    scale = np.array(image.shape[:2])[::-1]\n",
    "    scale = np.reshape(scale, [1, 2])\n",
    "    scale = np.concatenate((scale, scale), 1)\n",
    "\n",
    "    #input_image, _, ratio = resize_and_pad_image(image, jitter=None)\n",
    "    input_image = add_pad_32x_arr(image)     \n",
    "    ratio = 1\n",
    "    input_image = tf.expand_dims(input_image, axis=0)\n",
    "\n",
    "    detected_box = inference_model.predict(input_image)        \n",
    "    if len(detected_box) > 0:\n",
    "        #print(input_image.shape, 'detected_box', detected_box.shape, scale, 'ratio',ratio)\n",
    "        #(1, 1024, 1920, 3) detected_box (7, 6) [[1920 1080 1920 1080]] ratio tf.Tensor(0.94814813, shape=(), dtype=float32)\n",
    "        if is_display:\n",
    "            cls_h = detected_box[:, 0].astype(np.int)\n",
    "            scores = detected_box[:, 1]\n",
    "            box = detected_box[:, 2:] / ratio\n",
    "\n",
    "            visualize_detections(\n",
    "                image,\n",
    "                box,\n",
    "                cls_h,\n",
    "                scores                \n",
    "            )\n",
    "        if is_save:\n",
    "            img = Image.fromarray(image)\n",
    "            img.save(save_path +'.jpg')\n",
    "        else:\n",
    "            pass\n",
    "    return int(len(detected_box)>0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_jpg = glob(folder_water_bg + '*')\n",
    "print(len(list_jpg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong = 0\n",
    "for i in range(0, len(list_jpg)//1):    \n",
    "    jpg = list_jpg[i]\n",
    "    #print('i', i, jpg.split(os.sep)[-1])\n",
    "    img = Image.open(jpg)\n",
    "    arr = np.array(img)\n",
    "    \n",
    "    n = check_bg(arr, is_display=True)\n",
    "    if n>0:\n",
    "        wrong += n\n",
    "        print(wrong, i, jpg)#414/815, 348/544, 6/242, 43/1522, 59/2043, 64/2375"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## video inference, save result\n",
    "1. split image from video\n",
    "1. get target image paths\n",
    "1. inference, save result to list\n",
    "1. save as csv format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import time\n",
    "def video_to_frames(input_loc, stride=1):\n",
    "    \"\"\"Function to extract frames from input video file\n",
    "    and save them as separate frames in an output directory.\n",
    "    Args:\n",
    "        input_loc: Input video file.\n",
    "        output_loc: Output directory to save the frames.\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "        \n",
    "    folder_split = input_loc.split(os.sep)\n",
    "    file_name = folder_split[-1]\n",
    "    file_name = file_name.split('.')[0]\n",
    "    output_loc = os.sep.join(folder_split[:-1]) + os.sep + file_name + os.sep    \n",
    "    print('output_loc', output_loc)\n",
    "        \n",
    "    try:\n",
    "        os.mkdir(output_loc)\n",
    "    except OSError:\n",
    "        pass\n",
    "    # Log the time\n",
    "    time_start = time.time()\n",
    "    # Start capturing the feed\n",
    "    cap = cv2.VideoCapture(input_loc)\n",
    "    # Find the number of frames\n",
    "    \n",
    "    video_length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) - 1\n",
    "    print (\"Number of frames: \", video_length)\n",
    "    count = 0\n",
    "    save_count = 0\n",
    "    print (\"Converting video..\\n\")\n",
    "    # Start converting the video\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        # Extract the frame\n",
    "        ret, frame = cap.read()\n",
    "        count = count + 1        \n",
    "        file_name = \"%#05d.jpg\" % (count+1)        \n",
    "        dst_name = output_loc + file_name\n",
    "        \n",
    "        try:\n",
    "            cv2.imwrite(dst_name, frame)\n",
    "            #b, g, r = np.split(frame, 3, -1)\n",
    "            #rgb = np.concatenate((r,g,b), -1)        \n",
    "            save_count += 1\n",
    "        except:\n",
    "            print('except', cap, ret, frame, dst_name)\n",
    "            break\n",
    "        \n",
    "        # If there are no more frames left\n",
    "        if count%100==0:\n",
    "            print('count', video_length, count, save_count)\n",
    "       \n",
    "    time_end = time.time()\n",
    "    # Release the feed\n",
    "    cap.release()\n",
    "    # Print stats\n",
    "    print (\"Done extracting frames.\\n%d frames extracted\" % count)\n",
    "    print (\"It took %d seconds to save %d for conversion .\" % (time_end-time_start, save_count))\n",
    "    return output_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_avi = '/home/mvlab/Videos/미수_스마트시티_통영대교_미수방향1(Ch 01)_[20201217]140000-[20201217]141000(20201217_140001).avi'\n",
    "os.path.isfile(path_avi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output_loc = video_to_frames(path_avi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#                                미수_스마트시티_통영대교_미수방향1(Ch 01)_[20201217]135000-[20201217]140000(20201217_135001)\n",
    "output_loc = '/home/mvlab/Videos/미수방향1_2/'\n",
    "os.path.isdir(output_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#paths_test_img = glob('/media/mvlab/46FA9CA2FA9C8FB3/dataset/pedestrian/Tongyoung/event/*미수방향1*/*.jpg')\n",
    "paths_test_img = glob(output_loc + '*')\n",
    "print(len(paths_test_img), output_loc)\n",
    "paths_test_img = np.sort(paths_test_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_label_list_to_df(list_output):\n",
    "    print('convert_label_list_to_df')\n",
    "    df = pd.DataFrame(columns=['path', 'cls', 'x0', 'y0', 'x1', 'y1', 'confidence'])\n",
    "    \n",
    "    ratio = 0.9481481481481482\n",
    "    for output in list_output:\n",
    "        path_img = output[0]\n",
    "        detected_box = output[1]\n",
    "\n",
    "        for box in detected_box:\n",
    "            cls_h = box[0].astype(np.int)\n",
    "            scores = (box[1]*100).astype(np.int)\n",
    "            box_coord = box[2:2+6]/ratio\n",
    "            x0 = box_coord[0].astype(np.int)\n",
    "            y0 = box_coord[1].astype(np.int)\n",
    "            x1 = box_coord[2].astype(np.int)\n",
    "            y1 = box_coord[3].astype(np.int)\n",
    "            row = {'path':path_img, 'cls':cls_h, 'x0':x0, 'y0':y0, 'x1':x1, 'y1':y1, 'confidence':scores}\n",
    "            df = df.append(row, ignore_index=True)\n",
    "            \n",
    "    return df\n",
    "\n",
    "def save_inference_result(paths_img):\n",
    "    \n",
    "    print('len', len(paths_img))\n",
    "    \n",
    "    list_output = []\n",
    "    for i in range(len(paths_img)):\n",
    "        path_img = paths_img[i]\n",
    "        if i % 100 == 0:\n",
    "            print(len(paths_img), i, path_img.split(os.sep)[-1])\n",
    "        \n",
    "        img = Image.open(path_img)\n",
    "        img_arr = np.array(img)\n",
    "        input_image, _, ratio = resize_and_pad_image(img_arr, jitter=None)\n",
    "        input_image = tf.expand_dims(input_image, axis=0)        \n",
    "        detected_box = inference_model.predict(input_image)\n",
    "        \n",
    "        if len(detected_box) > 0:            \n",
    "            list_output.append([path_img, detected_box])\n",
    "        \n",
    "        if len(list_output) > 0 and len(list_output)%1000==0:\n",
    "            df = convert_label_list_to_df(list_output)            \n",
    "            df.to_csv('predict_temp.csv')\n",
    "    \n",
    "    df = convert_label_list_to_df(list_output)            \n",
    "    df.to_csv('predict.csv')\n",
    "    print('finished')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_inference_result(paths_test_img[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Freeze Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Convert Keras model to ConcreteFunction\n",
    "full_model = tf.function(lambda x: inference_model(x))\n",
    "full_model = full_model.get_concrete_function(\n",
    "    x=tf.TensorSpec(inference_model.inputs[0].shape, inference_model.inputs[0].dtype))\n",
    "\n",
    "# Get frozen ConcreteFunction\n",
    "frozen_func = convert_variables_to_constants_v2(full_model)\n",
    "frozen_func.graph.as_graph_def()\n",
    "\n",
    "layers = [op.name for op in frozen_func.graph.get_operations()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('inputs', type(frozen_func.inputs), len(frozen_func.inputs), frozen_func.inputs)\n",
    "print('outputs', type(frozen_func.outputs), len(frozen_func.outputs), frozen_func.outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.io.write_graph(graph_or_graph_def=frozen_func.graph,\n",
    "                  logdir=\"./frozen_models\",\n",
    "                  name=\"pedestrian_efficientSeg-D4_frozen_graph.pb\",\n",
    "                  as_text=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_error(X, Y, Path, step=1):\n",
    "    \n",
    "    for i in range(len(X)): \n",
    "        image = X[i]\n",
    "        bbox_annotation = Y[i]\n",
    "        path = Path[i]\n",
    "        \n",
    "        scale = np.array(image.shape[:2])[::-1]\n",
    "        scale = np.reshape(scale, [1, 2])\n",
    "        scale = np.concatenate((scale, scale), 1)\n",
    "        gt_bbox = bbox_annotation[:, 1:] * scale\n",
    "\n",
    "        input_image, _, ratio = resize_and_pad_image(image, jitter=None)\n",
    "        input_image = tf.expand_dims(input_image, axis=0)\n",
    "\n",
    "        detected_box = inference_model.predict(input_image)\n",
    "        detect_k = len(detected_box)\n",
    "        if detect_k!= len(bbox_annotation):\n",
    "            print(path, input_image.shape, 'detected_box', detected_box.shape, scale, 'ratio',ratio.numpy())\n",
    "            #(1, 1024, 1920, 3) detected_box (7, 6) [[1920 1080 1920 1080]] ratio tf.Tensor(0.94814813, shape=(), dtype=float32)\n",
    "            cls_h = detected_box[:, 0].astype(np.int)\n",
    "            scores = detected_box[:, 1]\n",
    "            box = detected_box[:, 2:] / ratio\n",
    "            #print('box', box)\n",
    "\n",
    "            visualize_detections(\n",
    "                image,\n",
    "                box,\n",
    "                cls_h,\n",
    "                scores,\n",
    "                boxes_gt=gt_bbox\n",
    "            )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 1000\n",
    "end = start + 100\n",
    "check_error(input_list[start:end], bbox_list[start:end], path_list[start:end])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load model from .pb file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model_pb = './pedestrian_efficientSeg-D4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_model.save(saved_model_pb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_objects = {'recall':recall,'precision':precision}\n",
    "model_loaded = keras.models.load_model(saved_model_pb, custom_objects=custom_objects, compile=False)\n",
    "#model_loaded.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_loaded.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for image, cbbox in val_dataset: \n",
    "    \n",
    "    detected_box = model_loaded.predict(image)\n",
    "    print('detected_box', detected_box.shape)\n",
    "    if len(detected_box) > 0:\n",
    "        cls_h = detected_box[:, 0].astype(np.int)\n",
    "        scores = detected_box[:, 1]\n",
    "        box = detected_box[:, 2:]\n",
    "\n",
    "        visualize_detections(\n",
    "            image[0],\n",
    "            box,\n",
    "            cls_h,\n",
    "            scores\n",
    "        )    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_pb)\n",
    "#converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tflite_model = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the TF Lite model.\n",
    "with tf.io.gfile.GFile('model.tflite', 'wb') as f:\n",
    "  f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls *.tflite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Convert the model.\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(inference_model)\n",
    "tflite_model = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the TF Lite model.\n",
    "with tf.io.gfile.GFile('model.tflite', 'wb') as f:\n",
    "  f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load TFLite model and allocate tensors.\n",
    "interpreter = tf.lite.Interpreter(model_content=tflite_model)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensors.\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Test the TensorFlow Lite model on random input data.\n",
    "input_shape = input_details[0]['shape']\n",
    "input_details, output_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = input_list_train[0]\n",
    "input_data = (np.expand_dims(input_data, 0)/255).astype(np.float32)\n",
    "input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = input_data[:, :padded_image_shape[0], :padded_image_shape[1]]\n",
    "input_data.shape, np.max(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)\n",
    "print('input_data', input_data.shape)\n",
    "interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "\n",
    "interpreter.invoke()\n",
    "tflite_results = interpreter.get_tensor(output_details[0]['index'])\n",
    "tflite_results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tflite_results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detected_box = tflite_results\n",
    "cls_h = detected_box[:, 0].astype(np.int)\n",
    "scores = detected_box[:, 1]\n",
    "box = detected_box[:, 2:] / ratio\n",
    "#print('box', box)\n",
    "\n",
    "visualize_detections(\n",
    "    input_data[0]*255,\n",
    "    box,\n",
    "    cls_h,\n",
    "    scores,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glob('/home/mvlab/Downloads/dataset/통영/label_noise/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
