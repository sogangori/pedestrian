{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_datasets as tfds\n",
    "import pandas as pd\n",
    "\n",
    "from PIL import Image\n",
    "from PIL import ImageFilter\n",
    "import ast\n",
    "import json\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://towardsdatascience.com/building-a-resnet-in-keras-e8f1322a49ba\n",
    "from tensorflow import Tensor\n",
    "from tensorflow.keras.layers import Input, Conv2D, ReLU, BatchNormalization,\\\n",
    "                                    Add, AveragePooling2D, Flatten, Dense, MaxPool2D\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/tensorflow/addons/blob/v0.11.2/tensorflow_addons/image/__init__.py\n",
    "from tensorflow_addons.image.color_ops import sharpness\n",
    "from tensorflow_addons.image.filters import gaussian_filter2d\n",
    "from tensorflow_addons.image.dense_image_warp import dense_image_warp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.__version__, tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\" #CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_image_shape = (1024, 1920)\n",
    "anchor_k = 9\n",
    "num_classes = 300\n",
    "num_classes_real = num_classes\n",
    "max_data_m = 50000\n",
    "use_zoom_up_data = False\n",
    "level_start = 4\n",
    "level_end = 7\n",
    "l1 = 1e-9\n",
    "activation = 'swish'#'selu' is not converted to tflite\n",
    "kernel_init = tf.initializers.he_normal()\n",
    "edgecolors = np.random.rand(num_classes, 3) \n",
    "edgecolors = np.minimum(edgecolors+0.1, 1.0)\n",
    "class_names = ['bg', 'person']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_pedestrian = '/home/mvlab/Downloads/dataset/통영/'\n",
    "forder_P_DESTRE = '/home/mvlab/Downloads/dataset/P-DESTRE/'\n",
    "folder_weather = '/home/sogangori/Downloads/dataset/weather/'\n",
    "class_names = ['bg', '\"water\"', '\"waterf\"', '\"land\"', '\"animal\"']\n",
    "names = ['fn','cname','id', 'x0', 'y0', 'w', 'h']\n",
    "edgecolors = np.random.rand(num_classes, 3) \n",
    "edgecolors = np.minimum(edgecolors+0.1, 1.0)\n",
    "path_weight = \"weight/pedestrian_efficientDet-D2\"\n",
    "\n",
    "os.path.isdir(path_pedestrian), os.path.isdir(forder_P_DESTRE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths_avi = glob(path_pedestrian+'*.avi')\n",
    "paths_txt = glob(path_pedestrian+'*/*.txt')\n",
    "paths_img = glob(path_pedestrian+'*/*')\n",
    "len(paths_avi), len(paths_txt), len(paths_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_weather_effect = os.path.isdir(folder_weather)\n",
    "use_weather_effect, folder_weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_images = []\n",
    "if use_weather_effect:\n",
    "    path_weathers = glob(folder_weather + '*.*')\n",
    "    if len(path_weathers)>0:\n",
    "        for path_weather in path_weathers:            \n",
    "            image = Image.open(path_weather) \n",
    "            weather_images.append(image)            \n",
    "            plt.imshow(image)\n",
    "            plt.show()\n",
    "    else:\n",
    "        use_weather_effect = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### video label load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_img = pd.read_csv(paths_txt[1], header=None, names=names)\n",
    "df_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def set_frame_path(df, paths_img):\n",
    "    frame = df['fn'].values\n",
    "    \n",
    "    file_name_dict = dict()\n",
    "    \n",
    "    for path_img in paths_img:\n",
    "        file_name = path_img.split(os.sep)[-1]\n",
    "        file_name = file_name.split('.')[0]\n",
    "        try:\n",
    "            file_name = int(file_name)\n",
    "        except:\n",
    "            pass\n",
    "        file_name_dict[file_name] = path_img\n",
    "            \n",
    "    print('file_name_dict', len(file_name_dict)) \n",
    "    \n",
    "    list_path = []\n",
    "            \n",
    "    for fr in frame:\n",
    "        if fr in file_name_dict.keys():\n",
    "            list_path.append(file_name_dict[fr])\n",
    "        else:\n",
    "            list_path.append(None)\n",
    "            \n",
    "    df['path'] = list_path    \n",
    "    return df_img\n",
    "\n",
    "df_img = set_frame_path(df_img, paths_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### image label load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_cut = path_pedestrian + 'bridge_img_01/'\n",
    "path_cut_label = glob(path_cut + '*.txt')\n",
    "path_cut, os.path.isdir(path_cut), len(path_cut_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_cut_csv = path_cut_label[0]\n",
    "path_cut_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['path','cname','id', 'x0', 'y0', 'w', 'h']\n",
    "df_cut = pd.read_csv(path_cut_csv, names=names)\n",
    "df_cut['path'] = path_cut + df_cut['path']\n",
    "df_cut.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P-DESTRE load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PDESTRE_columns = ['frame', 'ID', 'x', 'y', 'w', 'h', 'head', 'yaw', 'pitch', 'roll',          \n",
    "          'gender', 'age', 'height', 'body volume', 'ethnicity', 'hair color', 'hairstyle', 'beard', 'mustache', 'glasses', \n",
    "           'head accessories', 'upper cloth', 'lower cloth', 'feet', 'accessories', 'action']\n",
    "len(PDESTRE_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glob(forder_P_DESTRE + '*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#glob(forder_P_DESTRE + 'annotation/*.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_dataframe(forder_P_DESTRE):\n",
    "    list_df = []\n",
    "    \n",
    "    list_annotation_path = glob(forder_P_DESTRE + 'annotation/*.txt')\n",
    "    for i in range(len(list_annotation_path)):\n",
    "        path_anno = list_annotation_path[i]\n",
    "        anno_file_name = path_anno.split(os.sep)[-1]\n",
    "        anno_file_name = anno_file_name[:-4]\n",
    "        df = pd.read_csv(path_anno, header=None, names=PDESTRE_columns)\n",
    "        df['video'] = anno_file_name\n",
    "        list_df.append(df)\n",
    "        #print(i, anno_file_name, df.shape)\n",
    "    \n",
    "    df_all = pd.concat(list_df, axis=0)\n",
    "    df_all = df_all.reset_index()\n",
    "    return df_all \n",
    "\n",
    "df_P_DESTRE = get_dataframe(forder_P_DESTRE)    \n",
    "df_P_DESTRE.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_P_DESTRE.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_P_DESTRE.iloc[:, 11:-2].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ax = df_P_DESTRE.iloc[:, 11:-2].hist(figsize=(10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_P_DESTRE['ID'].hist() #2:unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_P_DESTRE['gender'].hist() #2:unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_P_DESTRE['body volume'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_P_DESTRE['action'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_P_DESTRE['upper cloth'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_P_DESTRE['gender_'] = df_P_DESTRE['gender'].max() - df_P_DESTRE['gender']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_P_DESTRE['body volume_'] = df_P_DESTRE['body volume'].max() - df_P_DESTRE['body volume']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_P_DESTRE['upper cloth_'] = df_P_DESTRE['upper cloth'].max() - df_P_DESTRE['upper cloth']\n",
    "df_P_DESTRE['upper cloth'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_k = 2\n",
    "body_volumn_k = 3\n",
    "upper_cloth_k = 12\n",
    "label_attribute_k = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_P_DESTRE['ID'].max(), df_P_DESTRE['ID'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes_real = df_P_DESTRE['ID'].max() + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str.format('%#05d' % 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = []\n",
    "for fr in df_P_DESTRE['frame'].values:\n",
    "    file_names.append(str.format('%#05d.jpg' % fr))\n",
    "    \n",
    "df_P_DESTRE['file_name'] = file_names\n",
    "df_P_DESTRE['path'] = forder_P_DESTRE + 'videos/' + df_P_DESTRE['video'] + os.sep + df_P_DESTRE['file_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_P_DESTRE.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove not exist file\n",
    "is_exist_file = []\n",
    "for path_pdestre in df_P_DESTRE['path']:\n",
    "    is_exist_file.append(os.path.isfile(path_pdestre))\n",
    "\n",
    "np.mean(is_exist_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_P_DESTRE_exist = df_P_DESTRE[is_exist_file]\n",
    "df_P_DESTRE_exist.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_P_DESTRE_cut = df_P_DESTRE_exist[\n",
    "    ['ID', 'ID', 'x', 'y', 'w', 'h', 'path','gender_','body volume_','upper cloth_']]\n",
    "df_P_DESTRE_cut.columns = [\n",
    "    'cname', 'id', 'x0', 'y0', 'w', 'h', 'path','gender_','body volume_','upper cloth_']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_P_DESTRE_cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond = df_P_DESTRE_cut['id']==-1\n",
    "cond.sum(), len(cond)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_P_DESTRE_cut['id'][cond] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_img.shape, df_cut.shape, df_P_DESTRE_cut.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_img.columns, df_cut.columns, df_P_DESTRE_cut.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_img['id'].unique(), df_cut['id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat((df_img.drop(columns='fn'), df_cut), axis=0)\n",
    "df['id']= 1 #finetune\n",
    "df['gender_'] = 0\n",
    "df['body volume_'] = 0\n",
    "df['upper cloth_'] = 0\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum().sum(), df['path'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['path'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def parsing_annotation(df):\n",
    "    annotation = dict()\n",
    "    for i in range(len(df)):\n",
    "      \n",
    "        row = df.iloc[i].values\n",
    "        cname, iden, x0, y0, w, h, path, gender, body_volume, upper_cloth = row\n",
    "        x1 = x0 + w\n",
    "        y1 = y0 + h\n",
    "        if i%10000==0:\n",
    "            print(i, row)\n",
    "       \n",
    "        cls = iden # land \n",
    "        bbox = np.array([x0, y0, x1, y1, cls, gender, body_volume, upper_cloth]).reshape((1, -1))        \n",
    "\n",
    "        path_image = path#finetune\n",
    "        if not os.path.isfile(path_image):\n",
    "            print('not exist', path_image)\n",
    "            continue\n",
    "            \n",
    "        if path_image in annotation.keys():\n",
    "            pre_bbox = annotation[path_image]\n",
    "            new_bbox = np.concatenate((pre_bbox, bbox), axis=0)\n",
    "            #cls_bbox = np.stack(cls_bbox, 0)#.reshape([-1, 6])\n",
    "            #annotation[path_image].extend(new_bbox)\n",
    "            annotation[path_image] = new_bbox\n",
    "        else:\n",
    "            annotation[path_image] = bbox        \n",
    "    return annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "annotation = parsing_annotation(df)\n",
    "annotation_PDESTRE = parsing_annotation(df_P_DESTRE_cut)\n",
    "\n",
    "len(annotation), len(annotation_PDESTRE) #basic:1530"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VisDrone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visdrone_categories = ['bg','pedestrian', 'person', 'car', 'van', 'bus', 'truck', 'motor', 'bicycle', 'awning-tricycle', 'tricycle','empty_0','empty_1']\n",
    "path_visDrone = '/home/mvlab/Downloads/dataset/VisDrone2019/VisDrone2019-DET-train/'\n",
    "path_visDrone_annotation = path_visDrone + 'annotations/'\n",
    "path_visDrone_image = path_visDrone + 'images/'\n",
    "os.path.isdir(path_visDrone_annotation), os.path.isdir(path_visDrone_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_visdrone_data(path_visDrone, pedestrian_only=True):    \n",
    "    annotation = dict()\n",
    "    path_visDrone_annotation = path_visDrone + 'annotations/'\n",
    "    path_visDrone_image = path_visDrone + 'images/'\n",
    "    os.path.isdir(path_visDrone_annotation), os.path.isdir(path_visDrone_image)\n",
    "    list_annotation = glob(path_visDrone_annotation+'*.*')\n",
    "    list_image_path = glob(path_visDrone_image+'*.*')\n",
    "    \n",
    "    print('len', len(list_annotation), len(list_image_path), list_annotation[0])\n",
    "    \n",
    "    for i in range(len(list_annotation)):\n",
    "        path_annotation = list_annotation[i]\n",
    "        \n",
    "        df = pd.read_csv(path_annotation, header=None)\n",
    "                \n",
    "        cls = np.array(df.iloc[:, 5])\n",
    "        bbox_xywh = np.array(df.iloc[:, :4])        \n",
    "        \n",
    "        file_name_annotation = path_annotation.split('/')[-1].split('.')[0]\n",
    "        file_name_image = path_visDrone_image+file_name_annotation+'.jpg'\n",
    "        \n",
    "        if not os.path.isfile(file_name_image):\n",
    "            print('not exist', file_name_image)\n",
    "            continue\n",
    "                \n",
    "        if pedestrian_only:            \n",
    "            human_mask = np.logical_and(cls > 0, cls < 3)            \n",
    "            is_human_contain = np.any(human_mask)\n",
    "            human_count = np.sum(human_mask)\n",
    "            if not is_human_contain or human_count < 10:#30:651\n",
    "                continue\n",
    "            \n",
    "            cls = np.where(human_mask, 1.0, 0.0)\n",
    "            cls = cls[human_mask]\n",
    "            bbox_xywh = bbox_xywh[human_mask]            \n",
    "                      \n",
    "        x0 = bbox_xywh[:, 0]\n",
    "        y0 = bbox_xywh[:, 1]\n",
    "        w = bbox_xywh[:, 2]\n",
    "        h = bbox_xywh[:, 3]        \n",
    "        \n",
    "        annotation[file_name_image] = np.stack((x0, y0, x0 + w, y0 + h, cls), axis=1)           \n",
    "        \n",
    "        if len(file_name_image)>max_data_m:\n",
    "            break\n",
    "        if i%100==0:\n",
    "            print(len(list_annotation), i, file_name_annotation, len(file_name_image))\n",
    "    return annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "annotation_visdrone = load_visdrone_data(path_visDrone)\n",
    "print(len(annotation_visdrone))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### image load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_xy(annotation, rescale=1, stride=1, size_cut=False):\n",
    "    input_list = []\n",
    "    bbox_list = []\n",
    "    path_list = []\n",
    "    for path_image in annotation:\n",
    "        if stride!=1 and np.random.randint(1, 1+stride)%stride==0:\n",
    "            continue\n",
    "            \n",
    "        cls_bbox = annotation[path_image]                \n",
    "        bbox = np.array(cls_bbox[:, :4])\n",
    "        attribute = cls_bbox[:, 4:]\n",
    "\n",
    "        if os.path.isfile(path_image):\n",
    "            img = Image.open(path_image)    \n",
    "            scale = np.array((img.width, img.height, img.width, img.height))\n",
    "            scale = np.reshape(scale, (1, 4))\n",
    "\n",
    "            if rescale!=1:\n",
    "                img = img.resize((img.width//rescale, img.height//rescale))\n",
    "            \n",
    "            img_arr = np.array(img)        \n",
    "            std_v = np.std(img_arr)\n",
    "            if std_v < 3:\n",
    "                print('std_v', std_v)\n",
    "                continue\n",
    "            \n",
    "            bbox_norm = bbox.astype(np.float) / scale.astype(np.float)\n",
    "            cls_bbox_norm = np.concatenate((bbox_norm, attribute), axis=1)\n",
    "            \n",
    "            if size_cut:                \n",
    "                box_height = cls_bbox_norm[:, 3] - cls_bbox_norm[:, 1]\n",
    "                box_height_max = np.max(box_height)\n",
    "                if box_height_max < 0.05 or box_height_max > 0.2:\n",
    "                    continue\n",
    "\n",
    "            input_list.append(img_arr)\n",
    "            bbox_list.append(cls_bbox_norm)\n",
    "            path_list.append(path_image)\n",
    "            if len(input_list)%100==0:        \n",
    "                print(len(annotation), len(input_list), len(bbox_list))   \n",
    "            if len(input_list) > max_data_m:\n",
    "                break\n",
    "        else:\n",
    "            print('not exist', path_image)\n",
    "            pass\n",
    "\n",
    "    print(len(input_list), len(bbox_list))\n",
    "    return input_list, bbox_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_list, bbox_list = load_xy(annotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(input_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "input_list_PDESTRE, bbox_list_PDESTRE = load_xy(annotation_PDESTRE, rescale=4, stride=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list_visdrone_x, list_visdrone_y = load_xy(annotation_visdrone, size_cut=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(input_list), len(input_list_PDESTRE), len(list_visdrone_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "list_visdrone_x, list_visdrone_y = load_visdrone_data(path_visDrone)\n",
    "print(len(list_visdrone_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_data_m, np.unique(np.concatenate(list_visdrone_y, 0)[:, -1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "display_data(list_visdrone_x, list_visdrone_y, stride=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(input_list_PDESTRE[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox_list_PDESTRE[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbbox = np.concatenate(bbox_list_PDESTRE, 0)\n",
    "print(cbbox.shape, np.max(cbbox[:, 4])) # basic:1736 + P-DESTRE > 9757\n",
    "h = plt.hist(cbbox[:, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "h = plt.hist(cbbox[:, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbbox = np.concatenate(bbox_list, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "box_w = cbbox[:, 2] - cbbox[:, 0]\n",
    "box_h = cbbox[:, 3] - cbbox[:, 1]\n",
    "h = plt.hist(box_h, label='h')\n",
    "h = plt.hist(box_w, label='w')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(box_w, box_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbbox.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_bbox_image(image, boxes):\n",
    "    img_objects = []\n",
    "    image = np.array(image)\n",
    "    for box in boxes:        \n",
    "        box = box.astype(np.int)\n",
    "        x1, y1, x2, y2 = box\n",
    "        w = x2 - x1\n",
    "        h = y2 - y1        \n",
    "        crop_image_arr = image[y1:y2, x1:x2]\n",
    "        ch, cw, cc = crop_image_arr.shape\n",
    "        if ch>1 and cw>1:\n",
    "            img_objects.append(crop_image_arr)\n",
    "        else:\n",
    "            print('crop_bbox_image', x2-x1, y2-y1, 'crop_image_arr.shape', crop_image_arr.shape)\n",
    "        \n",
    "    return img_objects\n",
    "    \n",
    "    \n",
    "def attach_crop_image(image, boxes, max_crop=200):\n",
    "        \n",
    "    crop_bbox_arr = crop_bbox_image(image, np.array(boxes)[:max_crop])\n",
    "    bbox_k = len(crop_bbox_arr)\n",
    "    max_col = 30\n",
    "    \n",
    "    if bbox_k > 0:\n",
    "        img_h, img_w, img_c = image.shape\n",
    "        object_img_w = img_w//bbox_k        \n",
    "        resize_h = img_h // 8\n",
    "        resize_w = img_w // bbox_k  \n",
    "        resize_w = min(max(resize_w, img_w//max_col), img_w//8)\n",
    "        \n",
    "        footer_h = resize_h * (1 + (bbox_k-1)//max_col)\n",
    "        footer = np.zeros((footer_h, img_w, img_c), np.uint8)\n",
    "        \n",
    "        for i in range(min(bbox_k, max_crop)):\n",
    "            crop_arr = crop_bbox_arr[i]\n",
    "            crop_img = Image.fromarray(crop_arr)                \n",
    "            crop_img = crop_img.resize((resize_w, resize_h))\n",
    "            crop_arr_resized = np.array(crop_img)\n",
    "            offset_y = (i//max_col) * resize_h\n",
    "            offset_x = (i%max_col) * resize_w\n",
    "            footer[offset_y:offset_y+resize_h, offset_x:offset_x+resize_w] = crop_arr_resized\n",
    "\n",
    "        seperate_line = np.zeros_like(footer[:2])\n",
    "        image = np.concatenate((image, seperate_line, footer), axis=0)    \n",
    "    return image    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_detections_simple(\n",
    "    image, boxes, classes, figsize=(12, 12), linewidth=1, color=[0, 0, 1]\n",
    "):\n",
    "    \"\"\"Visualize Detections\"\"\"\n",
    "    image = np.array(image, dtype=np.uint8)    \n",
    "    \n",
    "    img_h, img_w, img_c = image.shape\n",
    "    \n",
    "    image = attach_crop_image(image, boxes, max_crop=100)\n",
    "    \n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(image)\n",
    "    ax = plt.gca()\n",
    "    boxes_width = boxes[:, 2] - boxes[:, 0]\n",
    "    boxes_height = boxes[:, 3] - boxes[:, 1]\n",
    "    box_min_width = np.min(boxes_width)\n",
    "    box_max_width = np.max(boxes_width)\n",
    "    title = str.format('(%dx%d) %d box, width:%d ~ %d' \n",
    "                       %(img_h, img_w, len(boxes), box_min_width, box_max_width))\n",
    "    plt.title(title)\n",
    "    for box, cls in zip(boxes, classes):\n",
    "        x1, y1, x2, y2 = box        \n",
    "        w, h = x2 - x1, y2 - y1\n",
    "        \n",
    "        color = edgecolors[min(len(edgecolors)-1,int(cls))]\n",
    "        patch = plt.Rectangle(\n",
    "            [x1, y1], w, h, fill=False, edgecolor=color, linewidth=linewidth\n",
    "        )\n",
    "        ax.add_patch(patch)\n",
    "        if len(boxes) < 70:\n",
    "            #score_txt = class_names[int(cls)]\n",
    "            score_txt = str(int(cls))\n",
    "            ax.text(x1, y1, score_txt, bbox={\"facecolor\": [1,1,0], \"alpha\": 0.4}, clip_box=ax.clipbox, clip_on=True,)\n",
    "        \n",
    "    plt.show()\n",
    "    return ax\n",
    "\n",
    "def display_data(X, BBOX, stride=1):\n",
    "    for i in range(len(X)):\n",
    "        if i%stride==0:\n",
    "            img_arr = X[i]\n",
    "            sample_box = BBOX[i]            \n",
    "            bbox = sample_box[:, :4]\n",
    "            label = sample_box[:, 4]\n",
    "\n",
    "            h, w, c = img_arr.shape\n",
    "            scale = np.array((w, h, w, h))\n",
    "            scale = np.reshape(scale, (1, 4))\n",
    "            bbox_norm = bbox.astype(np.float) * scale.astype(np.float)\n",
    "            #print('bbox_norm', bbox, bbox_norm)\n",
    "            print(i, np.unique(label))\n",
    "            ax = visualize_detections_simple(img_arr,bbox_norm,label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tile_object(X, BBOX, scope=0.5):\n",
    "    \n",
    "    crop_xs = []\n",
    "    crop_bboxs = []\n",
    "    for i in range(len(X)):\n",
    "        x = X[i]\n",
    "        img_h, img_w, img_c = x.shape\n",
    "        bbox = BBOX[i]\n",
    "        #print('len', len(x), len(bbox), x.shape, bbox.shape)\n",
    "        \n",
    "        x0 = bbox[:, 0]\n",
    "        y0 = bbox[:, 1]\n",
    "        x1 = bbox[:, 2]\n",
    "        y1 = bbox[:, 3]\n",
    "        cls = bbox[:, 4]\n",
    "        attributes = bbox[:, 4:]\n",
    "        \n",
    "        if not np.any(cls>0):\n",
    "            continue\n",
    "            \n",
    "        box_h = y1 - y0\n",
    "        box_w = x1 - x0\n",
    "        box_y_min = np.min(y0)\n",
    "        box_x_min = np.min(x0)        \n",
    "        box_y_max = np.max(y1)\n",
    "        box_x_max = np.max(x1)        \n",
    "        if box_y_max - box_y_min < scope and box_x_max - box_x_min < scope:\n",
    "            \n",
    "            cx = np.mean((box_x_min + box_x_max)/2)\n",
    "            \n",
    "            if cx < 0.5:\n",
    "                tx0 = np.maximum(0, cx - scope/2)\n",
    "                tx1 = tx0 + scope\n",
    "            else:\n",
    "                tx1 = np.minimum(1.0, cx + scope/2)\n",
    "                tx0 = tx1 - scope\n",
    "                        \n",
    "            tbox = np.stack(((x0 - tx0)/scope, y0, (x1 - tx0)/scope, y1), axis=1)            \n",
    "            tbox = np.concatenate((tbox, attributes), -1)\n",
    "            \n",
    "            img_x0 = int(tx0 * img_w)\n",
    "            img_x1 = img_x0 + int(img_w*scope)\n",
    "            timg = x[:, img_x0:img_x1]\n",
    "            \n",
    "            img = Image.fromarray(timg)\n",
    "            img_resized = img.resize((padded_image_shape[1]//2, padded_image_shape[0]))\n",
    "            arr_resized = np.array(img_resized)            \n",
    "            \n",
    "            crop_xs.append(arr_resized)\n",
    "            crop_bboxs.append(tbox)\n",
    "    return crop_xs, crop_bboxs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def attach_tiled_data(X, BBOX, row=2, col=2):\n",
    "    m = len(X)\n",
    "    attach_m = int(np.ceil(m/(row*col)))\n",
    "    attach_xs = []\n",
    "    attach_bboxs = []\n",
    "    img_h, img_w, img_c = X[0].shape\n",
    "    for i in range(attach_m):\n",
    "        bg_color = np.median(X[i])\n",
    "        attach_xs.append(bg_color + np.zeros((img_h*row, img_w*col, img_c)))    \n",
    "        attach_bboxs.append([])\n",
    "    \n",
    "    m_rand = np.arange(m)\n",
    "    np.random.shuffle(m_rand)\n",
    "    for i in range(len(m_rand)):\n",
    "        j = m_rand[i]\n",
    "        x = X[j]\n",
    "        bbox = BBOX[j]        \n",
    "        img_h, img_w, img_c = x.shape\n",
    "        ti = i//(row*col)\n",
    "        ty = i%(row*col)//col\n",
    "        tx = i%(row*col)%col\n",
    "        dst_y0 = ty * img_h\n",
    "        dst_y1 = dst_y0 + img_h\n",
    "        dst_x0 = tx * img_w\n",
    "        dst_x1 = dst_x0 + img_w\n",
    "               \n",
    "        attach_xs[ti][dst_y0:dst_y1, dst_x0:dst_x1] = x                    \n",
    "        #cls, x0, y0, x1, y1, gender, body = np.split(bbox, label_attribute_k, -1)\n",
    "        cbox, attributes = np.split(bbox, [4], -1)\n",
    "        x0, y0, x1, y1 = np.split(cbox, 4, -1)\n",
    "        \n",
    "        x_scale = 1.0 / col\n",
    "        y_scale = 1.0 / row\n",
    "        x0 = x0 * x_scale + tx * x_scale\n",
    "        y0 = y0 * y_scale + ty * y_scale\n",
    "        x1 = x1 * x_scale + tx * x_scale\n",
    "        y1 = y1 * y_scale + ty * y_scale\n",
    "        bbox = np.concatenate((x0, y0, x1, y1, attributes), axis=1)        \n",
    "        attach_bboxs[ti].extend(bbox)\n",
    "            \n",
    "    for i in range(len(attach_bboxs)):\n",
    "        attach_bboxs[i] = np.stack(attach_bboxs[i], 0)\n",
    "        \n",
    "    return attach_xs, attach_bboxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bg_paths():\n",
    "    list_bg_jpg = glob(folder_water_bg + '*')\n",
    "    list_bg_jpg0 = glob(folder_water_bg[:-1] + '0/*')\n",
    "    print(len(list_bg_jpg), len(list_bg_jpg0))\n",
    "    return list_bg_jpg0[1::2]#+ list_bg_jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def box_convert_cxy(box):\n",
    "    y0, x0, y1, x1 = np.split(box, 4, axis=-1)    \n",
    "    return np.concatenate(( (y0+y1)/2, (x0+x1)/2 ), axis=1)\n",
    "\n",
    "def box_swap_xy(box):\n",
    "    y0, x0, y1, x1 = np.split(box, 4, axis=-1)    \n",
    "    return np.concatenate((x0, y0, x1, y1), axis=1)\n",
    "\n",
    "def box_convert_to_xywh(boxes):\n",
    "    return np.concatenate(\n",
    "        [(boxes[..., :2] + boxes[..., 2:]) / 2.0, boxes[..., 2:] - boxes[..., :2]], axis=-1,)\n",
    "\n",
    "def box_convert_to_corners(boxes):    \n",
    "    return np.concatenate(\n",
    "        [boxes[..., :2] - boxes[..., 2:] / 2.0, boxes[..., :2] + boxes[..., 2:] / 2.0], axis=-1,)\n",
    "\n",
    "def angle_to_radian(angle):\n",
    "    return angle * np.pi/180\n",
    "\n",
    "def rotate_images(X, angle):\n",
    "    rotate_X = []\n",
    "    for i in range(len(X)):        \n",
    "        x = X[i]\n",
    "        img_h = x.shape[0]\n",
    "        img_w = x.shape[1]\n",
    "        img = Image.fromarray(x)                \n",
    "        img_rotated = img.rotate(angle)\n",
    "        rotate_X.append(np.array(img_rotated))\n",
    "\n",
    "    return rotate_X\n",
    "    \n",
    "def gen_rotate_data(X, BBOX, angle):\n",
    "    rotate_xs = []\n",
    "    rotate_bboxs = []\n",
    "    m = len(X)\n",
    "    for i in range(m):        \n",
    "        x = X[i]\n",
    "        \n",
    "        bbox = BBOX[i]       \n",
    "        attributes = bbox[:, 4:]        \n",
    "        box = bbox[:, :4]\n",
    "        box_xywh = box_convert_to_xywh(box)\n",
    "        box_xy = box_xywh[:, :2] \n",
    "        box_wh = box_xywh[:, 2:] \n",
    "        box_uv = (np.reshape(box_xy, [-1, 2]) - 0.5) * 2\n",
    "        \n",
    "        img_h, img_w, img_c = x.shape\n",
    "        img = Image.fromarray(x)        \n",
    "        scale_mat = np.array([1, 0, 0, 1.0*img_h/img_w]).reshape((2,2))\n",
    "        scale_mat_rev = np.array([1, 0, 0, 1.0*img_w/img_h]).reshape((2,2))\n",
    "        \n",
    "        angle = angle# + np.random.normal(scale=np.abs(angle))\n",
    "        radian = angle_to_radian(angle)        \n",
    "        rotate_mat = np.array([np.cos(radian), -np.sin(radian), np.sin(radian), np.cos(radian)])        \n",
    "        rotate_mat = np.reshape(rotate_mat, (2, 2))\n",
    "        box_uv_trans = np.matmul(box_uv, scale_mat)\n",
    "        box_uv_trans = np.matmul(box_uv_trans, rotate_mat)\n",
    "        box_uv_trans = np.matmul(box_uv_trans, scale_mat_rev)\n",
    "        box_trans = (box_uv_trans + 1)/2\n",
    "        box_trans_xy = np.reshape(box_trans, [-1, 2])\n",
    "        box_trans_xywh = np.concatenate((box_trans_xy, box_wh), axis=1)\n",
    "        box_trans = box_convert_to_corners(box_trans_xywh)\n",
    "   \n",
    "        #if np.min(box_trans)<0 or np.max(box_trans)>1: continue               \n",
    "        \n",
    "        bbox_trans = np.concatenate((box_trans, attributes), -1)\n",
    "   \n",
    "        img_rotated = img.rotate(angle)\n",
    "        #plt.imshow(img_rotated)\n",
    "        rotate_xs.append(np.array(img_rotated))\n",
    "        rotate_bboxs.append(bbox_trans)\n",
    "\n",
    "    return rotate_xs, rotate_bboxs        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "display_data(input_list, bbox_list, stride=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "m = len(input_list)\n",
    "print('m', m)\n",
    "s=2\n",
    "input_list_train = input_list[::s]\n",
    "input_list_test = input_list[1::2]\n",
    "bbox_list_train = bbox_list[::s]\n",
    "bbox_list_test = bbox_list[1::2]\n",
    "print('bbox_list_train', len(bbox_list), len(bbox_list_train), len(bbox_list_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "crop_xs, crop_bboxs = tile_object(input_list_train, bbox_list_train)\n",
    "print('crop_xs', len(crop_xs), len(crop_bboxs), crop_xs[0].shape, crop_bboxs[0])\n",
    "#display_data(crop_xs, crop_bboxs, stride=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rotate_xs, rotate_bboxs = gen_rotate_data(crop_xs, crop_bboxs, angle=0.5)\n",
    "rotate_xs_1, rotate_bboxs_1 = gen_rotate_data(crop_xs, crop_bboxs, angle=-0.5)\n",
    "\n",
    "print('rotate_xs', len(rotate_xs), len(rotate_bboxs))\n",
    "rotate_xs.extend(rotate_xs_1)\n",
    "rotate_bboxs.extend(rotate_bboxs_1)\n",
    "\n",
    "print('crop_xs', len(crop_xs), len(rotate_xs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attach_x, attach_bbox = attach_tiled_data(rotate_xs, rotate_bboxs, row=1, col=2)\n",
    "len(attach_x), len(attach_bbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_data(list_visdrone_x, list_visdrone_y, stride=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display_data(attach_x, attach_bbox, stride=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attach_xs, attach_bboxs = attach_tiled_data(input_list_PDESTRE, bbox_list_PDESTRE)\n",
    "len(attach_xs), len(attach_bboxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attach_xs.extend(attach_x)\n",
    "attach_bboxs.extend(attach_bbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(attach_xs), len(attach_bboxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "display_data(attach_xs, attach_bboxs, stride=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_list_train.extend(attach_xs)\n",
    "bbox_list_train.extend(attach_bboxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k0, c0 = bbox_list_train[0].shape\n",
    "k1, c1 = list_visdrone_y[0].shape\n",
    "c0, c1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_list_train.extend(list_visdrone_x)\n",
    "bbox_list_train.extend(list_visdrone_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('len', len(input_list_train), len(input_list_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_data(input_list_train, bbox_list_train, stride=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swap_xy(boxes):\n",
    "    \"\"\"Swaps order the of x and y coordinates of the boxes.\n",
    "    Arguments:\n",
    "      boxes: A tensor with shape `(num_boxes, 4)` representing bounding boxes.\n",
    "    Returns:\n",
    "      swapped boxes with shape same as that of boxes.\n",
    "    \"\"\"\n",
    "    return tf.stack([boxes[:, 1], boxes[:, 0], boxes[:, 3], boxes[:, 2]], axis=-1)\n",
    "\n",
    "\n",
    "def convert_to_xywh(boxes):\n",
    "    \"\"\"Changes the box format to center, width and height.\n",
    "    Arguments:\n",
    "      boxes: A tensor of rank 2 or higher with a shape of `(..., num_boxes, 4)`\n",
    "        representing bounding boxes where each box is of the format\n",
    "        `[xmin, ymin, xmax, ymax]`.\n",
    "    Returns:\n",
    "      converted boxes with shape same as that of boxes.\n",
    "    \"\"\"\n",
    "    return tf.concat(\n",
    "        [(boxes[..., :2] + boxes[..., 2:]) / 2.0, boxes[..., 2:] - boxes[..., :2]],\n",
    "        axis=-1,\n",
    "    )\n",
    "\n",
    "\n",
    "def convert_to_corners(boxes):\n",
    "    \"\"\"Changes the box format to corner coordinates\n",
    "    Arguments:\n",
    "      boxes: A tensor of rank 2 or higher with a shape of `(..., num_boxes, 4)`\n",
    "        representing bounding boxes where each box is of the format\n",
    "        `[x, y, width, height]`.\n",
    "    Returns:\n",
    "      converted boxes with shape same as that of boxes.\n",
    "    \"\"\"\n",
    "    return tf.concat(\n",
    "        [boxes[..., :2] - boxes[..., 2:] / 2.0, boxes[..., :2] + boxes[..., 2:] / 2.0],\n",
    "        axis=-1,\n",
    "    )\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "## Computing pairwise Intersection Over Union (IOU)\n",
    "As we will see later in the example, we would be assigning ground truth boxes\n",
    "to anchor boxes based on the extent of overlapping. This will require us to\n",
    "calculate the Intersection Over Union (IOU) between all the anchor\n",
    "boxes and ground truth boxes pairs.\n",
    "\"\"\"\n",
    "\n",
    "def compute_iou(boxes1, boxes2):#compute_iou(anchor_boxes, gt_boxes)\n",
    "    \"\"\"Computes pairwise IOU matrix for given two sets of boxes\n",
    "    Arguments:\n",
    "      boxes1: A tensor with shape `(N, 4)` representing bounding boxes\n",
    "        where each box is of the format `[x, y, width, height]`.\n",
    "        boxes2: A tensor with shape `(M, 4)` representing bounding boxes\n",
    "        where each box is of the format `[x, y, width, height]`.\n",
    "    Returns:\n",
    "      pairwise IOU matrix with shape `(N, M)`, where the value at ith row\n",
    "        jth column holds the IOU between ith box and jth box from\n",
    "        boxes1 and boxes2 respectively.\n",
    "    \"\"\"\n",
    "    boxes1_corners = convert_to_corners(boxes1)\n",
    "    boxes2_corners = convert_to_corners(boxes2)\n",
    "    lu = tf.maximum(boxes1_corners[:, None, :2], boxes2_corners[:, :2])\n",
    "    rd = tf.minimum(boxes1_corners[:, None, 2:], boxes2_corners[:, 2:])\n",
    "    intersection = tf.maximum(0.0, rd - lu)\n",
    "    intersection_area = intersection[:, :, 0] * intersection[:, :, 1]\n",
    "    boxes1_area = boxes1[:, 2] * boxes1[:, 3]\n",
    "    boxes2_area = boxes2[:, 2] * boxes2[:, 3]\n",
    "    union_area = tf.maximum(\n",
    "        boxes1_area[:, None] + boxes2_area - intersection_area, 1e-8\n",
    "    )\n",
    "    return tf.clip_by_value(intersection_area / union_area, 0.0, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "level_end - level_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "## Implementing Anchor generator\n",
    "Anchor boxes are fixed sized boxes that the model uses to predict the bounding\n",
    "box for an object. It does this by regressing the offset between the location\n",
    "of the object's center and the center of an anchor box, and then uses the width\n",
    "and height of the anchor box to predict a relative scale of the object. In the\n",
    "case of RetinaNet, each location on a given feature map has nine anchor boxes\n",
    "(at three scales and three ratios).\n",
    "\"\"\"\n",
    "class AnchorBox:\n",
    "    \"\"\"Generates anchor boxes.\n",
    "    This class has operations to generate anchor boxes for feature maps at\n",
    "    strides `[8, 16, 32, 64, 128]`. Where each anchor each box is of the\n",
    "    format `[x, y, width, height]`.\n",
    "    Attributes:\n",
    "      aspect_ratios: A list of float values representing the aspect ratios of\n",
    "        the anchor boxes at each location on the feature map\n",
    "      scales: A list of float values representing the scale of the anchor boxes\n",
    "        at each location on the feature map.\n",
    "      num_anchors: The number of anchor boxes at each location on feature map\n",
    "      areas: A list of float values representing the areas of the anchor\n",
    "        boxes for each feature map in the feature pyramid.\n",
    "      strides: A list of float value representing the strides for each feature\n",
    "        map in the feature pyramid.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.level_start = level_start\n",
    "        self.level_end = level_end\n",
    "        \n",
    "        if anchor_k==9:\n",
    "            self.aspect_ratios = [0.5, 1.0, 2.0]        \n",
    "            self.scales = [2 ** x for x in [0, 1 / 3, 2 / 3]]\n",
    "        else:\n",
    "            self.aspect_ratios = [1.0]        \n",
    "            self.scales = [2 ** x for x in [0]]\n",
    "                \n",
    "        self._num_anchors = len(self.aspect_ratios) * len(self.scales)\n",
    "        self._strides = [2 ** i for i in range(self.level_start, self.level_end)]\n",
    "        self._areas = [x ** 2 for x in [32.0, 64.0, 128.0, 196.0, 256.0]]                        \n",
    "        self._areas = self._areas[:level_end - level_start]\n",
    "        \n",
    "        self._anchor_dims = self._compute_dims()\n",
    "\n",
    "    def _compute_dims(self):\n",
    "        \"\"\"Computes anchor box dimensions for all ratios and scales at all levels\n",
    "        of the feature pyramid.\n",
    "        \"\"\"\n",
    "        anchor_dims_all = []\n",
    "        for area in self._areas:\n",
    "            anchor_dims = []\n",
    "            for ratio in self.aspect_ratios:\n",
    "                anchor_height = tf.math.sqrt(area / ratio)\n",
    "                anchor_width = area / anchor_height\n",
    "                dims = tf.reshape(\n",
    "                    tf.stack([anchor_width, anchor_height], axis=-1), [1, 1, 2]\n",
    "                )\n",
    "                for scale in self.scales:\n",
    "                    anchor_dims.append(scale * dims)\n",
    "            anchor_dims_all.append(tf.stack(anchor_dims, axis=-2))\n",
    "        return anchor_dims_all\n",
    "\n",
    "    def _get_anchors(self, feature_height, feature_width, level):\n",
    "        \"\"\"Generates anchor boxes for a given feature map size and level\n",
    "        Arguments:\n",
    "          feature_height: An integer representing the height of the feature map.\n",
    "          feature_width: An integer representing the width of the feature map.\n",
    "          level: An integer representing the level of the feature map in the\n",
    "            feature pyramid.\n",
    "        Returns:\n",
    "          anchor boxes with the shape\n",
    "          `(feature_height * feature_width * num_anchors, 4)`\n",
    "        \"\"\"\n",
    "        rx = tf.range(feature_width, dtype=tf.float32) + 0.5\n",
    "        ry = tf.range(feature_height, dtype=tf.float32) + 0.5\n",
    "        centers = tf.stack(tf.meshgrid(rx, ry), axis=-1) * self._strides[level - self.level_start]\n",
    "        centers = tf.expand_dims(centers, axis=-2)\n",
    "        centers = tf.tile(centers, [1, 1, self._num_anchors, 1])\n",
    "        dims = tf.tile(\n",
    "            self._anchor_dims[level - self.level_start], [feature_height, feature_width, 1, 1]\n",
    "        )\n",
    "        anchors = tf.concat([centers, dims], axis=-1)\n",
    "        return tf.reshape(\n",
    "            anchors, [feature_height * feature_width * self._num_anchors, 4]\n",
    "        )\n",
    "\n",
    "    def get_anchors(self, image_height, image_width):\n",
    "        \"\"\"Generates anchor boxes for all the feature maps of the feature pyramid.\n",
    "        Arguments:\n",
    "          image_height: Height of the input image.\n",
    "          image_width: Width of the input image.\n",
    "        Returns:\n",
    "          anchor boxes for all the feature maps, stacked as a single tensor\n",
    "            with shape `(total_anchors, 4)`\n",
    "        \"\"\"\n",
    "        anchors = [\n",
    "            self._get_anchors(\n",
    "                tf.math.ceil(image_height / 2 ** i),\n",
    "                tf.math.ceil(image_width / 2 ** i),\n",
    "                i,\n",
    "            )\n",
    "            for i in range(self.level_start, self.level_end)\n",
    "        ]\n",
    "        return tf.concat(anchors, axis=0)\n",
    "    \n",
    "    def get_anchors_check(self, image_height, image_width):\n",
    "        \"\"\"Generates anchor boxes for all the feature maps of the feature pyramid.\n",
    "        Arguments:\n",
    "          image_height: Height of the input image.\n",
    "          image_width: Width of the input image.\n",
    "        Returns:\n",
    "          anchor boxes for all the feature maps, stacked as a single tensor\n",
    "            with shape `(total_anchors, 4)`\n",
    "        \"\"\"\n",
    "        anchors = [\n",
    "            self._get_anchors(\n",
    "                tf.math.ceil(image_height / 2 ** i),\n",
    "                tf.math.ceil(image_width / 2 ** i),\n",
    "                i,\n",
    "            )\n",
    "            for i in range(self.level_start, self.level_end)\n",
    "        ]\n",
    "        return anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "level_start, level_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_check = AnchorBox()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "anchors = anchor_check.get_anchors_check(128,128)\n",
    "for anchor in anchors:\n",
    "    print(anchor.shape, anchor[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_image_shape, 128*8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## Preprocessing data\n",
    "Preprocessing the images involves two steps:\n",
    "- Resizing the image: Images are resized such that the shortest size is equal\n",
    "to 800 px, after resizing if the longest side of the image exceeds 1333 px,\n",
    "the image is resized such that the longest size is now capped at 1333 px.\n",
    "- Applying augmentation: Random scale jittering  and random horizontal flipping\n",
    "are the only augmentations applied to the images.\n",
    "Along with the images, bounding boxes are rescaled and flipped if required.\n",
    "\"\"\"\n",
    "\n",
    "def random_flip_horizontal(image, boxes):\n",
    "    \"\"\"Flips image and boxes horizontally with 50% chance\n",
    "    Arguments:\n",
    "      image: A 3-D tensor of shape `(height, width, channels)` representing an\n",
    "        image.\n",
    "      boxes: A tensor with shape `(num_boxes, 4)` representing bounding boxes,\n",
    "        having normalized coordinates.\n",
    "    Returns:\n",
    "      Randomly flipped image and boxes\n",
    "    \"\"\"\n",
    "    if tf.random.uniform(()) > 0.5:\n",
    "        image = tf.image.flip_left_right(image)\n",
    "        boxes = tf.stack([1 - boxes[:, 2], boxes[:, 1], 1 - boxes[:, 0], boxes[:, 3]], axis=-1)\n",
    "   \n",
    "    return image, boxes\n",
    "\n",
    "def resize_and_pad_image(\n",
    "    image, mask_obj=None, min_side=1024.0, max_side=11333.0, jitter=[128*8, 128*8+1], stride=128.0\n",
    "):\n",
    "   \n",
    "    image_shape = tf.cast(tf.shape(image)[:2], dtype=tf.float32)\n",
    "    \n",
    "    ratio = min_side / tf.reduce_min(image_shape)\n",
    "    \n",
    "    image_shape = ratio * image_shape\n",
    "    image = tf.image.resize(image, tf.cast(image_shape, dtype=tf.int32))\n",
    "    if mask_obj!=None:\n",
    "        mask_obj = tf.image.resize(mask_obj, tf.cast(image_shape, dtype=tf.int32))\n",
    "    \n",
    "    image = tf.image.pad_to_bounding_box(image, 0, 0, padded_image_shape[0], padded_image_shape[1]) \n",
    "    if mask_obj!=None:\n",
    "        mask_obj = tf.image.pad_to_bounding_box(mask_obj, 0, 0, padded_image_shape[0], padded_image_shape[1])        \n",
    "    if mask_obj!=None:\n",
    "        return image, image_shape, ratio, mask_obj\n",
    "    return image, image_shape, ratio\n",
    "\n",
    "def resize_and_pad_image_bbox(\n",
    "    image, bbox, mask_obj=None, min_side=1024.0, max_side=1024.0*4, jitter=[128*7+32, 128*8-32], stride=128.0\n",
    "):\n",
    "    #image, min_side=800.0, max_side=1333.0, jitter=[640, 1024], stride=128.0\n",
    "    \"\"\"Resizes and pads image while preserving aspect ratio.\n",
    "    1. Resizes images so that the shorter side is equal to `min_side`\n",
    "    2. If the longer side is greater than `max_side`, then resize the image\n",
    "      with longer side equal to `max_side`\n",
    "    3. Pad with zeros on right and bottom to make the image shape divisible by\n",
    "    `stride`\n",
    "    Arguments:\n",
    "      image: A 3-D tensor of shape `(height, width, channels)` representing an\n",
    "        image.\n",
    "      min_side: The shorter side of the image is resized to this value, if\n",
    "        `jitter` is set to None.\n",
    "      max_side: If the longer side of the image exceeds this value after\n",
    "        resizing, the image is resized such that the longer side now equals to\n",
    "        this value.\n",
    "      jitter: A list of floats containing minimum and maximum size for scale\n",
    "        jittering. If available, the shorter side of the image will be\n",
    "        resized to a random value in this range.\n",
    "      stride: The stride of the smallest feature map in the feature pyramid.\n",
    "        Can be calculated using `image_size / feature_map_size`.\n",
    "    Returns:\n",
    "      image: Resized and padded image.\n",
    "      image_shape: Shape of the image before padding.\n",
    "      ratio: The scaling factor used to resize the image\n",
    "    \"\"\"\n",
    "    image_shape = tf.cast(tf.shape(image)[:2], dtype=tf.float32)\n",
    "    if jitter is not None:\n",
    "        min_side = tf.random.uniform((), jitter[0], jitter[1], dtype=tf.float32)\n",
    "    ratio = min_side / tf.reduce_min(image_shape)\n",
    "    if ratio * tf.reduce_max(image_shape) > max_side:\n",
    "        ratio = max_side / tf.reduce_max(image_shape)\n",
    "    image_shape = ratio * image_shape\n",
    "    ratio_jitter = tf.random.uniform(tf.shape(image_shape), -32, 32, dtype=tf.float32)\n",
    "    image_shape += ratio_jitter      \n",
    "    image = tf.image.resize(image, tf.cast(image_shape, dtype=tf.int32))\n",
    "    if mask_obj!=None:\n",
    "        mask_obj = tf.image.resize(mask_obj, tf.cast(image_shape, dtype=tf.int32))\n",
    "    padded_image_shape = tf.cast(\n",
    "        tf.math.ceil(image_shape / stride) * stride, dtype=tf.int32\n",
    "    )\n",
    "    image = tf.image.pad_to_bounding_box(image, 0, 0, padded_image_shape[0], padded_image_shape[1])\n",
    "    if mask_obj!=None:\n",
    "        mask_obj = tf.image.pad_to_bounding_box(mask_obj, 0, 0, padded_image_shape[0], padded_image_shape[1])        \n",
    "    padded_image_shape = tf.cast(padded_image_shape, tf.float32)              \n",
    "    pad_ratio = tf.cast(image_shape, tf.float32) / padded_image_shape\n",
    "    bbox_padded = tf.stack(\n",
    "        [\n",
    "            bbox[:, 0] * pad_ratio[1],\n",
    "            bbox[:, 1] * pad_ratio[0],\n",
    "            bbox[:, 2] * pad_ratio[1],\n",
    "            bbox[:, 3] * pad_ratio[0],\n",
    "        ],\n",
    "        axis=-1,\n",
    "    )\n",
    "    if mask_obj!=None:\n",
    "        return image, padded_image_shape, ratio, bbox_padded, mask_obj    \n",
    "    return image, padded_image_shape, ratio, bbox_padded\n",
    "\n",
    "\n",
    "def unnormalize_box(bbox, image_shape):\n",
    "    bbox = tf.stack(\n",
    "        [\n",
    "            bbox[:, 0] * image_shape[1],\n",
    "            bbox[:, 1] * image_shape[0],\n",
    "            bbox[:, 2] * image_shape[1],\n",
    "            bbox[:, 3] * image_shape[0],\n",
    "        ],\n",
    "        axis=-1,\n",
    "    )\n",
    "    bbox = convert_to_xywh(bbox)    \n",
    "    return bbox    \n",
    "\n",
    "\n",
    "def preprocess_data(image, cls_bbox):\n",
    "    \"\"\"Applies preprocessing step to a single sample\n",
    "    Arguments:\n",
    "      sample: A dict representing a single training sample.\n",
    "    Returns:\n",
    "      image: Resized and padded image with random horizontal flipping applied.\n",
    "      bbox: Bounding boxes with the shape `(num_objects, 4)` where each box is\n",
    "        of the format `[x, y, width, height]`.\n",
    "      class_id: An tensor representing the class id of the objects, having\n",
    "        shape `(num_objects,)`.\n",
    "    \"\"\"\n",
    "     \n",
    "    bbox = cls_bbox[:, :4]    \n",
    "    attribute = tf.cast(cls_bbox[:, 4:], dtype=tf.int32)\n",
    "\n",
    "    image, bbox = random_flip_horizontal(image, bbox)        \n",
    "    image, image_shape, _, bbox = resize_and_pad_image_bbox(image, bbox)    \n",
    "    bbox = unnormalize_box(bbox, image_shape)\n",
    "    \n",
    "    return image, bbox, attribute\n",
    "\n",
    "def preprocess_test_data(image, cls_bbox):         \n",
    "    bbox = cls_bbox[:, :4]    \n",
    "    attribute = tf.cast(cls_bbox[:, 4:], dtype=tf.int32)\n",
    "    \n",
    "    image, image_shape, _ = resize_and_pad_image(image)\n",
    "    bbox = unnormalize_box(bbox, image_shape)    \n",
    "    \n",
    "    return image, bbox, attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_weather_effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_color_augment(x):\n",
    "    if tf.random.uniform(()) < -0.5:\n",
    "        x_max = tf.reduce_max(x, [1, 2], True)\n",
    "        x = x_max - x\n",
    "    if tf.random.uniform(()) < -0.2:\n",
    "        r, g, b = tf.split(x, 3, axis=-1)\n",
    "        x = tf.concat((r, b, g), -1)\n",
    "    elif tf.random.uniform(()) < -0.4:\n",
    "        r, g, b = tf.split(x, 3, axis=-1)\n",
    "        x = tf.concat((b, r, g), -1)\n",
    "    if tf.random.uniform(()) < 0.2:\n",
    "        x = tf.image.random_hue(x, 0.08)\n",
    "        x = tf.image.random_saturation(x, 0.6, 1.6)\n",
    "    if tf.random.uniform(()) < 0.2:\n",
    "        x = tf.image.random_brightness(x, 0.05)\n",
    "        x = tf.image.random_contrast(x, 0.7, 1.3)\n",
    "    if tf.random.uniform(()) < -0.2:\n",
    "        gray = tf.image.rgb_to_grayscale(x)\n",
    "        x = tf.concat((gray, gray, gray), -1)        \n",
    "    if tf.random.uniform(()) < -0.2:\n",
    "        noise = tf.random.normal(tf.shape(x), stddev=tf.pow(tf.reduce_mean(x), 0.3))\n",
    "        x += noise\n",
    "    if tf.random.uniform(()) < 0.2:\n",
    "        x = gaussian_filter2d(x, filter_shape=tuple(np.random.randint(1, 10, (2))), sigma=10)\n",
    "        #x = gaussian_filter2d(x, filter_shape=np.random.randint(3, 10, (2)), sigma=10)\n",
    "    if tf.random.uniform(()) < 0.2:        \n",
    "        x = sharpness(x, factor=10)\n",
    "    if tf.random.uniform(()) < 0.2:        \n",
    "        if use_weather_effect:\n",
    "            weather_k = len(weather_images)                            \n",
    "            h = tf.shape(x)[1]\n",
    "            w = tf.shape(x)[2]\n",
    "            weather_image = weather_images[np.random.randint(weather_k)]\n",
    "            weather_image = tf.image.resize(weather_image, tf.cast((h, w), dtype=tf.int32))\n",
    "            weather_image = tf.expand_dims(weather_image, 0)\n",
    "            x = (x // 3) * 2 + weather_image//3\n",
    "            \n",
    "    #x = tf.image.random_jpeg_quality(x, 0, 1.0)\n",
    "    #x = tf.clip_by_value(x, 0, 1)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## Encoding labels\n",
    "The raw labels, consisting of bounding boxes and class ids need to be\n",
    "transformed into targets for training. This transformation consists of\n",
    "the following steps:\n",
    "- Generating anchor boxes for the given image dimensions\n",
    "- Assigning ground truth boxes to the anchor boxes\n",
    "- The anchor boxes that are not assigned any objects, are either assigned the\n",
    "background class or ignored depending on the IOU\n",
    "- Generating the classification and regression targets using anchor boxes\n",
    "\"\"\"\n",
    "\n",
    "class LabelEncoder:\n",
    "    \"\"\"Transforms the raw labels into targets for training.\n",
    "    This class has operations to generate targets for a batch of samples which\n",
    "    is made up of the input images, bounding boxes for the objects present and\n",
    "    their class ids.\n",
    "    Attributes:\n",
    "      anchor_box: Anchor box generator to encode the bounding boxes.\n",
    "      box_variance: The scaling factors used to scale the bounding box targets.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self._anchor_box = AnchorBox()\n",
    "        self._box_variance = tf.convert_to_tensor(\n",
    "            [0.1, 0.1, 0.2, 0.2], dtype=tf.float32\n",
    "        )    \n",
    "    \n",
    "    def _compute_box_target(self, anchor_boxes, matched_gt_boxes):\n",
    "        \"\"\"Transforms the ground truth boxes into targets for training\"\"\"\n",
    "        box_target = tf.concat(\n",
    "            [\n",
    "                (matched_gt_boxes[:, :2] - anchor_boxes[:, :2]) / anchor_boxes[:, 2:],\n",
    "                tf.math.log(matched_gt_boxes[:, 2:] / anchor_boxes[:, 2:]),\n",
    "            ],\n",
    "            axis=-1,\n",
    "        )\n",
    "        box_target = box_target / self._box_variance\n",
    "        return box_target\n",
    "\n",
    "    def _match_anchor_boxes(\n",
    "        self, anchor_boxes, gt_boxes, match_iou=0.5, ignore_iou=0.3\n",
    "    ):\n",
    "        iou_matrix = compute_iou(anchor_boxes, gt_boxes)\n",
    "        max_iou = tf.reduce_max(iou_matrix, axis=1)#from anchor to object-box        \n",
    "        matched_gt_idx = tf.argmax(iou_matrix, axis=1)    \n",
    "        positive_mask = tf.greater_equal(max_iou, match_iou)# not only this, but also need max iou cell\n",
    "        \n",
    "        positive_proposal_mask = tf.greater_equal(iou_matrix, match_iou)\n",
    "        positive_mask = tf.reduce_any(positive_proposal_mask, axis=1)\n",
    "        \n",
    "        negative_mask = tf.less(max_iou, ignore_iou)\n",
    "        \n",
    "        max_iou_anchor = tf.reduce_max(iou_matrix, axis=0, keepdims=True) \n",
    "        max_iou_anchor_mask = tf.greater_equal(iou_matrix, max_iou_anchor)\n",
    "        positive_max_mask = tf.reduce_any(max_iou_anchor_mask, axis=1)\n",
    "        positive_mask = tf.logical_or(positive_mask, positive_max_mask)#new      \n",
    "        \n",
    "        negative_mask = tf.logical_and(negative_mask, tf.logical_not(positive_mask))\n",
    "        ignore_mask = tf.logical_not(tf.logical_or(positive_mask, negative_mask))        \n",
    "        \n",
    "        return (\n",
    "            matched_gt_idx,            \n",
    "            tf.cast(positive_mask, dtype=tf.float32),\n",
    "            tf.cast(positive_max_mask, dtype=tf.float32),            \n",
    "            tf.cast(ignore_mask, dtype=tf.float32),\n",
    "        )\n",
    "\n",
    "    def _encode_sample(self, image_shape, gt_boxes, box_label):\n",
    "        \"\"\"Creates box and classification targets for a single sample\"\"\"\n",
    "        \n",
    "        anchor_boxes = self._anchor_box.get_anchors(image_shape[1], image_shape[2])\n",
    "        box_label = tf.cast(box_label, dtype=tf.float32)\n",
    "        \n",
    "        bx, by, bw, bh = tf.split(gt_boxes, 4, -1)#finetune xywh original size\n",
    "        \n",
    "        bw = tf.squeeze(bw * bh, -1)\n",
    "        bw = tf.sqrt(bw)\n",
    "        \n",
    "        cls_ids, cls_gender, cls_body, upper_cloth = tf.split(box_label, 4, -1)\n",
    "        cls_ids = tf.squeeze(cls_ids, -1)\n",
    "        cls_gender = tf.squeeze(cls_gender, -1)\n",
    "        cls_body = tf.squeeze(cls_body, -1)\n",
    "        upper_cloth = tf.squeeze(upper_cloth, -1)\n",
    "        \n",
    "        matched_gt_idx, positive_mask, positive_max_mask, ignore_mask = self._match_anchor_boxes(\n",
    "            anchor_boxes, gt_boxes\n",
    "        )\n",
    "        matched_gt_boxes = tf.gather(gt_boxes, matched_gt_idx)\n",
    "        matched_gt_boxes_size = tf.reduce_prod(matched_gt_boxes[:, 2:], 1)\n",
    "        matched_gt_boxes_size = tf.sqrt(matched_gt_boxes_size)        \n",
    "        \n",
    "        box_target = self._compute_box_target(anchor_boxes, matched_gt_boxes)    \n",
    "        matched_bw = tf.gather(bw, matched_gt_idx)\n",
    "        matched_gt_cls_ids = tf.gather(cls_ids, matched_gt_idx)\n",
    "        matched_gt_gender = tf.gather(cls_gender, matched_gt_idx)\n",
    "        matched_cls_body = tf.gather(cls_body, matched_gt_idx)\n",
    "        matched_upper_cloth = tf.gather(upper_cloth, matched_gt_idx)\n",
    "        \n",
    "        cls_target = tf.where(tf.not_equal(positive_mask, 1.0), 0.0, matched_gt_cls_ids)        \n",
    "        cls_target = tf.where(tf.equal(ignore_mask, 1.0), -1.0, cls_target)\n",
    "        bw_thresh = tf.minimum(tf.reduce_min(matched_bw)+1, 10)\n",
    "        cls_target = tf.where(matched_bw < bw_thresh, -1.0, cls_target) #finetune\n",
    "        \n",
    "        attribute_target = tf.stack((cls_target, matched_gt_gender, matched_cls_body, matched_upper_cloth), -1)\n",
    "        targets = tf.concat([box_target, attribute_target], axis=-1)        \n",
    "        return targets\n",
    "    \n",
    "    def encode_batch(self, batch_images, gt_boxes, box_label):\n",
    "        \"\"\"Creates box and classification targets for a batch\"\"\"\n",
    "        \n",
    "        images_shape = tf.shape(batch_images)\n",
    "        batch_size = images_shape[0]\n",
    "       \n",
    "        labels = tf.TensorArray(dtype=tf.float32, size=batch_size, dynamic_size=True)\n",
    "        for i in range(batch_size):\n",
    "            label = self._encode_sample(images_shape, gt_boxes[i], box_label[i])\n",
    "            labels = labels.write(i, label)\n",
    "        \n",
    "        batch_images = tf.cast(batch_images, tf.float32)\n",
    "        labels = labels.stack()\n",
    "        \n",
    "        #label = tf.concat((label, is_flipped_anchor), -1)\n",
    "        return batch_images, labels      \n",
    "    \n",
    "    def encode_batch_train(self, batch_images, gt_boxes, cls):\n",
    "        \"\"\"Creates box and classification targets for a batch\"\"\"\n",
    "        \n",
    "        batch_images = image_color_augment(batch_images)#finetune        \n",
    "        return self.encode_batch(batch_images, gt_boxes, cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BifeaturePyramidNet(c345):\n",
    "    filters = 128\n",
    "    a2 = c345[0]\n",
    "    a3 = c345[1]\n",
    "    a4 = c345[2]\n",
    "    a5 = c345[3]\n",
    "    \n",
    "    regulizer  = tf.keras.regularizers.L2(l1)\n",
    "    \n",
    "    #b3 = Conv2D(filters, 1, 1, \"same\", groups=1, activation=activation, kernel_regularizer=regulizer)(a3)\n",
    "    #b4 = Conv2D(filters, 1, 1, \"same\", groups=1, activation=activation, kernel_regularizer=regulizer)(a4)\n",
    "    \n",
    "    a2_0 = Conv2D(filters, 1, 1, \"same\", groups=1, activation=activation, kernel_regularizer=regulizer)(a2)\n",
    "    #a3_0 = Conv2D(filters, 1, 1, \"same\", groups=1, activation=activation, kernel_regularizer=regulizer)(a3)\n",
    "    #a4_0 = Conv2D(filters, 1, 1, \"same\", groups=1, activation=activation, kernel_regularizer=regulizer)(a4)\n",
    "    #a5_0 = Conv2D(filters, 1, 1, \"same\", groups=1, activation=activation, kernel_regularizer=regulizer)(a5)\n",
    "    #a3_1 = Conv2D(filters, 1, 1, \"same\", groups=1, activation=activation, kernel_regularizer=regulizer)(a3)\n",
    "    #a4_1 = Conv2D(filters, 1, 1, \"same\", groups=1, activation=activation, kernel_regularizer=regulizer)(a4)\n",
    "    \n",
    "    a33 = Conv2D(filters*2, 1, 1, \"same\", groups=1, activation=activation, kernel_regularizer=regulizer)(a3)\n",
    "    a44 = Conv2D(filters*2, 1, 1, \"same\", groups=1, activation=activation, kernel_regularizer=regulizer)(a4)\n",
    "    a55 = Conv2D(filters*2, 1, 1, \"same\", groups=1, activation=activation, kernel_regularizer=regulizer)(a5)\n",
    "    a66 = Conv2D(filters*2, 3, 2, \"same\", groups=1, activation=activation, kernel_regularizer=regulizer)(a5)\n",
    "    \n",
    "    a3_0, a3_1 = tf.split(a33, 2, -1)\n",
    "    a4_0, a4_1 = tf.split(a44, 2, -1)\n",
    "    a5_0, a5_1 = tf.split(a55, 2, -1)\n",
    "    a6_0, a6_1 = tf.split(a66, 2, -1)\n",
    "    \n",
    "    b6 = a6_0\n",
    "    \n",
    "    a6_up = keras.layers.UpSampling2D(2)(a6_1)    \n",
    "    b5 = keras.layers.Add()([a5_0, a6_up])  \n",
    "        \n",
    "    a5_up = keras.layers.UpSampling2D(2)(a5_1)    \n",
    "    b4 = keras.layers.Add()([a4_0, a5_up])  \n",
    "    \n",
    "    b4_up = keras.layers.UpSampling2D(2)(b4)\n",
    "    b3 = keras.layers.Add()([a3_0, b4_up])  \n",
    "    \n",
    "    b3_up = keras.layers.UpSampling2D(2)(b3)\n",
    "    b2 = keras.layers.Add()([a2_0, b3_up])\n",
    "    \n",
    "    b2_down = Conv2D(filters, 3, 2, \"same\", groups=1, activation=activation, kernel_regularizer=regulizer)(b2)\n",
    "    b3_1 = Conv2D(filters, 1, 1, \"same\", groups=1, activation=activation, kernel_regularizer=regulizer)(b3)    \n",
    "    c3 = keras.layers.Add()([a3_1, b3_1, b2_down])\n",
    "    \n",
    "    c3_down = Conv2D(filters, 3, 2, \"same\", groups=1, activation=activation, kernel_regularizer=regulizer)(c3)\n",
    "    b4_1 = Conv2D(filters, 1, 1, \"same\", groups=1, activation=activation, kernel_regularizer=regulizer)(b4)    \n",
    "    c4 = keras.layers.Add()([a4_1, b4_1, c3_down])    \n",
    "    \n",
    "    c4_down = Conv2D(filters, 3, 2, \"same\", groups=1, activation=activation, kernel_regularizer=regulizer)(c4)\n",
    "    b5_1 = Conv2D(filters, 1, 1, \"same\", groups=1, activation=activation, kernel_regularizer=regulizer)(b5)    \n",
    "    c5 = keras.layers.Add()([a5_1, b5_1, c4_down])    \n",
    "    \n",
    "    c5_down = Conv2D(filters, 3, 2, \"same\", groups=1, activation=activation, kernel_regularizer=regulizer)(c5)\n",
    "    b6_1 = Conv2D(filters, 1, 1, \"same\", groups=1, activation=activation, kernel_regularizer=regulizer)(b6)    \n",
    "    c6 = keras.layers.Add()([a6_1, b6_1, c5_down])\n",
    "    \n",
    "    return c4, c5, c6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inputs = Input(shape=(3, 3, 2))  # 18   \n",
    "outputs = Conv2D(10, 3)(inputs)# 18 * 10 + 10 = 190\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "backbone = keras.applications.EfficientNetB2(include_top=False, input_shape=[64, 64, 3])\n",
    "backbone.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inputs = Input(shape=(3, 3, 2))      # 9 + 9\n",
    "outputs = Conv2D(10, 3, groups=2)(inputs) # 9*5 + 5 + 9*5 + 5\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_backbone():\n",
    "    backbone = keras.applications.EfficientNetB2(include_top=False, input_shape=[None, None, 3])\n",
    "    c2_output, c3_output, c4_output, c5_output = [\n",
    "        backbone.get_layer(layer_name).output\n",
    "        for layer_name in [\"block2c_add\", \"block3c_add\", \"block5d_add\", \"top_activation\"]]#block5c_add, block6d_add\n",
    "    #c4_output = (c4_output + c4a_output[:, :, :, :80])/2\n",
    "    return keras.Model(\n",
    "        inputs=[backbone.inputs], outputs=[c2_output, c3_output, c4_output, c5_output]\n",
    "    )\n",
    "backbone = get_backbone()\n",
    "#D0 for layer_name in [\"block2b_add\", \"block3b_add\", \"block5c_add\", \"block6d_add\"]]\n",
    "#D7 for layer_name in [\"block2f_add\", \"block3g_add\", \"block5j_add\", \"block6d_add\"]]\n",
    "#input                           (None, 64, 64, 3)   \n",
    "#block2b_add (Add)               (None, 16, 16, 24) \n",
    "#block3b_add (Add)               (None, 8, 8, 40)    \n",
    "#block4c_add (Add)               (None, 4, 4, 80)\n",
    "#block5c_add (Add)               (None, 4, 4, 112) \n",
    "#block6d_add (Add)               (None, 2, 2, 192)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone.trainable = True #finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def get_backbone():\n",
    "    backbone = keras.applications.MobileNetV2(include_top=False, input_shape=[None, None, 3])\n",
    "    c3_output, c4_output, c5_output = [\n",
    "        backbone.get_layer(layer_name).output\n",
    "        for layer_name in [\"block_6_expand_relu\", \"block_13_expand_relu\", \"out_relu\"]]\n",
    "    return keras.Model(\n",
    "        inputs=[backbone.inputs], outputs=[c3_output, c4_output, c5_output]\n",
    "    )\n",
    "backbone = get_backbone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createRetinaNet(num_classes, anchor_k, is_train=False):\n",
    "    prior_probability = tf.constant_initializer(-np.log((1 - 0.01) / 0.01))\n",
    "    inputs = Input(shape=(None, None, 3))        \n",
    "    \n",
    "    nets_3 = backbone(inputs, training=is_train)            \n",
    "    p456 = BifeaturePyramidNet(nets_3)    \n",
    "    \n",
    "    cls_outputs = []\n",
    "    box_outputs = []\n",
    "    \n",
    "    kernel_init = tf.initializers.he_normal()\n",
    "    regulizer = tf.keras.regularizers.L2(l1)\n",
    "    \n",
    "    conv_h0 = keras.layers.Conv2D(anchor_k * (5+num_classes), 3, padding=\"same\", kernel_initializer=kernel_init, bias_initializer=prior_probability, name='head_0')   \n",
    "    conv_h1 = keras.layers.Conv2D(anchor_k * (5+num_classes), 3, padding=\"same\", kernel_initializer=kernel_init, bias_initializer=prior_probability, name='head_1')   \n",
    "    conv_h2 = keras.layers.Conv2D(anchor_k * (5+num_classes), 3, padding=\"same\", kernel_initializer=kernel_init, bias_initializer=prior_probability, name='head_2')\n",
    "    conv_kernels = [conv_h0, conv_h1, conv_h2]\n",
    "    \n",
    "    drop = keras.layers.Dropout(0.00001)\n",
    "    N = tf.shape(nets_3[0])[0]\n",
    "    \n",
    "    cbox_outputs = []    \n",
    "    \n",
    "    for i in range(len(p456)):            \n",
    "        feature = p456[i]\n",
    "        conv_kernel = conv_kernels[i]\n",
    "        cls_out = conv_kernel(drop(feature))        \n",
    "        cbox_out = tf.reshape(cls_out, [N, -1, (5+num_classes)])\n",
    "        cbox_outputs.append(cbox_out)\n",
    "    \n",
    "    outputs = tf.concat(cbox_outputs, axis=1)  \n",
    "    \n",
    "    #outputs = tf.reduce_mean(tf.stack(tf.split(outputs, 4, -1),0),0)          \n",
    "    \n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)#dual    \n",
    "    #model = keras.Model(inputs=inputs, outputs={\"detect_output\": outputs, \"segment_output\": red})#dual    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _decode_box_predictions(anchor_boxes, box_predictions):\n",
    "    _box_variance = tf.convert_to_tensor([0.1, 0.1, 0.2, 0.2], dtype=tf.float32)\n",
    "    boxes = box_predictions * _box_variance\n",
    "    boxes = tf.concat(\n",
    "        [\n",
    "            boxes[:, :, :2] * anchor_boxes[:, :, 2:] + anchor_boxes[:, :, :2],\n",
    "            tf.math.exp(boxes[:, :, 2:]) * anchor_boxes[:, :, 2:],\n",
    "        ],\n",
    "        axis=-1,\n",
    "    )\n",
    "    boxes_transformed = convert_to_corners(boxes)\n",
    "    return boxes_transformed\n",
    "\n",
    "def decodePredictions(images, predictions, \n",
    "                      num_classes=num_classes,\n",
    "                      confidence_threshold=0.5,\n",
    "                      nms_iou_threshold=0.2,\n",
    "                      max_detections_per_class=1000,\n",
    "                      max_detections=1500,\n",
    "                      box_variance=[0.1, 0.1, 0.2, 0.2]):\n",
    "    \n",
    "    _anchor_box = AnchorBox()\n",
    "        \n",
    "    image_shape = tf.cast(tf.shape(images), dtype=tf.float32)\n",
    "    image_h = padded_image_shape[0]\n",
    "    image_w = padded_image_shape[1]\n",
    "    anchor_boxes = _anchor_box.get_anchors(image_shape[1], image_shape[2])#free size    \n",
    "    #anchor_boxes = _anchor_box.get_anchors(image_h, image_w)\n",
    "    box_predictions = predictions[:, :, :4]\n",
    "    objectness = tf.nn.sigmoid(predictions[:, :, 4:5])\n",
    "    cls_score = predictions[:, :, 5:5+num_classes_real]\n",
    "    cls_prob = tf.nn.softmax(cls_score)\n",
    "    cls_prob_max = tf.reduce_max(cls_prob, -1)\n",
    "    #cls_predictions = tf.round(objectness) * cls_predictions         \n",
    "    #cls_predictions = objectness\n",
    "    cls = tf.argmax(cls_score, -1)\n",
    "    cls = tf.cast(cls, tf.float32)\n",
    "    \n",
    "    boxes = _decode_box_predictions(anchor_boxes[None, ...], box_predictions)\n",
    "    boxes_2d = tf.reshape(boxes, [-1, 4])    \n",
    "    scores = tf.reshape(objectness, [-1, 1])#new\n",
    "    #scores = tf.sqrt(scores * tf.reshape(cls_prob_max, [-1, 1]))#new\n",
    "    cls = tf.reshape(cls, [-1, 1])\n",
    "    ccbox = tf.concat((cls, scores, boxes_2d), -1)\n",
    "    \n",
    "    selected_indices, selected_scores = tf.image.non_max_suppression_with_scores(    \n",
    "        ccbox[:, 2:],\n",
    "        ccbox[:, 1],        \n",
    "        max_detections,\n",
    "        nms_iou_threshold,\n",
    "        confidence_threshold,        \n",
    "    )\n",
    "    output = tf.gather(ccbox, selected_indices)        \n",
    "    return output   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetinaNetBoxLoss(tf.losses.Loss):\n",
    "    \"\"\"Implements Smooth L1 loss\"\"\"\n",
    "\n",
    "    def __init__(self, delta):\n",
    "        super(RetinaNetBoxLoss, self).__init__(\n",
    "            reduction=\"none\", name=\"RetinaNetBoxLoss\"\n",
    "        )\n",
    "        self._delta = delta\n",
    "\n",
    "    def call(self, y_true, y_pred):    \n",
    "        \n",
    "        difference = y_true - y_pred\n",
    "        absolute_difference = tf.abs(difference)        \n",
    "        loss = tf.where(\n",
    "            tf.less(absolute_difference, self._delta),\n",
    "            0.5 * (difference ** 2),\n",
    "            absolute_difference - 0.5,\n",
    "        )\n",
    "        loss = tf.where(loss < 0.01, 0.0, loss)#new marginal loss        \n",
    "        return tf.reduce_sum(loss, axis=-1)\n",
    "\n",
    "\n",
    "class RetinaNetClassificationLoss(tf.losses.Loss):\n",
    "    \"\"\"Implements Focal loss\"\"\"\n",
    "\n",
    "    def __init__(self, alpha, gamma, num_classes):\n",
    "        super(RetinaNetClassificationLoss, self).__init__(\n",
    "            reduction=\"none\", name=\"RetinaNetClassificationLoss\"\n",
    "        )\n",
    "        self._alpha = alpha\n",
    "        self._gamma = gamma\n",
    "        self._num_classes = num_classes\n",
    "        \n",
    "    def call(self, y_cls, y_pred):\n",
    "        y_cls = tf.cast(y_cls, dtype=tf.int32)\n",
    "        y_hot = tf.one_hot(y_cls, depth=self._num_classes, dtype=tf.float32,)\n",
    "        \n",
    "        y_positive = tf.cast(y_cls > 0, tf.float32)#finetune, 1:unknown\n",
    "        y_positive_identity = tf.cast(y_cls > 1, tf.float32)# 1:unknown\n",
    "        \n",
    "        obj_score = tf.identity(y_pred[:, :, 0], name='obj_score')\n",
    "        objectness = obj_score + tf.reduce_mean(y_pred[:, :, 1:]*0, axis=-1)\n",
    "       \n",
    "        pt = tf.nn.sigmoid(objectness)        \n",
    "        pt = tf.clip_by_value(pt, 1e-7, 1.0 - 1e-7)\n",
    "                \n",
    "        loss_p = - (1.0 - self._alpha) * tf.pow(1.0 - pt, self._gamma) * y_positive * tf.math.log(pt)        \n",
    "        loss_f = - self._alpha * tf.pow(pt, self._gamma) * (1 - y_positive) * tf.math.log(1 - pt)\n",
    "        loss_obj = loss_p + loss_f\n",
    "      \n",
    "        cls_pt = tf.nn.softmax(y_pred[:, :, 1:1+self._num_classes])        \n",
    "        cls_pt = tf.clip_by_value(cls_pt, 1e-7, 1.0 - 1e-7)\n",
    "        loss_cls_p = - tf.pow(1.0 - cls_pt, self._gamma) * y_hot * tf.math.log(cls_pt)\n",
    "        loss_cls_f = - tf.pow(cls_pt, self._gamma) * (1 - y_hot) * tf.math.log(1 - cls_pt)\n",
    "        loss_cls = tf.reduce_sum(loss_cls_p + loss_cls_f, axis=-1)        \n",
    "        loss_cls = y_positive_identity * loss_cls\n",
    "        loss = self._gamma * (loss_obj + loss_cls)\n",
    "        return loss    \n",
    "\n",
    "class GenderLoss(tf.losses.Loss):\n",
    "    \"\"\"Implements Focal loss\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(GenderLoss, self).__init__(\n",
    "            reduction=\"none\", name=\"GenderLoss\"\n",
    "        )\n",
    "        self._alpha = 0.5\n",
    "        self._gamma = 1.0\n",
    "        \n",
    "    def call(self, y_cls, y_pred):\n",
    "        #y : 0:unknown, 1, 2        \n",
    "        y_positive = tf.cast(y_cls > 0, tf.float32)\n",
    "        y_cls = tf.cast(y_cls - 1, dtype=tf.int32)\n",
    "        y_cls = tf.maximum(y_cls, 0)\n",
    "        y_hot = tf.one_hot(y_cls, depth=2, dtype=tf.float32,)\n",
    "        \n",
    "        pt = tf.nn.sigmoid(y_pred)        \n",
    "        pt = tf.clip_by_value(pt, 1e-7, 1.0 - 1e-7)\n",
    "                \n",
    "        loss_p = - (1.0 - self._alpha) * tf.pow(1.0 - pt, self._gamma) * y_positive * tf.math.log(pt)        \n",
    "        loss_f = - self._alpha * tf.pow(pt, self._gamma) * (1 - y_positive) * tf.math.log(1 - pt)\n",
    "        loss_obj = loss_p + loss_f\n",
    "        return loss_obj\n",
    "\n",
    "class BodyLoss(tf.losses.Loss):\n",
    "    def __init__(self):\n",
    "        super(BodyLoss, self).__init__(reduction=\"none\", name=\"BodyLoss\")\n",
    "        self._alpha = 0.5\n",
    "        self._gamma = 3.0\n",
    "\n",
    "    def call(self, y_cls, y_pred):\n",
    "        #y : 0:unknown, 1:thin, 2:medium, 3:fat         \n",
    "        y_positive = tf.cast(y_cls > 0, tf.float32)\n",
    "        y_cls = tf.cast(y_cls - 1, dtype=tf.int32)\n",
    "        y_cls = tf.maximum(y_cls, 0)\n",
    "        y_hot = tf.one_hot(y_cls, depth=body_volumn_k, dtype=tf.float32,)\n",
    "\n",
    "        pt = tf.nn.softmax(y_pred)        \n",
    "        pt = tf.clip_by_value(pt, 1e-7, 1.0 - 1e-7)\n",
    "\n",
    "        loss_p = - (1.0 - self._alpha) * tf.pow(1.0 - pt, self._gamma) * y_hot * tf.math.log(pt)        \n",
    "        loss_f = - self._alpha * tf.pow(pt, self._gamma) * (1 - y_hot) * tf.math.log(1 - pt)\n",
    "        loss_obj = tf.reduce_sum(loss_p + loss_f, -1)\n",
    "        return loss_obj\n",
    "\n",
    "class UpperClothLoss(tf.losses.Loss):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(UpperClothLoss, self).__init__(reduction=\"none\", name=\"UpperClothLoss\")\n",
    "        self._alpha = 0.5\n",
    "        self._gamma = 3.0\n",
    "\n",
    "    def call(self, y_cls, y_pred):\n",
    "        #y : 0:unknown, 1:thin, 2:medium, 3:fat         \n",
    "        y_positive = tf.cast(y_cls > 0, tf.float32)        \n",
    "        y_cls = tf.cast(y_cls - 1, dtype=tf.int32)\n",
    "        y_cls = tf.maximum(y_cls, 0)\n",
    "        y_hot = tf.one_hot(y_cls, depth=upper_cloth_k, dtype=tf.float32,)\n",
    "\n",
    "        pt = tf.nn.softmax(y_pred)        \n",
    "        pt = tf.clip_by_value(pt, 1e-7, 1.0 - 1e-7)\n",
    "\n",
    "        loss_p = - (1.0 - self._alpha) * tf.pow(1.0 - pt, self._gamma) * y_hot * tf.math.log(pt)        \n",
    "        loss_f = - self._alpha * tf.pow(pt, self._gamma) * (1 - y_hot) * tf.math.log(1 - pt)\n",
    "        loss_obj = tf.reduce_sum(loss_p + loss_f, -1)\n",
    "        return loss_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_k, body_volumn_k, upper_cloth_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetinaNetLoss(tf.losses.Loss):\n",
    "    \"\"\"Wrapper to combine both the losses\"\"\"\n",
    "\n",
    "    def __init__(self, num_classes=80, alpha=0.2, gamma=3.0, delta=1.0):#alpha=0.25\n",
    "        super(RetinaNetLoss, self).__init__(reduction=\"auto\", name=\"RetinaNetLoss\")\n",
    "        self._clf_loss = RetinaNetClassificationLoss(alpha, gamma, num_classes-1)\n",
    "        self._box_loss = RetinaNetBoxLoss(delta)\n",
    "        self._gender_loss = GenderLoss()\n",
    "        self._body_loss = BodyLoss()\n",
    "        self._upper_cloth_loss = UpperClothLoss()\n",
    "        self._num_classes = num_classes\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        # y_pred : tf.concat([box_outputs, cls_outputs], axis=-1)\n",
    "        #y_pred = tf.cast(y_pred, dtype=tf.float32)\n",
    "        \n",
    "        y_box = y_true[:, :, :4]\n",
    "        y_cls = y_true[:, :, 4]\n",
    "        y_gender = y_true[:, :, 5]\n",
    "        y_body = y_true[:, :, 6]\n",
    "        y_upper_cloth = y_true[:, :, 7]\n",
    "        \n",
    "        h_box = y_pred[:, :, :4]\n",
    "        h_obj = tf.nn.sigmoid(y_pred[:, :, 4])        \n",
    "        h_cls = y_pred[:, :, 4:-10]        \n",
    "        h_gender = y_pred[:, :, -1]#k2\n",
    "        h_body = y_pred[:, :, -body_volumn_k-1:-1]#k3\n",
    "        h_upper_cloth = y_pred[:, :, -upper_cloth_k-body_volumn_k-1:-body_volumn_k-1]\n",
    "      \n",
    "        positive_mask = tf.greater(y_cls, 0.0)\n",
    "        ignore_mask = tf.less(y_cls, 0.0)\n",
    "        \n",
    "        clf_loss = self._clf_loss(y_cls, h_cls)\n",
    "        box_loss = self._box_loss(y_box, h_box) \n",
    "        gender_loss = self._gender_loss(y_gender, h_gender)\n",
    "        body_loss = self._body_loss(y_body, h_body)\n",
    "        upper_cloth_loss = self._upper_cloth_loss(y_upper_cloth, h_upper_cloth)\n",
    "                                \n",
    "        clf_loss = tf.where(ignore_mask, 0.0, clf_loss)\n",
    "        attribute_loss = box_loss + gender_loss + body_loss + upper_cloth_loss\n",
    "        attribute_loss = tf.where(positive_mask, attribute_loss, 0.0)\n",
    "                \n",
    "        loss = clf_loss + attribute_loss\n",
    "      \n",
    "        positive_mask = tf.cast(positive_mask, tf.float32)        \n",
    "        normalizer = tf.reduce_sum(positive_mask, axis=-1)     \n",
    "        loss = tf.math.divide_no_nan(tf.reduce_sum(loss, axis=-1), normalizer)\n",
    "        \n",
    "        return loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(y_true, y_pred):    \n",
    "    y_cls = tf.cast(y_true[:, :, 4], tf.int32)\n",
    "    y_positive = tf.cast(y_cls > 0, tf.int32)\n",
    "    y_bg = tf.cast(tf.abs(y_cls)==0, tf.int32)\n",
    "    h_score = y_pred[:, :, 4]\n",
    "    h_prob = tf.nn.sigmoid(h_score)    \n",
    "    h_postive = tf.cast(tf.round(h_prob), tf.int32)\n",
    "    \n",
    "    true_positives = tf.cast(tf.logical_and(y_cls > 0, h_postive>0), tf.float32)\n",
    "    false_negative = y_positive * (1 - h_postive)\n",
    "                \n",
    "    tp = tf.reduce_sum(true_positives, axis=1)# + 0.01\n",
    "    fn = tf.reduce_sum(false_negative, axis=1)\n",
    "    tp = tf.cast(tp, tf.float32)\n",
    "    fn = tf.cast(fn, tf.float32)\n",
    "    \n",
    "    rec = tp / (tp + fn + 1e-8)\n",
    "    return rec\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    \n",
    "    y_cls_symbol = tf.cast(y_true[:, :, 4], dtype=tf.int32)    \n",
    "    y_cls_symbol = tf.cast(y_cls_symbol != 0, tf.int32)\n",
    "    h_obj_prob = tf.nn.sigmoid(y_pred[:, :, 4])\n",
    "    h_cls_symbol = tf.round(h_obj_prob)    \n",
    "    h_cls_symbol = tf.cast(h_cls_symbol, tf.int32)\n",
    "    \n",
    "    true_positives = y_cls_symbol * h_cls_symbol\n",
    "    false_positive = (1 - y_cls_symbol) * h_cls_symbol\n",
    "    \n",
    "    ones = tf.ones_like(true_positives)\n",
    "    zeeros = tf.zeros_like(true_positives)\n",
    "    true_positives = tf.cast(tf.equal(true_positives, ones), tf.float32)\n",
    "    false_positive = tf.cast(tf.equal(false_positive, ones), tf.float32)\n",
    "    \n",
    "    tp = tf.reduce_sum(true_positives, axis=1)# + 0.01\n",
    "    fp = tf.reduce_sum(false_positive, axis=1)\n",
    "    tp = tf.cast(tp, tf.float32)\n",
    "    fp = tf.cast(fp, tf.float32)\n",
    "    prec = tp / (tp + fp + 1e-8)\n",
    "    return prec\n",
    "\n",
    "def acc(y_true, y_pred):    \n",
    "    y_cls = tf.cast(y_true[:, :, 4], tf.int32)\n",
    "    y_positive = y_cls > 0\n",
    "    y_bg = tf.cast(tf.abs(y_cls)==0, tf.int32)\n",
    "    h_score = y_pred[:, :, 4]        \n",
    "    h_prob = tf.nn.sigmoid(h_score)\n",
    "    h_postive = tf.cast(tf.round(h_prob), tf.int32)\n",
    "    h_cls = tf.math.argmax(y_pred[:, :, 5:5+num_classes_real], -1, output_type=tf.int32)        \n",
    "    acc = tf.boolean_mask(tf.equal(y_cls, h_cls), y_positive)    \n",
    "    return acc\n",
    "\n",
    "def gender_acc(y_true, y_pred):    \n",
    "    y_cls = tf.cast(y_true[:, :, 4], tf.int32)\n",
    "    y_gender = tf.cast(y_true[:, :, 5], tf.int32)\n",
    "    y_positive = tf.logical_and(y_cls > 0, y_gender > 0)\n",
    "    \n",
    "    y_gender = tf.maximum(y_gender - 1, 0)    \n",
    "    h_prob = tf.nn.sigmoid(y_pred[:, :, -1])\n",
    "    h = tf.cast(tf.round(h_prob), tf.int32)    \n",
    "    acc = tf.boolean_mask(tf.equal(y_gender, h), y_positive)        \n",
    "    return acc\n",
    "\n",
    "def body_acc(y_true, y_pred):    \n",
    "    y_cls = tf.cast(y_true[:, :, 4], tf.int32)\n",
    "    y_gender = tf.cast(y_true[:, :, 6], tf.int32)\n",
    "    y_positive = tf.logical_and(y_cls > 0, y_gender > 0)\n",
    "    \n",
    "    y_gender = tf.maximum(y_gender - 1, 0)    \n",
    "    h_prob = tf.nn.softmax(y_pred[:, :, -3-1:-1])\n",
    "    h = tf.cast(tf.argmax(h_prob, -1), tf.int32)    \n",
    "    acc = tf.boolean_mask(tf.equal(y_gender, h), y_positive)        \n",
    "    return acc\n",
    "\n",
    "def up_cloth_acc(y_true, y_pred):    \n",
    "    y_cls = tf.cast(y_true[:, :, 4], tf.int32)\n",
    "    y_gender = tf.cast(y_true[:, :, 7], tf.int32)\n",
    "    y_positive = tf.logical_and(y_cls > 0, y_gender > 0)\n",
    "    \n",
    "    y_gender = tf.maximum(y_gender - 1, 0)\n",
    "    \n",
    "    h_score = y_pred[:, :, -upper_cloth_k-body_volumn_k-1:-body_volumn_k-1]    \n",
    "    h_prob = tf.nn.softmax(h_score)\n",
    "    h = tf.cast(tf.argmax(h_prob, -1), tf.int32)    \n",
    "    acc = tf.boolean_mask(tf.equal(y_gender, h), y_positive)        \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "anchor_k = len(label_encoder._anchor_box.aspect_ratios)*len(label_encoder._anchor_box.scales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bbox_list_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def generator():    \n",
    "    for i in range(len(input_list_train)):\n",
    "        x = input_list_train[i]\n",
    "        y_box = bbox_list_train[i]\n",
    "        if y_box.shape[1] < label_attribute_k:\n",
    "            z = np.zeros_like(y_box[:, :label_attribute_k - y_box.shape[1]])\n",
    "            y_box = np.concatenate((y_box, z), -1)\n",
    "        yield (x, y_box)\n",
    "\n",
    "def generator_test():    \n",
    "    for i in range(len(input_list_test)):\n",
    "        x = input_list_test[i]\n",
    "        y_box = bbox_list_test[i]\n",
    "        if y_box.shape[1] < label_attribute_k:\n",
    "            z = np.zeros_like(y_box[:, :label_attribute_k - y_box.shape[1]])\n",
    "            y_box = np.concatenate((y_box, z), -1)\n",
    "        yield (x, y_box)\n",
    "\n",
    "dataset = tf.data.Dataset.from_generator(\n",
    "    generator, \n",
    "    output_types=(tf.uint8, tf.float32), \n",
    "    output_shapes=(tf.TensorShape([None, None, 3]), tf.TensorShape([None, label_attribute_k])))\n",
    "dataset_test = tf.data.Dataset.from_generator(\n",
    "    generator_test, \n",
    "    output_types=(tf.uint8, tf.float32), \n",
    "    output_shapes=(tf.TensorShape([None, None, 3]), tf.TensorShape([None, label_attribute_k])))\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "for example in tfds.as_numpy(dataset):\n",
    "    image = example[0]\n",
    "    bbox = example[1]\n",
    "    print(image.dtype, image.shape, bbox.shape, bbox[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1 #finetune 1 or 4\n",
    "autotune = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_dataset = dataset.map(preprocess_data, num_parallel_calls=autotune)\n",
    "train_dataset = train_dataset.shuffle(8 * batch_size)\n",
    "train_dataset = train_dataset.padded_batch(batch_size=batch_size, padding_values=(0.0, 1e-8, -1), drop_remainder=False)\n",
    "#train_dataset = train_dataset.padded_batch(batch_size=batch_size)\n",
    "train_dataset = train_dataset.map(label_encoder.encode_batch_train, num_parallel_calls=autotune)\n",
    "train_dataset = train_dataset.prefetch(autotune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = dataset_test.map(preprocess_test_data, num_parallel_calls=autotune)\n",
    "val_dataset = val_dataset.padded_batch(batch_size=1, padding_values=(0.0, 1e-8, -1), drop_remainder=False)\n",
    "#val_dataset = val_dataset.padded_batch(batch_size=batch_size)\n",
    "val_dataset = val_dataset.map(label_encoder.encode_batch, num_parallel_calls=autotune)\n",
    "val_dataset = val_dataset.prefetch(autotune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=3, linewidth=200)\n",
    "image_height, image_width = padded_image_shape\n",
    "\n",
    "img_check = 0\n",
    "for image, output_map in train_dataset:\n",
    "    print('output_map', output_map.shape)\n",
    "    cbbox = output_map    \n",
    "    bbox = cbbox[:, :, :4]\n",
    "    cls_gt = cbbox[:,:,4]\n",
    "    img_m, image_height, image_width, image_ch = image.shape\n",
    "    anchor_feature_size = [(np.ceil(image_height / 2 ** i), np.ceil(image_width / 2 ** i)) \n",
    "                           for i in range(level_start, level_end)]\n",
    "    print('anchor_feature_size', anchor_feature_size)    \n",
    "    m = len(cbbox)    \n",
    "    positive_count = np.sum(cls_gt>0)\n",
    "    print('cbbox', cbbox.shape)\n",
    "    print('cls_sum',np.sum(cls_gt < 0.0), np.sum(cls_gt == 0.0), \n",
    "          np.sum(cls_gt == 1.0), np.sum(cls_gt > 1.0))\n",
    "    print('cls_mean',np.mean(cls_gt < 0.0), np.mean(cls_gt == 0.0), \n",
    "          np.mean(cls_gt == 1.0), np.mean(cls_gt > 0.0))\n",
    "    print('shape',image.shape, cbbox.shape,'unique', np.unique(cls_gt))\n",
    "    print('anchor_feature_size', anchor_feature_size)\n",
    "    offset = 0\n",
    "    positive_maps = []\n",
    "    for anchor_feature_size_1 in anchor_feature_size:        \n",
    "        fm_h, fm_w = anchor_feature_size_1\n",
    "        fm_h = int(fm_h)\n",
    "        fm_w = int(fm_w)        \n",
    "        fm_wh = int(fm_h * fm_w * anchor_k)\n",
    "        cbbox_anchor = cbbox[:, offset:offset+fm_wh, 4]\n",
    "        cbbox_anchor = np.reshape(cbbox_anchor, [m, fm_h, fm_w, anchor_k])\n",
    "        coount_m1 = np.count_nonzero(cbbox_anchor==-1)\n",
    "        coount_0 = np.count_nonzero(cbbox_anchor==0)\n",
    "        coount_1 = np.count_nonzero(cbbox_anchor==1)\n",
    "        coount_1_over = np.count_nonzero(cbbox_anchor>1)\n",
    "        positive_ratio = np.mean(cbbox_anchor>0)\n",
    "        positive_maps.append(cbbox_anchor>0)\n",
    "        print('cbbox_anchor', cbbox_anchor.shape, coount_m1, coount_0, coount_1, coount_1_over, 'ratio', positive_ratio)\n",
    "        sample_0_cbbox = cbbox_anchor[0]\n",
    "        sample_0_cbbox_sum = np.max(sample_0_cbbox, -1).astype(np.int)       \n",
    "      \n",
    "        offset += fm_wh\n",
    "        if False:            \n",
    "            file_name = str(fm_h)+ '_' + str(fm_w)+ '.txt'\n",
    "            np.savetxt(file_name,sample_0_cbbox_sum, fmt='%d',delimiter='')\n",
    "    img_check = image\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.max(positive_maps[0][0], -1))\n",
    "plt.title(str(positive_maps[0].shape)+ str(np.mean(positive_maps[0][0]))+ ' ' + str(np.sum(positive_maps[0][0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmap0 = np.array(Image.fromarray(np.max(positive_maps[0][0],-1)).resize((image_width, image_height)))\n",
    "pmap1 = np.array(Image.fromarray(np.max(positive_maps[1][0],-1)).resize((image_width, image_height)))\n",
    "pmap2 = np.array(Image.fromarray(np.max(positive_maps[2][0],-1)).resize((image_width, image_height)))\n",
    "#pmap3 = np.array(Image.fromarray(np.max(positive_maps[3][0],-1)).resize((image_width, image_height)))\n",
    "#pmap4 = np.array(Image.fromarray(np.max(positive_maps[4][0],-1)).resize((image_width, image_height)))\n",
    "pmap0 = pmap0.astype(np.uint8)\n",
    "pmap1 = pmap1.astype(np.uint8)\n",
    "pmap2 = pmap2.astype(np.uint8)\n",
    "pmap3 = 0#pmap3.astype(np.uint8)\n",
    "pmap4 = 0#pmap4.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pmap_with_img = np.array(img_check)[0]#*255\n",
    "pmap_with_img = pmap_with_img.astype(np.uint8)\n",
    "pmap_add = np.expand_dims(pmap0+pmap1+pmap2+pmap3+pmap4, -1)\n",
    "pmap = (pmap_add>0).astype(np.uint8)*255\n",
    "mix_rgb = np.concatenate((pmap, pmap_with_img[:,:,1:]),-1)\n",
    "plt.figure(figsize=(14,14))\n",
    "plt.imshow(mix_rgb)\n",
    "plt.title(str(np.mean(pmap_add)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_weight():   \n",
    "    weights_dir = path_weight#\"data\"\n",
    "    #latest_checkpoint = tf.train.latest_checkpoint(weights_dir)\n",
    "    latest_checkpoint = weights_dir \n",
    "    print('latest_checkpoint', latest_checkpoint)\n",
    "    model.load_weights(weights_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes, num_classes_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer = tf.optimizers.SGD(learning_rate=1e-5, momentum=0.1, clipvalue=5.)#warm up clipvalue=10. !\n",
    "optimizer = tf.optimizers.SGD(learning_rate=1e-4, momentum=0.1, clipvalue=10.)\n",
    "loss_detect = RetinaNetLoss(num_classes_real)\n",
    "model = createRetinaNet(num_classes, anchor_k)\n",
    "metrics = [recall, precision, acc, gender_acc, body_acc, up_cloth_acc]\n",
    "model.compile(loss=loss_detect, optimizer=optimizer, metrics=metrics)#[recall, precision, accuracy]\n",
    "callbacks_list = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=path_weight,\n",
    "        monitor=\"loss\",\n",
    "        save_best_only=False,\n",
    "        save_weights_only=True,\n",
    "        verbose=0,\n",
    "        save_freq=200\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "load_weight()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(input_list_train), len(input_list_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "resnet-34  81ms/step, resnet-24  53ms/step\n",
    "efficientB0  : 63ms/step - loss: 1.5916 - recall: 0.9155 - precision: 0.9330 - accuracy: 0.8751\n",
    "effi_single  : 50ms/step - loss: 1.7789 - recall: 0.9034 - precision: 0.9472 - accuracy: 0.9202\n",
    "eff-D7 Freeze: 182ms/step - loss: 2.1849 - recall: 0.9470 - precision: 0.9660 - accuracy: 0.9055 - flip_accuracy: 0.0052\n",
    "eff-D2 finetu: 85ms/step - loss: 0.7479 - recall: 0.9376 - precision: 0.9458\n",
    "'''\n",
    "out = model.evaluate(val_dataset.take(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "hist = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=None,#val_dataset.take(2)\n",
    "    epochs=epochs, \n",
    "    callbacks=callbacks_list,#callbacks_list\n",
    "    verbose=1,\n",
    ")\n",
    "'''\n",
    "\n",
    "effD2 Freeze:32s 474ms/step - loss: 14.7472 - recall: 0.4458 - precision: 0.6697 - accuracy: 0.0049\n",
    "effD2 fine :409s 494ms/step - loss:14.2226 - recall: 0.7724 - precision: 0.8478 - accuracy: 0.0278 - gender_accuracy: 0.5578                        \n",
    "            468s 505ms/step - loss: 6.2690 - recall: 0.9019 - precision: 0.9178 - acc: 0.2438 - gender_acc: 0.6009 - body_acc: 0.6215\n",
    "            480s 517ms/step - loss: 4.6561 - recall: 0.9110 - precision: 0.9214 - acc: 0.3581 - gender_acc: 0.6018 - body_acc: 0.6569\n",
    "            110s 473ms/step - loss: 4.0227 - recall: 0.8709 - precision: 0.9028 - acc: 0.4196 - gender_acc: 0.6024 - body_acc: 0.6746\n",
    "            844s 460ms/step - loss: 2.9238 - recall: 0.6501 - precision: 0.7888 - acc: 0.1660 - gender_acc: 0.6034 - body_acc: 0.6868 - up_cloth_acc: 0.5377\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for image, output_map in train_dataset:\n",
    "    print('output_map', output_map.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(path_weight)\n",
    "path_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#image = tf.keras.Input(shape=[padded_image_shape[0], padded_image_shape[1], 3], name=\"image\")\n",
    "image = tf.keras.Input(shape=[None, None, 3], name=\"image\")\n",
    "predictions = model(image, training=False)\n",
    "\n",
    "detections = decodePredictions(image, predictions, confidence_threshold=0.5, nms_iou_threshold=0.2)\n",
    "inference_model = tf.keras.Model(inputs=image, outputs=detections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def visualize_detections(\n",
    "    image, boxes, classes, scores, figsize=(12, 10), linewidth=1, color=[0, 0, 1], \n",
    "    boxes_gt=None):\n",
    "    \n",
    "    image = np.array(image, dtype=np.uint8)\n",
    "    image = attach_crop_image(image, boxes, max_crop=200)        \n",
    "    \n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(image)\n",
    "    ax = plt.gca()        \n",
    "   \n",
    "    if boxes_gt is not None:\n",
    "        for box in boxes_gt:        \n",
    "            x1, y1, x2, y2 = box\n",
    "            w, h = x2 - x1, y2 - y1\n",
    "            patch = plt.Rectangle(\n",
    "                [x1, y1], w, h, fill=False, edgecolor=[0,1,0], linewidth=2\n",
    "            )\n",
    "            ax.add_patch(patch)\n",
    "            \n",
    "    for box, cls, score in zip(boxes, classes, scores):        \n",
    "        x1, y1, x2, y2 = box\n",
    "        w, h = x2 - x1, y2 - y1\n",
    "        color_text = edgecolors[cls]\n",
    "        color = [0, 0, 1]\n",
    "        patch = plt.Rectangle(\n",
    "            [x1, y1], w, h, fill=False, edgecolor=color, linewidth=linewidth\n",
    "        )\n",
    "        ax.add_patch(patch)\n",
    "        if len(boxes) < 100:\n",
    "            score_txt = str.format('(%d)%.2f' %(cls, score))\n",
    "            ax.text(x1, y1, score_txt, bbox={\"facecolor\": color_text, \"alpha\": 0.4}, clip_box=ax.clipbox, clip_on=True,)\n",
    "          \n",
    "    plt.show()\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_test(test_datas, bboxs_label, step=1):\n",
    "    i = 0\n",
    "    for image, cbbox in test_datas: \n",
    "        if i%step==0:\n",
    "            #cbbox = output_map[\"detect\"]\n",
    "            #mask_obj = output_map[\"segment\"]  \n",
    "            bbox_annotation = bboxs_label[i]\n",
    "            scale = np.array(image.shape[:2])[::-1]\n",
    "            scale = np.reshape(scale, [1, 2])\n",
    "            scale = np.concatenate((scale, scale), 1)\n",
    "            gt_bbox = bbox_annotation[ :, :4] * scale\n",
    "               \n",
    "            input_image, _, ratio = resize_and_pad_image(image, jitter=None)\n",
    "            input_image = tf.expand_dims(input_image, axis=0)\n",
    "            #input_image = tf.cast(input_image, tf.uint8)\n",
    "            detected_box = inference_model.predict(input_image)\n",
    "                        \n",
    "            print(input_image.shape, 'detected_box', detected_box.shape, scale, 'ratio',ratio)\n",
    "            #(1, 1024, 1920, 3) detected_box (7, 6) [[1920 1080 1920 1080]] ratio tf.Tensor(0.94814813, shape=(), dtype=float32)\n",
    "            cls_h = detected_box[:, 0].astype(np.int)\n",
    "            scores = detected_box[:, 1]\n",
    "            box = detected_box[:, 2:] / ratio\n",
    "                        \n",
    "            visualize_detections(\n",
    "                image,\n",
    "                box,\n",
    "                cls_h,\n",
    "                scores,\n",
    "                boxes_gt=gt_bbox\n",
    "            )\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_test(dataset, bbox_list_train, step=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_test(dataset_test, bbox_list_test, step=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_wrong(X, Y, annot):\n",
    "    i=0\n",
    "    for ann in annot:\n",
    "        image, y = X[i], Y[i]\n",
    "        #print('ann', ann)            \n",
    "        i+=1\n",
    "        \n",
    "        scale = np.array(image.shape[:2])[::-1]\n",
    "        scale = np.reshape(scale, [1, 2])\n",
    "        scale = np.concatenate((scale, scale), 1)\n",
    "        gt_bbox = y[ :, :4] * scale\n",
    "        #gt_bbox = y[:, 1:1+4]\n",
    "        input_image, _, ratio = resize_and_pad_image(image, jitter=None)\n",
    "        input_image = tf.expand_dims(input_image, axis=0)\n",
    "        #input_image = tf.cast(input_image, tf.uint8)\n",
    "        detected_box = inference_model.predict(input_image)\n",
    "        if len(detected_box)>len(y):\n",
    "            print('ann', ann)\n",
    "            print('detected_box', detected_box.shape, 'gt', len(y))\n",
    "            #(1, 1024, 1920, 3) detected_box (7, 6) [[1920 1080 1920 1080]] ratio tf.Tensor(0.94814813, shape=(), dtype=float32)\n",
    "            cls_h = detected_box[:, 0].astype(np.int)\n",
    "            scores = detected_box[:, 1]\n",
    "            box = detected_box[:, 2:] / ratio\n",
    "\n",
    "            visualize_detections(\n",
    "                image,\n",
    "                box,\n",
    "                cls_h,\n",
    "                scores,\n",
    "                boxes_gt=gt_bbox\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "check_wrong(input_list, bbox_list, annotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_pad_32x_arr(image_arr):\n",
    "    #print('image_arr', image_arr.shape, image_arr.dtype)\n",
    "    stride = 32\n",
    "    img_h = image_arr.shape[0]\n",
    "    img_w = image_arr.shape[1]\n",
    "    img_c = image_arr.shape[2]\n",
    "    #print('add_pad_32x', image_arr.shape, img_h, img_w)\n",
    "    pad_h = (stride - (img_h % stride)) % stride\n",
    "    pad_w = (stride - (img_w % stride)) % stride\n",
    "    padded_h = img_h + pad_h\n",
    "    padded_w = img_w + pad_w\n",
    "    #print('pad_h', pad_h, 'pad_w', pad_w)\n",
    "    image_padded = np.zeros((padded_h, padded_w, img_c), dtype=np.uint8)\n",
    "    image_padded[:img_h, :img_w] = image_arr\n",
    "        \n",
    "    return image_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_bg(image, is_display=True, is_save=False, save_path=''):\n",
    "    scale = np.array(image.shape[:2])[::-1]\n",
    "    scale = np.reshape(scale, [1, 2])\n",
    "    scale = np.concatenate((scale, scale), 1)\n",
    "\n",
    "    #input_image, _, ratio = resize_and_pad_image(image, jitter=None)\n",
    "    input_image = add_pad_32x_arr(image)     \n",
    "    ratio = 1\n",
    "    input_image = tf.expand_dims(input_image, axis=0)\n",
    "\n",
    "    detected_box = inference_model.predict(input_image)        \n",
    "    if len(detected_box) > 0:\n",
    "        #print(input_image.shape, 'detected_box', detected_box.shape, scale, 'ratio',ratio)\n",
    "        #(1, 1024, 1920, 3) detected_box (7, 6) [[1920 1080 1920 1080]] ratio tf.Tensor(0.94814813, shape=(), dtype=float32)\n",
    "        if is_display:\n",
    "            cls_h = detected_box[:, 0].astype(np.int)\n",
    "            scores = detected_box[:, 1]\n",
    "            box = detected_box[:, 2:] / ratio\n",
    "\n",
    "            visualize_detections(\n",
    "                image,\n",
    "                box,\n",
    "                cls_h,\n",
    "                scores                \n",
    "            )\n",
    "        if is_save:\n",
    "            img = Image.fromarray(image)\n",
    "            img.save(save_path +'.jpg')\n",
    "        else:\n",
    "            pass\n",
    "    return int(len(detected_box)>0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_jpg = glob(folder_water_bg + '*')\n",
    "print(len(list_jpg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong = 0\n",
    "for i in range(0, len(list_jpg)//1):    \n",
    "    jpg = list_jpg[i]\n",
    "    #print('i', i, jpg.split(os.sep)[-1])\n",
    "    img = Image.open(jpg)\n",
    "    arr = np.array(img)\n",
    "    \n",
    "    n = check_bg(arr, is_display=True)\n",
    "    if n>0:\n",
    "        wrong += n\n",
    "        print(wrong, i, jpg)#414/815, 348/544, 6/242, 43/1522, 59/2043, 64/2375"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_model.save('./pedestrian_efficientDet-D2_save')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "avi Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import time\n",
    "def video_to_frames(input_loc):\n",
    "    \"\"\"Function to extract frames from input video file\n",
    "    and save them as separate frames in an output directory.\n",
    "    Args:\n",
    "        input_loc: Input video file.\n",
    "        output_loc: Output directory to save the frames.\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "        \n",
    "    folder_split = input_loc.split(os.sep)\n",
    "    file_name = folder_split[-1]\n",
    "    file_name = file_name.split('.')[0]\n",
    "    bg_dir = os.sep.join(folder_split[:-1]) + os.sep + 'bg' + os.sep\n",
    "    output_loc = bg_dir#input_loc[:-4]\n",
    "    print('output_loc', output_loc)\n",
    "    list_img = []\n",
    "    list_path = []\n",
    "    \n",
    "    try:\n",
    "        os.mkdir(output_loc)\n",
    "    except OSError:\n",
    "        pass\n",
    "    # Log the time\n",
    "    time_start = time.time()\n",
    "    # Start capturing the feed\n",
    "    cap = cv2.VideoCapture(input_loc)\n",
    "    # Find the number of frames\n",
    "    \n",
    "    video_length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) - 1\n",
    "    print (\"Number of frames: \", video_length)\n",
    "    count = 0\n",
    "    save_count = 0\n",
    "    print (\"Converting video..\\n\")\n",
    "    # Start converting the video\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        # Extract the frame\n",
    "        ret, frame = cap.read()\n",
    "        count = count + 1\n",
    "        if count%10!=0:continue\n",
    "        # Write the results back to output location.\n",
    "        dst_name = output_loc +\"/%s_%#05d.jpg\" % (file_name, count+1)\n",
    "        try:\n",
    "            #cv2.imwrite(file_name, frame)\n",
    "            b, g, r = np.split(frame, 3, -1)\n",
    "            rgb = np.concatenate((r,g,b), -1)        \n",
    "            \n",
    "            list_img.append(rgb)\n",
    "            list_path.append(file_name + '_' + str(save_count))\n",
    "            save_count += 1\n",
    "        except:\n",
    "            print('except')\n",
    "            break\n",
    "        \n",
    "        # If there are no more frames left\n",
    "        if count%100==0:\n",
    "            print('count', video_length, count)\n",
    "        if (count > (video_length*0.9)):\n",
    "            # Log the time again\n",
    "            break\n",
    "    \n",
    "    time_end = time.time()\n",
    "    # Release the feed\n",
    "    cap.release()\n",
    "    # Print stats\n",
    "    print (\"Done extracting frames.\\n%d frames extracted\" % count)\n",
    "    print (\"It took %d seconds to save %d for conversion .\" % (time_end-time_start, save_count))\n",
    "\n",
    "    return list_img, list_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_avi = '/home/mvlab/Downloads/dataset/water_movie_obj/'\n",
    "file_avi = folder_avi +'20201031-100620~20201031-100820_대야리2_332874518.avi'\n",
    "os.path.isdir(folder_avi), os.path.isfile(file_avi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_bg_avi = '/home/mvlab/Downloads/dataset/water_movie_bg/'\n",
    "path_bg_avi = glob(folder_bg_avi + '*.avi' )\n",
    "if len(path_bg_avi)>0:\n",
    "    file_avi = path_bg_avi[0]\n",
    "    print('file_avi', file_avi)\n",
    "os.path.isdir(folder_bg_avi), os.path.isfile(file_avi), len(path_bg_avi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "list_avi_arr, list_avi_path = video_to_frames(file_avi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_water_bg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_new_bg = '/home/mvlab/Downloads/dataset/water_movie_bg/bg0/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "wrong = 0\n",
    "for j in range(34, len(path_bg_avi)):\n",
    "    file_avi = path_bg_avi[j]\n",
    "    print('open_video',len(path_bg_avi), j, file_avi)\n",
    "    list_avi_arr, list_avi_path = video_to_frames(file_avi)\n",
    "    \n",
    "    for i in range(1, len(list_avi_arr), 1):    \n",
    "        arr = list_avi_arr[i]    \n",
    "        filename = folder_new_bg + list_avi_path[i]    \n",
    "        wrong += check_bg(arr, is_display=False, is_save=True, save_path=filename)\n",
    "        if i%100==0:\n",
    "            print(j, len(list_avi_arr), i, wrong)#414 815, 348 544\n",
    "    list_avi_arr.clear()\n",
    "    list_avi_path.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Freeze Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Convert Keras model to ConcreteFunction\n",
    "full_model = tf.function(lambda x: inference_model(x))\n",
    "full_model = full_model.get_concrete_function(\n",
    "    x=tf.TensorSpec(inference_model.inputs[0].shape, inference_model.inputs[0].dtype))\n",
    "\n",
    "# Get frozen ConcreteFunction\n",
    "frozen_func = convert_variables_to_constants_v2(full_model)\n",
    "frozen_func.graph.as_graph_def()\n",
    "\n",
    "layers = [op.name for op in frozen_func.graph.get_operations()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(type(frozen_func.outputs))\n",
    "frozen_list = frozen_func.outputs\n",
    "print(frozen_list)\n",
    "print(len(frozen_list))\n",
    "\n",
    "print(type(frozen_func.inputs))\n",
    "frozen_list = frozen_func.inputs\n",
    "print(frozen_list)\n",
    "print(len(frozen_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.io.write_graph(graph_or_graph_def=frozen_func.graph,\n",
    "                  logdir=\"./frozen_models\",\n",
    "                  name=\"pedestrian_efficientDet-D2_frozen_graph.pb\",\n",
    "                  as_text=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_error(X, Y, Path, step=1):\n",
    "    \n",
    "    for i in range(len(X)): \n",
    "        image = X[i]\n",
    "        bbox_annotation = Y[i]\n",
    "        path = Path[i]\n",
    "        \n",
    "        scale = np.array(image.shape[:2])[::-1]\n",
    "        scale = np.reshape(scale, [1, 2])\n",
    "        scale = np.concatenate((scale, scale), 1)\n",
    "        gt_bbox = bbox_annotation[:, 1:] * scale\n",
    "\n",
    "        input_image, _, ratio = resize_and_pad_image(image, jitter=None)\n",
    "        input_image = tf.expand_dims(input_image, axis=0)\n",
    "\n",
    "        detected_box = inference_model.predict(input_image)\n",
    "        detect_k = len(detected_box)\n",
    "        if detect_k!= len(bbox_annotation):\n",
    "            print(path, input_image.shape, 'detected_box', detected_box.shape, scale, 'ratio',ratio.numpy())\n",
    "            #(1, 1024, 1920, 3) detected_box (7, 6) [[1920 1080 1920 1080]] ratio tf.Tensor(0.94814813, shape=(), dtype=float32)\n",
    "            cls_h = detected_box[:, 0].astype(np.int)\n",
    "            scores = detected_box[:, 1]\n",
    "            box = detected_box[:, 2:] / ratio\n",
    "            #print('box', box)\n",
    "\n",
    "            visualize_detections(\n",
    "                image,\n",
    "                box,\n",
    "                cls_h,\n",
    "                scores,\n",
    "                boxes_gt=gt_bbox\n",
    "            )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 1000\n",
    "end = start + 100\n",
    "check_error(input_list[start:end], bbox_list[start:end], path_list[start:end])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load model from .pb file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model_pb = './pedestrian_efficientDet-D2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_model.save(saved_model_pb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_objects = {'recall':recall,'precision':precision}\n",
    "model_loaded = keras.models.load_model(saved_model_pb, custom_objects=custom_objects, compile=False)\n",
    "#model_loaded.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_loaded.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for image, cbbox in val_dataset: \n",
    "    \n",
    "    detected_box = model_loaded.predict(image)\n",
    "    print('detected_box', detected_box.shape)\n",
    "    if len(detected_box) > 0:\n",
    "        cls_h = detected_box[:, 0].astype(np.int)\n",
    "        scores = detected_box[:, 1]\n",
    "        box = detected_box[:, 2:]\n",
    "\n",
    "        visualize_detections(\n",
    "            image[0],\n",
    "            box,\n",
    "            cls_h,\n",
    "            scores\n",
    "        )    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_pb)\n",
    "#converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tflite_model = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the TF Lite model.\n",
    "with tf.io.gfile.GFile('model.tflite', 'wb') as f:\n",
    "  f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls *.tflite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Convert the model.\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(inference_model)\n",
    "tflite_model = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the TF Lite model.\n",
    "with tf.io.gfile.GFile('model.tflite', 'wb') as f:\n",
    "  f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load TFLite model and allocate tensors.\n",
    "interpreter = tf.lite.Interpreter(model_content=tflite_model)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensors.\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Test the TensorFlow Lite model on random input data.\n",
    "input_shape = input_details[0]['shape']\n",
    "input_details, output_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = input_list_train[0]\n",
    "input_data = (np.expand_dims(input_data, 0)/255).astype(np.float32)\n",
    "input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = input_data[:, :padded_image_shape[0], :padded_image_shape[1]]\n",
    "input_data.shape, np.max(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)\n",
    "print('input_data', input_data.shape)\n",
    "interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "\n",
    "interpreter.invoke()\n",
    "tflite_results = interpreter.get_tensor(output_details[0]['index'])\n",
    "tflite_results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tflite_results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detected_box = tflite_results\n",
    "cls_h = detected_box[:, 0].astype(np.int)\n",
    "scores = detected_box[:, 1]\n",
    "box = detected_box[:, 2:] / ratio\n",
    "#print('box', box)\n",
    "\n",
    "visualize_detections(\n",
    "    input_data[0]*255,\n",
    "    box,\n",
    "    cls_h,\n",
    "    scores,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
