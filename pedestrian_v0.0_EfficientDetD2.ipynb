{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_datasets as tfds\n",
    "import pandas as pd\n",
    "\n",
    "from PIL import Image\n",
    "from PIL import ImageFilter\n",
    "import ast\n",
    "import json\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-5405bb3c52bd>:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('2.3.1', True)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__, tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\" #CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_image_shape = (1024, 1920)\n",
    "anchor_k = 9\n",
    "num_classes = 300\n",
    "num_classes_real = num_classes\n",
    "max_data_m = 50000\n",
    "use_zoom_up_data = False\n",
    "level_start = 4\n",
    "level_end = 7\n",
    "l1 = 1e-9\n",
    "activation = 'swish'#'selu' is not converted to tflite\n",
    "kernel_init = tf.initializers.he_normal()\n",
    "edgecolors = np.random.rand(num_classes, 3) \n",
    "edgecolors = np.minimum(edgecolors+0.1, 1.0)\n",
    "class_names = ['bg', 'person']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_pedestrian = '/home/mvlab/Downloads/dataset/통영/'\n",
    "forder_P_DESTRE = '/home/mvlab/Downloads/dataset/P-DESTRE/'\n",
    "folder_weather = '/home/sogangori/Downloads/dataset/weather/'\n",
    "class_names = ['bg', '\"water\"', '\"waterf\"', '\"land\"', '\"animal\"']\n",
    "names = ['fn','cname','id', 'x0', 'y0', 'w', 'h']\n",
    "max_data_m = 10000\n",
    "edgecolors = np.random.rand(num_classes, 3) \n",
    "edgecolors = np.minimum(edgecolors+0.1, 1.0)\n",
    "path_weight = \"weight/pedestrian_efficientDet-D2\"\n",
    "\n",
    "os.path.isdir(path_pedestrian), os.path.isdir(forder_P_DESTRE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 9235)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths_avi = glob(path_pedestrian+'*.avi')\n",
    "paths_txt = glob(path_pedestrian+'*.txt')\n",
    "paths_img = glob(path_pedestrian+'*/*')\n",
    "len(paths_avi), len(paths_txt), len(paths_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/mvlab/Downloads/dataset/통영/bridge_img_01/172056_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171223_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170532_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170208_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/173451_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170822_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/173338_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172633_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171812_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170024_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170512_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171535_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/165954_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171008_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170128_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172443_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172520_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171541_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172148_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/165735_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172310_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171303_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170318_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170009_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172445_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172758_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/165620_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170228_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172830_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172343_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/165909_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172856_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172505_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/165849_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172923_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/173027_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171111_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171948_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172015_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172936_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172813_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/165815_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172438_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/165950_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170033_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/165731_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171536_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170218_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171636_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170103_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170105_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170223_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171302_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/165919_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/173137_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/173151_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172753_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/165945_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171927_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172258_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172943_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171507_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172956_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/173243_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172448_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172330_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/165740_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171353_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172605_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170842_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172846_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/165625_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171823_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170110_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170710_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172319_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172631_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172428_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172135_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/173303_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171233_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/165811_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170058_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172921_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/173147_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/165817_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170258_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/173212_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171040_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170243_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171029_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170659_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171726_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170038_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172735_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171833_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170100_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171917_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171942_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170111_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/173105_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172325_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171822_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170732_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172218_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172143_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/173229_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171918_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170150_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171258_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/173325_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171947_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172105_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170705_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170931_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/173236_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/165944_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/165646_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172208_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172515_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171520_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172313_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170004_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170949_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170756_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172158_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/173435_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172636_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170054_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/173112_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/173002_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170253_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171932_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172348_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/165940_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171157_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/173323_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170823_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172144_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170203_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/165654_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/165720_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172024_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170125_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171937_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172418_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172715_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172203_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170213_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/165635_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172951_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/173353_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172114_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172641_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172540_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/165712_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/165826_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/165751_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171150_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172621_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170924_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172557_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/165904_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170120_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171025_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170029_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172530_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/165839_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172714_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172331_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/165745_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170145_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172213_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170503_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171546_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170859_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170124_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/173032_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171030_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171641_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/173127_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/173310_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171101_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/173318_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/165914_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172159_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170845_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/173420_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/165822_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170812_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171543_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/165730_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171408_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170010_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171227_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/165928_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171309_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172205_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170129_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172730_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/165810_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172125_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172130_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171100_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170719_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/173110_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171958_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171333_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/165821_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172405_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/173107_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172401_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170005_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172748_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/165851_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172012_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170337_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/173019_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/173343_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171206_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171808_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172210_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/165726_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172345_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170155_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171327_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170405_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171517_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172450_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/173340_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170805_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172725_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172546_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171403_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170839_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171731_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171922_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/173037_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171140_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/173014_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172836_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170444_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/173102_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172246_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171054_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171256_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170926_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171716_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170517_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/173122_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/165705_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/173227_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172941_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172326_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172019_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170954_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171515_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/165757_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170339_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171736_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171155_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170946_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/173350_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172113_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172646_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170911_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/165924_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172841_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/165833_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170135_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172516_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170034_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/165650_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170434_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170847_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172820_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/165850_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/173022_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170902_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/165656_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172154_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171044_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172225_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172139_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/173421_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/165900_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171243_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170554_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171316_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171446_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170737_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/173415_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171211_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171556_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171419_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172400_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172510_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170700_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172034_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171712_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170356_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/173335_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/173355_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172946_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171116_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170308_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/173132_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172541_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170042_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172906_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170200_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170724_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/image_label.txt',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/165710_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172423_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/173440_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/173253_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/165746_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170414_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171218_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170627_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172003_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171308_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/165700_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172149_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171035_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/165855_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170158_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171631_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171656_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/173328_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170130_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170313_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/165857_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172433_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172826_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172320_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170649_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172550_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/165655_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171751_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172535_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170654_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170749_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/173441_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171741_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170852_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170421_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170020_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170457_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172931_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171357_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/165915_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170026_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172425_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172742_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171829_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172545_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171611_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172124_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/173217_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/165920_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/173431_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171418_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/173401_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170857_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172928_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171626_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170355_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170040_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170552_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/173146_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172007_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171500_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170400_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171603_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170233_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171441_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/173238_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/165801_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/173248_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/173416_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171000_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172250_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171342_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/173315_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172118_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170025_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170333_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/173400_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172315_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170938_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/173258_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170204_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170729_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/165825_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/173128_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171020_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170617_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/165930_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171606_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172926_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170616_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170030_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171651_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171130_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/165807_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172901_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171756_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/173320_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/173007_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172656_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171903_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170537_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/173142_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170754_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170045_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172440_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171251_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170810_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/165831_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172902_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171050_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172008_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/173426_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172420_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172002_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171817_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172153_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170916_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172759_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172220_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172055_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172245_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171458_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170806_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172255_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170608_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172229_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171711_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/173430_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172039_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/173042_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171450_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/165806_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172703_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171024_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/165645_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171621_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/165949_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/173345_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/165816_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172915_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172045_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172810_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171952_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/165736_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171845_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171352_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/165939_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/165905_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171348_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170140_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/165845_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/165721_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/165715_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172616_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/165755_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171742_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/173017_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172408_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170153_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/173406_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170941_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170545_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/173425_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171455_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/165830_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172234_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171554_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/165835_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172500_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170403_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172353_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170759_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/173117_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172950_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172119_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/173115_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/173456_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171442_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171253_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171437_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/165711_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172129_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172101_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172355_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171624_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171746_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170048_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171214_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171721_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171049_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/173222_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172304_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/165725_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172050_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171512_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170736_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/173004_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172916_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172643_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/173012_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171729_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170059_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170508_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171908_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172300_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172805_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/173232_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170220_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170248_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/173300_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/165859_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171238_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171010_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171705_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172729_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170742_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171436_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170323_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/165717_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171839_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172504_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170652_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/173348_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171055_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172106_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172350_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171828_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170115_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/165820_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170238_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170427_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171015_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170245_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172911_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/165854_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171201_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170622_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170912_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/165641_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170148_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172722_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/173305_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/165934_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170320_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/165614_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171219_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170014_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170750_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170634_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/165828_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172435_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/173436_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170632_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/173220_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171803_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/173100_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172555_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170015_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/173047_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/165840_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/173157_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170303_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170557_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/173405_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171844_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/165925_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/173410_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172413_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170442_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171531_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171152_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172415_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171413_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170106_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170447_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171646_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172335_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170134_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171121_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/165615_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170603_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171801_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/165929_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/173245_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171957_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170342_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172626_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170055_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171505_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/165706_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/173044_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171429_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170829_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170151_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170116_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170955_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172455_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172223_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172648_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171610_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171616_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172410_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171701_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170328_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172430_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171248_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170527_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172743_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170637_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171135_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170146_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172750_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172029_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171551_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171106_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170432_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/165935_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172601_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/165902_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171953_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/165610_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171005_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/165701_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171629_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171045_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170647_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172606_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172815_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170906_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/165756_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172305_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/165640_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172230_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171525_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/173313_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172720_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/173123_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170331_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170350_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170050_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170410_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170815_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/165812_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170458_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/165750_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170143_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170053_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/173202_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171850_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170035_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/173152_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172831_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172818_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172235_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/165910_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170408_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/173057_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170205_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172215_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/165629_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170019_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170936_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/173446_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/165802_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172710_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171530_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170907_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170210_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171601_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171453_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171734_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170727_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/173330_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172628_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172851_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170300_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172100_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171338_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/165630_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/173333_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/165955_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170707_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171858_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170000_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170642_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/173250_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/165805_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172955_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172525_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172340_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/165716_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171522_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/165959_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172651_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170416_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172857_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/173052_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/165651_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170817_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/173255_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170522_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/165800_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171818_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171706_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/165636_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170345_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172240_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/173411_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171634_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172338_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172026_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170921_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171510_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172134_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170429_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171145_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/173308_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/165741_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171336_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/173207_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/165844_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171423_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170452_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170133_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/170138_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/172611_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171322_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/171125_raw.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/02539.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/00794.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/06894.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/01148.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/03364.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/07626.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/04413.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/02684.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/00065.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/07375.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/05124.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/02127.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/01640.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/04036.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/07256.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/04414.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/07022.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/03709.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/01540.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/01770.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/05726.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/03670.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/05643.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/04224.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/01537.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/02686.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/02384.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/03754.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/04914.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/00628.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/05986.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/02255.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/08166.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/05532.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/04206.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/02599.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/04179.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/01767.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/08254.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/06033.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/02275.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/03392.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/05914.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/07102.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/03611.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/07118.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/07641.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/04444.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/00294.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/03984.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/03788.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/04575.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/07156.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/01721.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/08081.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/06154.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/08406.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/01005.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/05948.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/07322.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/06667.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/02771.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/01205.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/05479.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/06639.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/05569.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/06770.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/01723.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/03449.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/02205.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/07883.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/01595.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/00198.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/00538.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/06948.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/03797.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/02095.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/00082.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/06903.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/04497.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/08086.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/06387.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/08090.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/04907.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/00914.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/02034.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/04169.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/00351.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/03510.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/06062.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/08120.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/05857.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/05684.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/03027.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/06365.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/01914.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/00950.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/00228.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/02796.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/00032.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/01104.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/06305.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/04909.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/07851.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/06885.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/04360.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/07700.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/04174.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/03281.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/06999.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/03021.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/03140.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/01125.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/02461.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/04838.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/07967.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/00139.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/07120.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/00189.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/07311.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/02800.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/07140.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/07214.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/05164.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/07654.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/06308.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/04166.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/05767.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/00762.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/03304.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/00386.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/06271.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/03701.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/04461.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/00471.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/06792.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/03415.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/07804.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/01833.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/02415.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/05751.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/03503.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/07761.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/06349.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/03809.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/02807.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/05246.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/05103.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/00320.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/06743.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/07410.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/06839.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/06696.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/02552.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/08324.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/00269.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/05262.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/00210.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/07658.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/05794.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/06978.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/07193.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/03736.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/05475.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/06587.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/07927.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/01523.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/00976.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/06084.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/01141.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/07157.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/03915.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/06663.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/02054.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/05360.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/07435.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/04230.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/00076.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/04298.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/00114.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/08396.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/06672.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/04078.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/01265.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/01152.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/06788.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/05422.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/02791.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/01755.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/01008.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/06526.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/01051.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/01611.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/06272.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/08403.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/02946.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/03587.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/00921.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/07514.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/06243.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/01316.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/04605.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/01460.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/02928.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/01696.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/06350.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/00623.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/06675.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/00345.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/07866.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/00952.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/07327.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/07458.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/01539.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/07274.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/08067.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/04081.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/05847.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/02237.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/01786.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/04248.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/03578.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/00310.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/04906.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/04505.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/07789.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/02585.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/04066.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/05938.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/06801.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/05964.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/05425.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/07164.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/01291.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/05559.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/06068.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/03376.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/01180.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/00239.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/02568.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/00433.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/02220.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/06684.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/05611.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/05865.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/02281.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/00666.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/02103.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/05111.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/07428.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/06912.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/01259.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/02752.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/02451.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/01688.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/06191.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/00326.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/07298.jpg',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/01069.jpg',\n",
       " ...]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, '/home/sogangori/Downloads/dataset/weather/')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_weather_effect = os.path.isdir(folder_weather)\n",
    "use_weather_effect, folder_weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_images = []\n",
    "if use_weather_effect:\n",
    "    path_weathers = glob(folder_weather + '*.*')\n",
    "    if len(path_weathers)>0:\n",
    "        for path_weather in path_weathers:            \n",
    "            image = Image.open(path_weather) \n",
    "            weather_images.append(image)            \n",
    "            plt.imshow(image)\n",
    "            plt.show()\n",
    "    else:\n",
    "        use_weather_effect = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### video label load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1327, 7)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_img = pd.read_csv(paths_txt[0], header=None, names=names)\n",
    "df_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fn</th>\n",
       "      <th>cname</th>\n",
       "      <th>id</th>\n",
       "      <th>x0</th>\n",
       "      <th>y0</th>\n",
       "      <th>w</th>\n",
       "      <th>h</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2322</td>\n",
       "      <td>person</td>\n",
       "      <td>0</td>\n",
       "      <td>708</td>\n",
       "      <td>832</td>\n",
       "      <td>114</td>\n",
       "      <td>244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2323</td>\n",
       "      <td>person</td>\n",
       "      <td>0</td>\n",
       "      <td>710</td>\n",
       "      <td>824</td>\n",
       "      <td>114</td>\n",
       "      <td>244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2324</td>\n",
       "      <td>person</td>\n",
       "      <td>0</td>\n",
       "      <td>708</td>\n",
       "      <td>818</td>\n",
       "      <td>114</td>\n",
       "      <td>244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2325</td>\n",
       "      <td>person</td>\n",
       "      <td>0</td>\n",
       "      <td>709</td>\n",
       "      <td>817</td>\n",
       "      <td>114</td>\n",
       "      <td>244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2326</td>\n",
       "      <td>person</td>\n",
       "      <td>0</td>\n",
       "      <td>708</td>\n",
       "      <td>812</td>\n",
       "      <td>114</td>\n",
       "      <td>244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1322</th>\n",
       "      <td>3644</td>\n",
       "      <td>person</td>\n",
       "      <td>0</td>\n",
       "      <td>748</td>\n",
       "      <td>276</td>\n",
       "      <td>18</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1323</th>\n",
       "      <td>3645</td>\n",
       "      <td>person</td>\n",
       "      <td>0</td>\n",
       "      <td>749</td>\n",
       "      <td>277</td>\n",
       "      <td>18</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1324</th>\n",
       "      <td>3646</td>\n",
       "      <td>person</td>\n",
       "      <td>0</td>\n",
       "      <td>749</td>\n",
       "      <td>277</td>\n",
       "      <td>18</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1325</th>\n",
       "      <td>3647</td>\n",
       "      <td>person</td>\n",
       "      <td>0</td>\n",
       "      <td>748</td>\n",
       "      <td>276</td>\n",
       "      <td>19</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1326</th>\n",
       "      <td>3648</td>\n",
       "      <td>person</td>\n",
       "      <td>0</td>\n",
       "      <td>749</td>\n",
       "      <td>275</td>\n",
       "      <td>19</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1327 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        fn   cname  id   x0   y0    w    h\n",
       "0     2322  person   0  708  832  114  244\n",
       "1     2323  person   0  710  824  114  244\n",
       "2     2324  person   0  708  818  114  244\n",
       "3     2325  person   0  709  817  114  244\n",
       "4     2326  person   0  708  812  114  244\n",
       "...    ...     ...  ..  ...  ...  ...  ...\n",
       "1322  3644  person   0  748  276   18   38\n",
       "1323  3645  person   0  749  277   18   38\n",
       "1324  3646  person   0  749  277   18   38\n",
       "1325  3647  person   0  748  276   19   40\n",
       "1326  3648  person   0  749  275   19   40\n",
       "\n",
       "[1327 rows x 7 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_name_dict 21\n"
     ]
    }
   ],
   "source": [
    "def set_frame_path(df, paths_img):\n",
    "    frame = df['fn'].values\n",
    "    \n",
    "    file_name_dict = dict()\n",
    "    \n",
    "    for path_img in paths_img:\n",
    "        file_name = path_img.split(os.sep)[-1]\n",
    "        file_name = file_name.split('.')[0]\n",
    "        try:\n",
    "            file_name = int(file_name)\n",
    "        except:\n",
    "            pass\n",
    "        file_name_dict[file_name] = path_img\n",
    "            \n",
    "    print('file_name_dict', len(file_name_dict)) \n",
    "    \n",
    "    list_path = []\n",
    "            \n",
    "    for fr in frame:\n",
    "        if fr in file_name_dict.keys():\n",
    "            list_path.append(file_name_dict[fr])\n",
    "        else:\n",
    "            list_path.append(None)\n",
    "            \n",
    "    df['path'] = list_path    \n",
    "    return df_img\n",
    "\n",
    "df_img = set_frame_path(df_img, forder_P_DESTRE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### image label load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/home/mvlab/Downloads/dataset/통영/bridge_img_01/', True, 1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_cut = path_pedestrian + 'bridge_img_01/'\n",
    "path_cut_label = glob(path_cut + '*.txt')\n",
    "path_cut, os.path.isdir(path_cut), len(path_cut_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/mvlab/Downloads/dataset/통영/bridge_img_01/image_label.txt'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_cut_csv = path_cut_label[0]\n",
    "path_cut_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(409, 7)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names = ['path','cname','id', 'x0', 'y0', 'w', 'h']\n",
    "df_cut = pd.read_csv(path_cut_csv, names=names)\n",
    "df_cut['path'] = path_cut + df_cut['path']\n",
    "df_cut.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P-DESTRE load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PDESTRE_columns = ['frame', 'ID', 'x', 'y', 'w', 'h', 'head', 'yaw', 'pitch', 'roll',          \n",
    "          'gender', 'age', 'height', 'body volume', 'ethnicity', 'hair color', 'hairstyle', 'beard', 'mustache', 'glasses', \n",
    "           'head accessories', 'upper cloth', 'lower cloth', 'feet', 'accessories', 'action']\n",
    "len(PDESTRE_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/mvlab/Downloads/dataset/P-DESTRE/annotation',\n",
       " '/home/mvlab/Downloads/dataset/P-DESTRE/videos']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glob(forder_P_DESTRE + '*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#glob(forder_P_DESTRE + 'annotation/*.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1477385, 28)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_dataframe(forder_P_DESTRE):\n",
    "    list_df = []\n",
    "    \n",
    "    list_annotation_path = glob(forder_P_DESTRE + 'annotation/*.txt')\n",
    "    for i in range(len(list_annotation_path)):\n",
    "        path_anno = list_annotation_path[i]\n",
    "        anno_file_name = path_anno.split(os.sep)[-1]\n",
    "        anno_file_name = anno_file_name[:-4]\n",
    "        df = pd.read_csv(path_anno, header=None, names=PDESTRE_columns)\n",
    "        df['video'] = anno_file_name\n",
    "        list_df.append(df)\n",
    "        #print(i, anno_file_name, df.shape)\n",
    "    \n",
    "    df_all = pd.concat(list_df, axis=0)\n",
    "    df_all = df_all.reset_index()\n",
    "    return df_all \n",
    "\n",
    "df_P_DESTRE = get_dataframe(forder_P_DESTRE)    \n",
    "df_P_DESTRE.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "274"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_P_DESTRE['ID'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes_real = df_P_DESTRE['ID'].max() + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'00011'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str.format('%#05d' % 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = []\n",
    "for fr in df_P_DESTRE['frame'].values:\n",
    "    file_names.append(str.format('%#05d.jpg' % fr))\n",
    "    \n",
    "df_P_DESTRE['file_name'] = file_names\n",
    "df_P_DESTRE['path'] = forder_P_DESTRE + 'videos/' + df_P_DESTRE['video'] + os.sep + df_P_DESTRE['file_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>frame</th>\n",
       "      <th>ID</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>w</th>\n",
       "      <th>h</th>\n",
       "      <th>head</th>\n",
       "      <th>yaw</th>\n",
       "      <th>pitch</th>\n",
       "      <th>...</th>\n",
       "      <th>glasses</th>\n",
       "      <th>head accessories</th>\n",
       "      <th>upper cloth</th>\n",
       "      <th>lower cloth</th>\n",
       "      <th>feet</th>\n",
       "      <th>accessories</th>\n",
       "      <th>action</th>\n",
       "      <th>video</th>\n",
       "      <th>file_name</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "      <td>2593.3</td>\n",
       "      <td>1094.3</td>\n",
       "      <td>143.78</td>\n",
       "      <td>386.91</td>\n",
       "      <td>1</td>\n",
       "      <td>-46.228</td>\n",
       "      <td>-12.39</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>18-07-2019-1-1</td>\n",
       "      <td>00001.jpg</td>\n",
       "      <td>/home/mvlab/Downloads/dataset/P-DESTRE/videos/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>2060.2</td>\n",
       "      <td>1052.2</td>\n",
       "      <td>108.69</td>\n",
       "      <td>342.89</td>\n",
       "      <td>1</td>\n",
       "      <td>-46.228</td>\n",
       "      <td>-12.39</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>18-07-2019-1-1</td>\n",
       "      <td>00001.jpg</td>\n",
       "      <td>/home/mvlab/Downloads/dataset/P-DESTRE/videos/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>36</td>\n",
       "      <td>1897.5</td>\n",
       "      <td>1077.2</td>\n",
       "      <td>126.69</td>\n",
       "      <td>331.07</td>\n",
       "      <td>1</td>\n",
       "      <td>-46.228</td>\n",
       "      <td>-12.39</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>18-07-2019-1-1</td>\n",
       "      <td>00001.jpg</td>\n",
       "      <td>/home/mvlab/Downloads/dataset/P-DESTRE/videos/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>2733.7</td>\n",
       "      <td>1086.1</td>\n",
       "      <td>104.15</td>\n",
       "      <td>315.39</td>\n",
       "      <td>1</td>\n",
       "      <td>-46.228</td>\n",
       "      <td>-12.39</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>18-07-2019-1-1</td>\n",
       "      <td>00001.jpg</td>\n",
       "      <td>/home/mvlab/Downloads/dataset/P-DESTRE/videos/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>2540.7</td>\n",
       "      <td>1080.6</td>\n",
       "      <td>91.90</td>\n",
       "      <td>338.05</td>\n",
       "      <td>1</td>\n",
       "      <td>-46.228</td>\n",
       "      <td>-12.39</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>18-07-2019-1-1</td>\n",
       "      <td>00001.jpg</td>\n",
       "      <td>/home/mvlab/Downloads/dataset/P-DESTRE/videos/...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  frame  ID       x       y       w       h  head     yaw  pitch  ...  \\\n",
       "0      0      1  35  2593.3  1094.3  143.78  386.91     1 -46.228 -12.39  ...   \n",
       "1      1      1  25  2060.2  1052.2  108.69  342.89     1 -46.228 -12.39  ...   \n",
       "2      2      1  36  1897.5  1077.2  126.69  331.07     1 -46.228 -12.39  ...   \n",
       "3      3      1  28  2733.7  1086.1  104.15  315.39     1 -46.228 -12.39  ...   \n",
       "4      4      1  37  2540.7  1080.6   91.90  338.05     1 -46.228 -12.39  ...   \n",
       "\n",
       "   glasses  head accessories  upper cloth  lower cloth  feet  accessories  \\\n",
       "0        2                 3           10            0     0            1   \n",
       "1        1                 3            0            0     0            1   \n",
       "2        2                 3            0            0     0            1   \n",
       "3        2                 3            0            0     0            1   \n",
       "4        2                 3            0            0     0            1   \n",
       "\n",
       "   action           video  file_name  \\\n",
       "0       2  18-07-2019-1-1  00001.jpg   \n",
       "1       2  18-07-2019-1-1  00001.jpg   \n",
       "2       2  18-07-2019-1-1  00001.jpg   \n",
       "3       2  18-07-2019-1-1  00001.jpg   \n",
       "4       2  18-07-2019-1-1  00001.jpg   \n",
       "\n",
       "                                                path  \n",
       "0  /home/mvlab/Downloads/dataset/P-DESTRE/videos/...  \n",
       "1  /home/mvlab/Downloads/dataset/P-DESTRE/videos/...  \n",
       "2  /home/mvlab/Downloads/dataset/P-DESTRE/videos/...  \n",
       "3  /home/mvlab/Downloads/dataset/P-DESTRE/videos/...  \n",
       "4  /home/mvlab/Downloads/dataset/P-DESTRE/videos/...  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_P_DESTRE.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.016966464394859836"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remove not exist file\n",
    "is_exist_file = []\n",
    "for path_pdestre in df_P_DESTRE['path']:\n",
    "    is_exist_file.append(os.path.isfile(path_pdestre))\n",
    "\n",
    "np.mean(is_exist_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25066, 30)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_P_DESTRE_exist = df_P_DESTRE[is_exist_file]\n",
    "df_P_DESTRE_exist.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_P_DESTRE_cut = df_P_DESTRE_exist[['ID', 'ID', 'x', 'y', 'w', 'h', 'path']]\n",
    "df_P_DESTRE_cut.columns = ['cname', 'id', 'x0', 'y0', 'w', 'h', 'path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cname</th>\n",
       "      <th>id</th>\n",
       "      <th>x0</th>\n",
       "      <th>y0</th>\n",
       "      <th>w</th>\n",
       "      <th>h</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1618</th>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>2200.2</td>\n",
       "      <td>2042.80</td>\n",
       "      <td>110.42</td>\n",
       "      <td>118.37</td>\n",
       "      <td>/home/mvlab/Downloads/dataset/P-DESTRE/videos/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1619</th>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>2028.9</td>\n",
       "      <td>761.69</td>\n",
       "      <td>86.19</td>\n",
       "      <td>270.58</td>\n",
       "      <td>/home/mvlab/Downloads/dataset/P-DESTRE/videos/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1620</th>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>1804.4</td>\n",
       "      <td>1283.60</td>\n",
       "      <td>125.83</td>\n",
       "      <td>466.63</td>\n",
       "      <td>/home/mvlab/Downloads/dataset/P-DESTRE/videos/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1621</th>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>2111.7</td>\n",
       "      <td>757.48</td>\n",
       "      <td>69.43</td>\n",
       "      <td>254.57</td>\n",
       "      <td>/home/mvlab/Downloads/dataset/P-DESTRE/videos/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1622</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2011.8</td>\n",
       "      <td>1380.10</td>\n",
       "      <td>162.78</td>\n",
       "      <td>476.97</td>\n",
       "      <td>/home/mvlab/Downloads/dataset/P-DESTRE/videos/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1477190</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2500.8</td>\n",
       "      <td>1276.40</td>\n",
       "      <td>135.73</td>\n",
       "      <td>385.91</td>\n",
       "      <td>/home/mvlab/Downloads/dataset/P-DESTRE/videos/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1477191</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>3499.4</td>\n",
       "      <td>1034.10</td>\n",
       "      <td>100.55</td>\n",
       "      <td>152.40</td>\n",
       "      <td>/home/mvlab/Downloads/dataset/P-DESTRE/videos/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1477192</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2830.9</td>\n",
       "      <td>1202.50</td>\n",
       "      <td>105.56</td>\n",
       "      <td>276.40</td>\n",
       "      <td>/home/mvlab/Downloads/dataset/P-DESTRE/videos/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1477193</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2711.3</td>\n",
       "      <td>1171.20</td>\n",
       "      <td>105.94</td>\n",
       "      <td>320.12</td>\n",
       "      <td>/home/mvlab/Downloads/dataset/P-DESTRE/videos/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1477194</th>\n",
       "      <td>151</td>\n",
       "      <td>151</td>\n",
       "      <td>2729.9</td>\n",
       "      <td>1150.10</td>\n",
       "      <td>73.46</td>\n",
       "      <td>114.48</td>\n",
       "      <td>/home/mvlab/Downloads/dataset/P-DESTRE/videos/...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25066 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         cname   id      x0       y0       w       h  \\\n",
       "1618        25   25  2200.2  2042.80  110.42  118.37   \n",
       "1619        38   38  2028.9   761.69   86.19  270.58   \n",
       "1620        30   30  1804.4  1283.60  125.83  466.63   \n",
       "1621        13   13  2111.7   757.48   69.43  254.57   \n",
       "1622        -1   -1  2011.8  1380.10  162.78  476.97   \n",
       "...        ...  ...     ...      ...     ...     ...   \n",
       "1477190     -1   -1  2500.8  1276.40  135.73  385.91   \n",
       "1477191     -1   -1  3499.4  1034.10  100.55  152.40   \n",
       "1477192     -1   -1  2830.9  1202.50  105.56  276.40   \n",
       "1477193     -1   -1  2711.3  1171.20  105.94  320.12   \n",
       "1477194    151  151  2729.9  1150.10   73.46  114.48   \n",
       "\n",
       "                                                      path  \n",
       "1618     /home/mvlab/Downloads/dataset/P-DESTRE/videos/...  \n",
       "1619     /home/mvlab/Downloads/dataset/P-DESTRE/videos/...  \n",
       "1620     /home/mvlab/Downloads/dataset/P-DESTRE/videos/...  \n",
       "1621     /home/mvlab/Downloads/dataset/P-DESTRE/videos/...  \n",
       "1622     /home/mvlab/Downloads/dataset/P-DESTRE/videos/...  \n",
       "...                                                    ...  \n",
       "1477190  /home/mvlab/Downloads/dataset/P-DESTRE/videos/...  \n",
       "1477191  /home/mvlab/Downloads/dataset/P-DESTRE/videos/...  \n",
       "1477192  /home/mvlab/Downloads/dataset/P-DESTRE/videos/...  \n",
       "1477193  /home/mvlab/Downloads/dataset/P-DESTRE/videos/...  \n",
       "1477194  /home/mvlab/Downloads/dataset/P-DESTRE/videos/...  \n",
       "\n",
       "[25066 rows x 7 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_P_DESTRE_cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3351, 25066)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cond = df_P_DESTRE_cut['id']==-1\n",
    "cond.sum(), len(cond)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-30-25745db2f8fa>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_P_DESTRE_cut['id'][cond] = 1\n",
      "/home/mvlab/anaconda3/lib/python3.8/site-packages/pandas/core/generic.py:8765: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._update_inplace(new_data)\n"
     ]
    }
   ],
   "source": [
    "df_P_DESTRE_cut['id'][cond] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1327, 8), (409, 7), (25066, 7))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_img.shape, df_cut.shape, df_P_DESTRE_cut.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Index(['fn', 'cname', 'id', 'x0', 'y0', 'w', 'h', 'path'], dtype='object'),\n",
       " Index(['path', 'cname', 'id', 'x0', 'y0', 'w', 'h'], dtype='object'),\n",
       " Index(['cname', 'id', 'x0', 'y0', 'w', 'h', 'path'], dtype='object'))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_img.columns, df_cut.columns, df_P_DESTRE_cut.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0]), array([-1]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_img['id'].unique(), df_cut['id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1736, 7)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat((df_img.drop(columns='fn'), df_cut), axis=0)\n",
    "df['id']= 1 #finetune\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1327, 203)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum().sum(), df['path'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(409, 7)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.dropna()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "203"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['path'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def parsing_annotation(df):\n",
    "    annotation = dict()\n",
    "    for i in range(len(df)):\n",
    "        row = df.iloc[i].values\n",
    "        cname, iden, x0, y0, w, h, path = row\n",
    "        x1 = x0 + w\n",
    "        y1 = y0 + h\n",
    "        if i%10000==0:\n",
    "            print(i, cname, iden, x0, y0, x1, y1)\n",
    "       \n",
    "        cls = iden # land \n",
    "        bbox = [cls, x0, y0, x1, y1]\n",
    "\n",
    "        path_image = path\n",
    "        if path_image in annotation.keys():\n",
    "            annotation[path_image].extend(bbox)\n",
    "        else:\n",
    "            annotation[path_image] = bbox        \n",
    "    return annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 person 1 1064 549 1086 605\n",
      "0 25 25 2200.2 2042.8 2310.62 2161.17\n",
      "10000 35 35 682.66 1791.3 856.12 2159.84\n",
      "20000 -1 1 2083.7 300.42 2146.4199999999996 476.04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(203, 1781)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotation = parsing_annotation(df)\n",
    "annotation_PDESTRE = parsing_annotation(df_P_DESTRE_cut)\n",
    "\n",
    "len(annotation), len(annotation_PDESTRE) #basic:1530"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### image load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_xy(annotation, rescale=1):\n",
    "    input_list = []\n",
    "    bbox_list = []\n",
    "    path_list = []\n",
    "    for path_image in annotation:\n",
    "        cls_bbox = annotation[path_image]\n",
    "        cls_bbox = np.array(cls_bbox).reshape([-1, 5])\n",
    "        cls = cls_bbox[:, 0:1]\n",
    "        bbox = np.array(cls_bbox[:, 1:])\n",
    "\n",
    "        if os.path.isfile(path_image):\n",
    "            img = Image.open(path_image)    \n",
    "            scale = np.array((img.width, img.height, img.width, img.height))\n",
    "            scale = np.reshape(scale, (1, 4))\n",
    "\n",
    "            #print(key, cls, cls_bbox.dtype, cls_bbox, 'wh',img.width, img.height)\n",
    "            if rescale!=1:\n",
    "                img = img.resize((img.width//rescale, img.height//rescale))\n",
    "            img_arr = np.array(img)        \n",
    "            bbox_norm = bbox.astype(np.float) / scale.astype(np.float)\n",
    "            cls_bbox_norm = np.concatenate((cls, bbox_norm), axis=1)\n",
    "\n",
    "            input_list.append(img_arr)\n",
    "            bbox_list.append(cls_bbox_norm)\n",
    "            path_list.append(path_image)\n",
    "            if len(input_list)%100==0:        \n",
    "                print(len(annotation), len(input_list), len(bbox_list))   \n",
    "            if len(input_list) > max_data_m:\n",
    "                break\n",
    "        else:\n",
    "            #print('not exist', path_image)\n",
    "            pass\n",
    "\n",
    "    print(len(input_list), len(bbox_list))\n",
    "    return input_list, bbox_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "203 100 100\n",
      "203 200 200\n",
      "203 203\n",
      "1781 100 100\n",
      "1781 200 200\n",
      "1781 300 300\n",
      "1781 400 400\n",
      "1781 500 500\n",
      "1781 600 600\n",
      "1781 700 700\n",
      "1781 800 800\n",
      "1781 900 900\n",
      "1781 1000 1000\n",
      "1781 1100 1100\n"
     ]
    }
   ],
   "source": [
    "input_list, bbox_list = load_xy(annotation)\n",
    "input_list_PDESTRE, bbox_list_PDESTRE = load_xy(annotation_PDESTRE, rescale=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(input_list_PDESTRE[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbbox = np.concatenate(bbox_list_PDESTRE, 0)\n",
    "print(cbbox.shape, np.max(cbbox[:, 0])) # basic:1736 + P-DESTRE > 9757\n",
    "h = plt.hist(cbbox[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "box_w = cbbox[:, -2] - cbbox[:, -4]\n",
    "box_h = cbbox[:, -1] - cbbox[:, -3]\n",
    "h = plt.hist(box_h)\n",
    "h = plt.hist(box_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(box_w, box_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbbox.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_bbox_image(image, boxes):\n",
    "    img_objects = []\n",
    "    image = np.array(image)\n",
    "    for box in boxes:        \n",
    "        box = box.astype(np.int)\n",
    "        x1, y1, x2, y2 = box\n",
    "        w = x2 - x1\n",
    "        h = y2 - y1        \n",
    "        crop_image_arr = image[y1:y2, x1:x2]\n",
    "        ch, cw, cc = crop_image_arr.shape\n",
    "        if ch>1 and cw>1:\n",
    "            img_objects.append(crop_image_arr)\n",
    "        else:\n",
    "            print('crop_bbox_image', x2-x1, y2-y1, 'crop_image_arr.shape', crop_image_arr.shape)\n",
    "        \n",
    "    return img_objects\n",
    "    \n",
    "    \n",
    "def attach_crop_image(image, boxes, max_crop=200):\n",
    "        \n",
    "    crop_bbox_arr = crop_bbox_image(image, np.array(boxes)[:max_crop])\n",
    "    bbox_k = len(crop_bbox_arr)\n",
    "    max_col = 30\n",
    "    \n",
    "    if bbox_k > 0:\n",
    "        img_h, img_w, img_c = image.shape\n",
    "        object_img_w = img_w//bbox_k        \n",
    "        resize_h = img_h // 8\n",
    "        resize_w = img_w // bbox_k  \n",
    "        resize_w = min(max(resize_w, img_w//max_col), img_w//8)\n",
    "        \n",
    "        footer_h = resize_h * (1 + (bbox_k-1)//max_col)\n",
    "        footer = np.zeros((footer_h, img_w, img_c), np.uint8)\n",
    "        \n",
    "        for i in range(min(bbox_k, max_crop)):\n",
    "            crop_arr = crop_bbox_arr[i]\n",
    "            crop_img = Image.fromarray(crop_arr)                \n",
    "            crop_img = crop_img.resize((resize_w, resize_h))\n",
    "            crop_arr_resized = np.array(crop_img)\n",
    "            offset_y = (i//max_col) * resize_h\n",
    "            offset_x = (i%max_col) * resize_w\n",
    "            footer[offset_y:offset_y+resize_h, offset_x:offset_x+resize_w] = crop_arr_resized\n",
    "\n",
    "        seperate_line = np.zeros_like(footer[:2])\n",
    "        image = np.concatenate((image, seperate_line, footer), axis=0)    \n",
    "    return image    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_detections_simple(\n",
    "    image, boxes, classes, figsize=(12, 12), linewidth=1, color=[0, 0, 1]\n",
    "):\n",
    "    \"\"\"Visualize Detections\"\"\"\n",
    "    image = np.array(image, dtype=np.uint8)    \n",
    "    \n",
    "    img_h, img_w, img_c = image.shape\n",
    "    \n",
    "    image = attach_crop_image(image, boxes, max_crop=100)\n",
    "    \n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(image)\n",
    "    ax = plt.gca()\n",
    "    boxes_width = boxes[:, 2] - boxes[:, 0]\n",
    "    boxes_height = boxes[:, 3] - boxes[:, 1]\n",
    "    box_min_width = np.min(boxes_width)\n",
    "    box_max_width = np.max(boxes_width)\n",
    "    title = str.format('(%dx%d) %d box, width:%d ~ %d' \n",
    "                       %(img_h, img_w, len(boxes), box_min_width, box_max_width))\n",
    "    plt.title(title)\n",
    "    for box, cls in zip(boxes, classes):\n",
    "        x1, y1, x2, y2 = box        \n",
    "        w, h = x2 - x1, y2 - y1\n",
    "        \n",
    "        color = edgecolors[min(len(edgecolors)-1,int(cls))]\n",
    "        patch = plt.Rectangle(\n",
    "            [x1, y1], w, h, fill=False, edgecolor=color, linewidth=linewidth\n",
    "        )\n",
    "        ax.add_patch(patch)\n",
    "        if len(boxes) < 70:\n",
    "            #score_txt = class_names[int(cls)]\n",
    "            score_txt = str(int(cls))\n",
    "            ax.text(x1, y1, score_txt, bbox={\"facecolor\": [1,1,0], \"alpha\": 0.4}, clip_box=ax.clipbox, clip_on=True,)\n",
    "        \n",
    "    plt.show()\n",
    "    return ax\n",
    "\n",
    "def display_data(X, BBOX, stride=1):\n",
    "    for i in range(len(X)):\n",
    "        if i%stride==0:\n",
    "            img_arr = X[i]\n",
    "            sample_box = BBOX[i]\n",
    "            label = sample_box[:, 0]\n",
    "            bbox = sample_box[:, 1:]\n",
    "\n",
    "            h, w, c = img_arr.shape\n",
    "            scale = np.array((w, h, w, h))\n",
    "            scale = np.reshape(scale, (1, 4))\n",
    "            bbox_norm = bbox.astype(np.float) * scale.astype(np.float)\n",
    "            #print('bbox_norm', bbox, bbox_norm)\n",
    "            print(i, np.unique(label))\n",
    "            ax = visualize_detections_simple(img_arr,bbox_norm,label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tile_object(X, BBOX, scope=0.5):\n",
    "    \n",
    "    crop_xs = []\n",
    "    crop_bboxs = []\n",
    "    for i in range(len(X)):\n",
    "        x = X[i]\n",
    "        img_h, img_w, img_c = x.shape\n",
    "        bbox = BBOX[i]\n",
    "        #print('len', len(x), len(bbox), x.shape, bbox.shape)\n",
    "        cls = bbox[:, 0]\n",
    "        x0 = bbox[:, 1]\n",
    "        y0 = bbox[:, 2]\n",
    "        x1 = bbox[:, 3]\n",
    "        y1 = bbox[:, 4]\n",
    "        \n",
    "        if not np.any(cls>0):\n",
    "            continue\n",
    "            \n",
    "        box_h = y1 - y0\n",
    "        box_w = x1 - x0\n",
    "        box_y_min = np.min(y0)\n",
    "        box_x_min = np.min(x0)        \n",
    "        box_y_max = np.max(y1)\n",
    "        box_x_max = np.max(x1)        \n",
    "        if box_y_max - box_y_min < scope and box_x_max - box_x_min < scope:\n",
    "            \n",
    "            cx = np.mean((box_x_min + box_x_max)/2)\n",
    "            \n",
    "            if cx < 0.5:\n",
    "                tx0 = np.maximum(0, cx - scope/2)\n",
    "                tx1 = tx0 + scope\n",
    "            else:\n",
    "                tx1 = np.minimum(1.0, cx + scope/2)\n",
    "                tx0 = tx1 - scope\n",
    "                        \n",
    "            tbox = np.stack((cls, (x0 - tx0)/scope, y0, (x1 - tx0)/scope, y1), axis=1)            \n",
    "            \n",
    "            img_x0 = int(tx0 * img_w)\n",
    "            img_x1 = img_x0 + int(img_w*scope)\n",
    "            timg = x[:, img_x0:img_x1]\n",
    "            \n",
    "            img = Image.fromarray(timg)\n",
    "            img_resized = img.resize((padded_image_shape[1]//2, padded_image_shape[0]))\n",
    "            arr_resized = np.array(img_resized)            \n",
    "            \n",
    "            crop_xs.append(arr_resized)\n",
    "            crop_bboxs.append(tbox)\n",
    "    return crop_xs, crop_bboxs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def attach_tiled_data(X, BBOX, row=2, col=2):\n",
    "    m = len(X)\n",
    "    attach_m = int(np.ceil(m/(row*col)))\n",
    "    attach_xs = []\n",
    "    attach_bboxs = []\n",
    "    img_h, img_w, img_c = X[0].shape\n",
    "    for i in range(attach_m):\n",
    "        bg_color = np.median(X[i])\n",
    "        attach_xs.append(bg_color + np.zeros((img_h*row, img_w*col, img_c)))    \n",
    "        attach_bboxs.append([])\n",
    "    \n",
    "    m_rand = np.arange(m)\n",
    "    np.random.shuffle(m_rand)\n",
    "    for i in range(len(m_rand)):\n",
    "        j = i#m_rand[i]\n",
    "        x = X[j]\n",
    "        bbox = BBOX[j]        \n",
    "        img_h, img_w, img_c = x.shape\n",
    "        ti = i//(row*col)\n",
    "        ty = i%(row*col)//col\n",
    "        tx = i%(row*col)%col\n",
    "        dst_y0 = ty * img_h\n",
    "        dst_y1 = dst_y0 + img_h\n",
    "        dst_x0 = tx * img_w\n",
    "        dst_x1 = dst_x0 + img_w\n",
    "               \n",
    "        attach_xs[ti][dst_y0:dst_y1, dst_x0:dst_x1] = x                    \n",
    "        cls, x0, y0, x1, y1 = np.split(bbox, 5, -1)\n",
    "        \n",
    "        x_scale = 1.0 / col\n",
    "        y_scale = 1.0 / row\n",
    "        x0 = x0 * x_scale + tx * x_scale\n",
    "        y0 = y0 * y_scale + ty * y_scale\n",
    "        x1 = x1 * x_scale + tx * x_scale\n",
    "        y1 = y1 * y_scale + ty * y_scale\n",
    "        bbox = np.concatenate((cls, x0, y0, x1, y1), axis=1)        \n",
    "        attach_bboxs[ti].extend(bbox)\n",
    "            \n",
    "    for i in range(len(attach_bboxs)):\n",
    "        attach_bboxs[i] = np.stack(attach_bboxs[i], 0)\n",
    "        \n",
    "    return attach_xs, attach_bboxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bg_paths():\n",
    "    list_bg_jpg = glob(folder_water_bg + '*')\n",
    "    list_bg_jpg0 = glob(folder_water_bg[:-1] + '0/*')\n",
    "    print(len(list_bg_jpg), len(list_bg_jpg0))\n",
    "    return list_bg_jpg0[1::2]#+ list_bg_jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attach_with_bg(X, BBOX):\n",
    "    m = len(X)\n",
    "    list_bg_jpg = get_bg_paths()    \n",
    "    attach_xs = []\n",
    "    attach_bboxs = []\n",
    "    \n",
    "    for i in range(len(list_bg_jpg)):\n",
    "        obj_i = (i*2)%m\n",
    "        if i%100==0:\n",
    "            print(i, len(list_bg_jpg))\n",
    "        bg_img = Image.open(list_bg_jpg[i])\n",
    "        bg_img = bg_img.resize((padded_image_shape[1], padded_image_shape[0]))\n",
    "        bg_arr = np.array(bg_img)\n",
    "        bg_l, bg_r = np.split(bg_arr, 2, axis=1)\n",
    "        \n",
    "        attach_xs.append(np.concatenate((X[obj_i], bg_r), axis=1))\n",
    "        c, x0, y0, x1, y1 = np.split(BBOX[obj_i], 5, -1)\n",
    "        bbox = np.concatenate((c, x0*0.5, y0, x1*0.5, y1), -1)\n",
    "        attach_bboxs.append(bbox)\n",
    "        \n",
    "        attach_xs.append(np.concatenate((bg_l, X[obj_i+1]), axis=1))\n",
    "        c, x0, y0, x1, y1 = np.split(BBOX[obj_i+1], 5, -1)\n",
    "        bbox = np.concatenate((c, 0.5+x0*0.5, y0, 0.5+x1*0.5, y1), -1)\n",
    "        attach_bboxs.append(bbox)\n",
    "        \n",
    "    return attach_xs, attach_bboxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def box_convert_cxy(box):\n",
    "    y0, x0, y1, x1 = np.split(box, 4, axis=-1)    \n",
    "    return np.concatenate(( (y0+y1)/2, (x0+x1)/2 ), axis=1)\n",
    "\n",
    "def box_swap_xy(box):\n",
    "    y0, x0, y1, x1 = np.split(box, 4, axis=-1)    \n",
    "    return np.concatenate((x0, y0, x1, y1), axis=1)\n",
    "\n",
    "def box_convert_to_xywh(boxes):\n",
    "    return np.concatenate(\n",
    "        [(boxes[..., :2] + boxes[..., 2:]) / 2.0, boxes[..., 2:] - boxes[..., :2]], axis=-1,)\n",
    "\n",
    "def box_convert_to_corners(boxes):    \n",
    "    return np.concatenate(\n",
    "        [boxes[..., :2] - boxes[..., 2:] / 2.0, boxes[..., :2] + boxes[..., 2:] / 2.0], axis=-1,)\n",
    "\n",
    "def angle_to_radian(angle):\n",
    "    return angle * np.pi/180\n",
    "\n",
    "def rotate_images(X, angle):\n",
    "    rotate_X = []\n",
    "    for i in range(len(X)):        \n",
    "        x = X[i]\n",
    "        img_h = x.shape[0]\n",
    "        img_w = x.shape[1]\n",
    "        img = Image.fromarray(x)                \n",
    "        img_rotated = img.rotate(angle)\n",
    "        rotate_X.append(np.array(img_rotated))\n",
    "\n",
    "    return rotate_X\n",
    "    \n",
    "def gen_rotate_data(X, BBOX, angle):\n",
    "    rotate_xs = []\n",
    "    rotate_bboxs = []\n",
    "    m = len(X)\n",
    "    for i in range(m):        \n",
    "        x = X[i]\n",
    "        \n",
    "        bbox = BBOX[i]\n",
    "        cls = bbox[:, 0]\n",
    "        x0 = bbox[:, 1]\n",
    "        y0 = bbox[:, 2]\n",
    "        x1 = bbox[:, 3]\n",
    "        y1 = bbox[:, 4]\n",
    "        \n",
    "        box = bbox[:, 1:]\n",
    "        box_xywh = box_convert_to_xywh(box)\n",
    "        box_xy = box_xywh[:, :2] \n",
    "        box_wh = box_xywh[:, 2:] \n",
    "        box_uv = (np.reshape(box_xy, [-1, 2]) - 0.5) * 2\n",
    "        \n",
    "        img_h, img_w, img_c = x.shape\n",
    "        img = Image.fromarray(x)        \n",
    "        scale_mat = np.array([1, 0, 0, 1.0*img_h/img_w]).reshape((2,2))\n",
    "        scale_mat_rev = np.array([1, 0, 0, 1.0*img_w/img_h]).reshape((2,2))\n",
    "        \n",
    "        angle = angle + np.random.normal(scale=np.abs(angle))\n",
    "        radian = angle_to_radian(angle)        \n",
    "        rotate_mat = np.array([np.cos(radian), -np.sin(radian), np.sin(radian), np.cos(radian)])        \n",
    "        rotate_mat = np.reshape(rotate_mat, (2, 2))\n",
    "        box_uv_trans = np.matmul(box_uv, scale_mat)\n",
    "        box_uv_trans = np.matmul(box_uv_trans, rotate_mat)\n",
    "        box_uv_trans = np.matmul(box_uv_trans, scale_mat_rev)\n",
    "        box_trans = (box_uv_trans + 1)/2\n",
    "        box_trans_xy = np.reshape(box_trans, [-1, 2])\n",
    "        box_trans_xywh = np.concatenate((box_trans_xy, box_wh), axis=1)\n",
    "        box_trans = box_convert_to_corners(box_trans_xywh)\n",
    "   \n",
    "        #if np.min(box_trans)<0 or np.max(box_trans)>1: continue\n",
    "        \n",
    "        bbox_trans = np.concatenate((np.expand_dims(cls, 1), box_trans), axis=1)\n",
    "   \n",
    "        img_rotated = img.rotate(angle)\n",
    "        #plt.imshow(img_rotated)\n",
    "        rotate_xs.append(np.array(img_rotated))\n",
    "        rotate_bboxs.append(bbox_trans)\n",
    "\n",
    "    return rotate_xs, rotate_bboxs        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_obj(X, Y):\n",
    "    crop_obj_list = []\n",
    "    shape_list = []\n",
    "    cls_list = []\n",
    "    for i in range(len(X)):        \n",
    "        x = X[i]        \n",
    "        bboxs = Y[i]\n",
    "        img_h, img_w, img_c = x.shape\n",
    "        for j in range(len(bboxs)):\n",
    "            box = bboxs[j]\n",
    "            #print('box', box)\n",
    "            cls, x0, y0, x1, y1 = list(box)\n",
    "            x0 = int(x0 * img_w)\n",
    "            x1 = int(x1 * img_w)            \n",
    "            y0 = int(y0 * img_h)\n",
    "            y1 = int(y1 * img_h)\n",
    "            \n",
    "            x_crop = x[y0:y1, x0:x1]\n",
    "            if np.min(x_crop.shape)<3:\n",
    "                continue\n",
    "            crop_obj_list.append(x_crop)\n",
    "            cls_list.append(cls)\n",
    "            shape_list.append((x1-x0, y1-y0))\n",
    "        \n",
    "    return crop_obj_list, cls_list, shape_list\n",
    "\n",
    "def attach_obj(crop_obj_list, cls_list, resize_w=32, resize_h=48):\n",
    "    m = len(crop_obj_list)    \n",
    "    img_h = padded_image_shape[0]\n",
    "    img_w = padded_image_shape[1]\n",
    "    row = img_h//resize_h\n",
    "    col = img_w//resize_w\n",
    "    img_m = (m - 1) // (row * col) + 1\n",
    "    canvas = np.zeros((img_m, img_h, img_w, 3), dtype=np.uint8)\n",
    "    cbbox_list = []\n",
    "    for i in range(img_m):\n",
    "        cbbox_list.append([])\n",
    "    print('canvas', canvas.shape, row, col)\n",
    "    cls_sort_index = np.argsort(cls_list)\n",
    "    for j in range(m):\n",
    "        i = cls_sort_index[j]\n",
    "        x = crop_obj_list[i]\n",
    "        cls = cls_list[i]\n",
    "        #print(i, x.shape)\n",
    "        arr = x.astype(np.uint8)\n",
    "        img = Image.fromarray(arr)\n",
    "        img = img.resize((resize_w, resize_h))\n",
    "        x = np.array(img)\n",
    "        tm = i // (row * col)\n",
    "        ty = (i % (row * col) )// col\n",
    "        tx = i % col\n",
    "        #print(i, tm, ty, tx)\n",
    "        x0 = tx*resize_w\n",
    "        x1 = (tx+1)*resize_w\n",
    "        y0 = ty*resize_h\n",
    "        y1 = (ty+1)*resize_h\n",
    "        canvas[tm, y0:y1, x0:x1] = x\n",
    "        \n",
    "        cbbox_list[tm].append(np.array([cls, 1.0*x0/img_w, 1.0*y0/img_h, 1.0*x1/img_w, 1.0*y1/img_h]))\n",
    "        \n",
    "    for i in range(img_m):\n",
    "        cbboxs = cbbox_list[i]\n",
    "        cbbox_list[i] = np.stack(cbboxs, axis=0)\n",
    "    return canvas, cbbox_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "m = len(input_list)\n",
    "print('m', m)\n",
    "s=2\n",
    "input_list_train = input_list[::s]\n",
    "input_list_test = input_list[1::2]\n",
    "bbox_list_train = bbox_list[::s]\n",
    "bbox_list_test = bbox_list[1::2]\n",
    "print('bbox_list_train', len(bbox_list), len(bbox_list_train), len(bbox_list_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attach_xs, attach_bboxs = attach_tiled_data(input_list_PDESTRE, bbox_list_PDESTRE)\n",
    "len(attach_xs), len(attach_bboxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(attach_xs), len(attach_bboxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "display_data(attach_xs, attach_bboxs, stride=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_list_train.extend(attach_xs)\n",
    "bbox_list_train.extend(attach_bboxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('len', len(input_list_train), len(input_list_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_data(input_list_train, bbox_list_train, stride=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "\n",
    "def swap_xy(boxes):\n",
    "    \"\"\"Swaps order the of x and y coordinates of the boxes.\n",
    "    Arguments:\n",
    "      boxes: A tensor with shape `(num_boxes, 4)` representing bounding boxes.\n",
    "    Returns:\n",
    "      swapped boxes with shape same as that of boxes.\n",
    "    \"\"\"\n",
    "    return tf.stack([boxes[:, 1], boxes[:, 0], boxes[:, 3], boxes[:, 2]], axis=-1)\n",
    "\n",
    "\n",
    "def convert_to_xywh(boxes):\n",
    "    \"\"\"Changes the box format to center, width and height.\n",
    "    Arguments:\n",
    "      boxes: A tensor of rank 2 or higher with a shape of `(..., num_boxes, 4)`\n",
    "        representing bounding boxes where each box is of the format\n",
    "        `[xmin, ymin, xmax, ymax]`.\n",
    "    Returns:\n",
    "      converted boxes with shape same as that of boxes.\n",
    "    \"\"\"\n",
    "    return tf.concat(\n",
    "        [(boxes[..., :2] + boxes[..., 2:]) / 2.0, boxes[..., 2:] - boxes[..., :2]],\n",
    "        axis=-1,\n",
    "    )\n",
    "\n",
    "\n",
    "def convert_to_corners(boxes):\n",
    "    \"\"\"Changes the box format to corner coordinates\n",
    "    Arguments:\n",
    "      boxes: A tensor of rank 2 or higher with a shape of `(..., num_boxes, 4)`\n",
    "        representing bounding boxes where each box is of the format\n",
    "        `[x, y, width, height]`.\n",
    "    Returns:\n",
    "      converted boxes with shape same as that of boxes.\n",
    "    \"\"\"\n",
    "    return tf.concat(\n",
    "        [boxes[..., :2] - boxes[..., 2:] / 2.0, boxes[..., :2] + boxes[..., 2:] / 2.0],\n",
    "        axis=-1,\n",
    "    )\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "## Computing pairwise Intersection Over Union (IOU)\n",
    "As we will see later in the example, we would be assigning ground truth boxes\n",
    "to anchor boxes based on the extent of overlapping. This will require us to\n",
    "calculate the Intersection Over Union (IOU) between all the anchor\n",
    "boxes and ground truth boxes pairs.\n",
    "\"\"\"\n",
    "\n",
    "def compute_iou(boxes1, boxes2):#compute_iou(anchor_boxes, gt_boxes)\n",
    "    \"\"\"Computes pairwise IOU matrix for given two sets of boxes\n",
    "    Arguments:\n",
    "      boxes1: A tensor with shape `(N, 4)` representing bounding boxes\n",
    "        where each box is of the format `[x, y, width, height]`.\n",
    "        boxes2: A tensor with shape `(M, 4)` representing bounding boxes\n",
    "        where each box is of the format `[x, y, width, height]`.\n",
    "    Returns:\n",
    "      pairwise IOU matrix with shape `(N, M)`, where the value at ith row\n",
    "        jth column holds the IOU between ith box and jth box from\n",
    "        boxes1 and boxes2 respectively.\n",
    "    \"\"\"\n",
    "    boxes1_corners = convert_to_corners(boxes1)\n",
    "    boxes2_corners = convert_to_corners(boxes2)\n",
    "    lu = tf.maximum(boxes1_corners[:, None, :2], boxes2_corners[:, :2])\n",
    "    rd = tf.minimum(boxes1_corners[:, None, 2:], boxes2_corners[:, 2:])\n",
    "    intersection = tf.maximum(0.0, rd - lu)\n",
    "    intersection_area = intersection[:, :, 0] * intersection[:, :, 1]\n",
    "    boxes1_area = boxes1[:, 2] * boxes1[:, 3]\n",
    "    boxes2_area = boxes2[:, 2] * boxes2[:, 3]\n",
    "    union_area = tf.maximum(\n",
    "        boxes1_area[:, None] + boxes2_area - intersection_area, 1e-8\n",
    "    )\n",
    "    return tf.clip_by_value(intersection_area / union_area, 0.0, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "level_end - level_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "## Implementing Anchor generator\n",
    "Anchor boxes are fixed sized boxes that the model uses to predict the bounding\n",
    "box for an object. It does this by regressing the offset between the location\n",
    "of the object's center and the center of an anchor box, and then uses the width\n",
    "and height of the anchor box to predict a relative scale of the object. In the\n",
    "case of RetinaNet, each location on a given feature map has nine anchor boxes\n",
    "(at three scales and three ratios).\n",
    "\"\"\"\n",
    "class AnchorBox:\n",
    "    \"\"\"Generates anchor boxes.\n",
    "    This class has operations to generate anchor boxes for feature maps at\n",
    "    strides `[8, 16, 32, 64, 128]`. Where each anchor each box is of the\n",
    "    format `[x, y, width, height]`.\n",
    "    Attributes:\n",
    "      aspect_ratios: A list of float values representing the aspect ratios of\n",
    "        the anchor boxes at each location on the feature map\n",
    "      scales: A list of float values representing the scale of the anchor boxes\n",
    "        at each location on the feature map.\n",
    "      num_anchors: The number of anchor boxes at each location on feature map\n",
    "      areas: A list of float values representing the areas of the anchor\n",
    "        boxes for each feature map in the feature pyramid.\n",
    "      strides: A list of float value representing the strides for each feature\n",
    "        map in the feature pyramid.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.level_start = level_start\n",
    "        self.level_end = level_end\n",
    "        \n",
    "        if anchor_k==9:\n",
    "            self.aspect_ratios = [0.5, 1.0, 2.0]        \n",
    "            self.scales = [2 ** x for x in [0, 1 / 3, 2 / 3]]\n",
    "        else:\n",
    "            self.aspect_ratios = [1.0]        \n",
    "            self.scales = [2 ** x for x in [0]]\n",
    "                \n",
    "        self._num_anchors = len(self.aspect_ratios) * len(self.scales)\n",
    "        self._strides = [2 ** i for i in range(self.level_start, self.level_end)]\n",
    "        self._areas = [x ** 2 for x in [32.0, 64.0, 128.0, 196.0, 256.0]]                        \n",
    "        self._areas = self._areas[:level_end - level_start]\n",
    "        \n",
    "        self._anchor_dims = self._compute_dims()\n",
    "\n",
    "    def _compute_dims(self):\n",
    "        \"\"\"Computes anchor box dimensions for all ratios and scales at all levels\n",
    "        of the feature pyramid.\n",
    "        \"\"\"\n",
    "        anchor_dims_all = []\n",
    "        for area in self._areas:\n",
    "            anchor_dims = []\n",
    "            for ratio in self.aspect_ratios:\n",
    "                anchor_height = tf.math.sqrt(area / ratio)\n",
    "                anchor_width = area / anchor_height\n",
    "                dims = tf.reshape(\n",
    "                    tf.stack([anchor_width, anchor_height], axis=-1), [1, 1, 2]\n",
    "                )\n",
    "                for scale in self.scales:\n",
    "                    anchor_dims.append(scale * dims)\n",
    "            anchor_dims_all.append(tf.stack(anchor_dims, axis=-2))\n",
    "        return anchor_dims_all\n",
    "\n",
    "    def _get_anchors(self, feature_height, feature_width, level):\n",
    "        \"\"\"Generates anchor boxes for a given feature map size and level\n",
    "        Arguments:\n",
    "          feature_height: An integer representing the height of the feature map.\n",
    "          feature_width: An integer representing the width of the feature map.\n",
    "          level: An integer representing the level of the feature map in the\n",
    "            feature pyramid.\n",
    "        Returns:\n",
    "          anchor boxes with the shape\n",
    "          `(feature_height * feature_width * num_anchors, 4)`\n",
    "        \"\"\"\n",
    "        rx = tf.range(feature_width, dtype=tf.float32) + 0.5\n",
    "        ry = tf.range(feature_height, dtype=tf.float32) + 0.5\n",
    "        centers = tf.stack(tf.meshgrid(rx, ry), axis=-1) * self._strides[level - self.level_start]\n",
    "        centers = tf.expand_dims(centers, axis=-2)\n",
    "        centers = tf.tile(centers, [1, 1, self._num_anchors, 1])\n",
    "        dims = tf.tile(\n",
    "            self._anchor_dims[level - self.level_start], [feature_height, feature_width, 1, 1]\n",
    "        )\n",
    "        anchors = tf.concat([centers, dims], axis=-1)\n",
    "        return tf.reshape(\n",
    "            anchors, [feature_height * feature_width * self._num_anchors, 4]\n",
    "        )\n",
    "\n",
    "    def get_anchors(self, image_height, image_width):\n",
    "        \"\"\"Generates anchor boxes for all the feature maps of the feature pyramid.\n",
    "        Arguments:\n",
    "          image_height: Height of the input image.\n",
    "          image_width: Width of the input image.\n",
    "        Returns:\n",
    "          anchor boxes for all the feature maps, stacked as a single tensor\n",
    "            with shape `(total_anchors, 4)`\n",
    "        \"\"\"\n",
    "        anchors = [\n",
    "            self._get_anchors(\n",
    "                tf.math.ceil(image_height / 2 ** i),\n",
    "                tf.math.ceil(image_width / 2 ** i),\n",
    "                i,\n",
    "            )\n",
    "            for i in range(self.level_start, self.level_end)\n",
    "        ]\n",
    "        return tf.concat(anchors, axis=0)\n",
    "    \n",
    "    def get_anchors_check(self, image_height, image_width):\n",
    "        \"\"\"Generates anchor boxes for all the feature maps of the feature pyramid.\n",
    "        Arguments:\n",
    "          image_height: Height of the input image.\n",
    "          image_width: Width of the input image.\n",
    "        Returns:\n",
    "          anchor boxes for all the feature maps, stacked as a single tensor\n",
    "            with shape `(total_anchors, 4)`\n",
    "        \"\"\"\n",
    "        anchors = [\n",
    "            self._get_anchors(\n",
    "                tf.math.ceil(image_height / 2 ** i),\n",
    "                tf.math.ceil(image_width / 2 ** i),\n",
    "                i,\n",
    "            )\n",
    "            for i in range(self.level_start, self.level_end)\n",
    "        ]\n",
    "        return anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "level_start, level_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_check = AnchorBox()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "anchors = anchor_check.get_anchors_check(128,128)\n",
    "for anchor in anchors:\n",
    "    print(anchor.shape, anchor[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_image_shape, 128*8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## Preprocessing data\n",
    "Preprocessing the images involves two steps:\n",
    "- Resizing the image: Images are resized such that the shortest size is equal\n",
    "to 800 px, after resizing if the longest side of the image exceeds 1333 px,\n",
    "the image is resized such that the longest size is now capped at 1333 px.\n",
    "- Applying augmentation: Random scale jittering  and random horizontal flipping\n",
    "are the only augmentations applied to the images.\n",
    "Along with the images, bounding boxes are rescaled and flipped if required.\n",
    "\"\"\"\n",
    "\n",
    "def random_flip_horizontal(image, boxes):\n",
    "    \"\"\"Flips image and boxes horizontally with 50% chance\n",
    "    Arguments:\n",
    "      image: A 3-D tensor of shape `(height, width, channels)` representing an\n",
    "        image.\n",
    "      boxes: A tensor with shape `(num_boxes, 4)` representing bounding boxes,\n",
    "        having normalized coordinates.\n",
    "    Returns:\n",
    "      Randomly flipped image and boxes\n",
    "    \"\"\"\n",
    "    if tf.random.uniform(()) > 0.5:\n",
    "        image = tf.image.flip_left_right(image)\n",
    "        boxes = tf.stack([1 - boxes[:, 2], boxes[:, 1], 1 - boxes[:, 0], boxes[:, 3]], axis=-1)\n",
    "   \n",
    "    return image, boxes\n",
    "\n",
    "def resize_and_pad_image(\n",
    "    image, mask_obj=None, min_side=1024.0, max_side=11333.0, jitter=[128*8, 128*8+1], stride=128.0\n",
    "):\n",
    "   \n",
    "    image_shape = tf.cast(tf.shape(image)[:2], dtype=tf.float32)\n",
    "    \n",
    "    ratio = min_side / tf.reduce_min(image_shape)\n",
    "    \n",
    "    image_shape = ratio * image_shape\n",
    "    image = tf.image.resize(image, tf.cast(image_shape, dtype=tf.int32))\n",
    "    if mask_obj!=None:\n",
    "        mask_obj = tf.image.resize(mask_obj, tf.cast(image_shape, dtype=tf.int32))\n",
    "    \n",
    "    image = tf.image.pad_to_bounding_box(image, 0, 0, padded_image_shape[0], padded_image_shape[1]) \n",
    "    if mask_obj!=None:\n",
    "        mask_obj = tf.image.pad_to_bounding_box(mask_obj, 0, 0, padded_image_shape[0], padded_image_shape[1])        \n",
    "    if mask_obj!=None:\n",
    "        return image, image_shape, ratio, mask_obj\n",
    "    return image, image_shape, ratio\n",
    "\n",
    "def resize_and_pad_image_bbox(\n",
    "    image, bbox, mask_obj=None, min_side=1024.0, max_side=1024.0*4, jitter=[128*7+32, 128*8-32], stride=128.0\n",
    "):\n",
    "    #image, min_side=800.0, max_side=1333.0, jitter=[640, 1024], stride=128.0\n",
    "    \"\"\"Resizes and pads image while preserving aspect ratio.\n",
    "    1. Resizes images so that the shorter side is equal to `min_side`\n",
    "    2. If the longer side is greater than `max_side`, then resize the image\n",
    "      with longer side equal to `max_side`\n",
    "    3. Pad with zeros on right and bottom to make the image shape divisible by\n",
    "    `stride`\n",
    "    Arguments:\n",
    "      image: A 3-D tensor of shape `(height, width, channels)` representing an\n",
    "        image.\n",
    "      min_side: The shorter side of the image is resized to this value, if\n",
    "        `jitter` is set to None.\n",
    "      max_side: If the longer side of the image exceeds this value after\n",
    "        resizing, the image is resized such that the longer side now equals to\n",
    "        this value.\n",
    "      jitter: A list of floats containing minimum and maximum size for scale\n",
    "        jittering. If available, the shorter side of the image will be\n",
    "        resized to a random value in this range.\n",
    "      stride: The stride of the smallest feature map in the feature pyramid.\n",
    "        Can be calculated using `image_size / feature_map_size`.\n",
    "    Returns:\n",
    "      image: Resized and padded image.\n",
    "      image_shape: Shape of the image before padding.\n",
    "      ratio: The scaling factor used to resize the image\n",
    "    \"\"\"\n",
    "    image_shape = tf.cast(tf.shape(image)[:2], dtype=tf.float32)\n",
    "    if jitter is not None:\n",
    "        min_side = tf.random.uniform((), jitter[0], jitter[1], dtype=tf.float32)\n",
    "    ratio = min_side / tf.reduce_min(image_shape)\n",
    "    if ratio * tf.reduce_max(image_shape) > max_side:\n",
    "        ratio = max_side / tf.reduce_max(image_shape)\n",
    "    image_shape = ratio * image_shape\n",
    "    ratio_jitter = tf.random.uniform(tf.shape(image_shape), -32, 32, dtype=tf.float32)\n",
    "    image_shape += ratio_jitter      \n",
    "    image = tf.image.resize(image, tf.cast(image_shape, dtype=tf.int32))\n",
    "    if mask_obj!=None:\n",
    "        mask_obj = tf.image.resize(mask_obj, tf.cast(image_shape, dtype=tf.int32))\n",
    "    padded_image_shape = tf.cast(\n",
    "        tf.math.ceil(image_shape / stride) * stride, dtype=tf.int32\n",
    "    )\n",
    "    image = tf.image.pad_to_bounding_box(image, 0, 0, padded_image_shape[0], padded_image_shape[1])\n",
    "    if mask_obj!=None:\n",
    "        mask_obj = tf.image.pad_to_bounding_box(mask_obj, 0, 0, padded_image_shape[0], padded_image_shape[1])        \n",
    "    padded_image_shape = tf.cast(padded_image_shape, tf.float32)              \n",
    "    pad_ratio = tf.cast(image_shape, tf.float32) / padded_image_shape\n",
    "    bbox_padded = tf.stack(\n",
    "        [\n",
    "            bbox[:, 0] * pad_ratio[1],\n",
    "            bbox[:, 1] * pad_ratio[0],\n",
    "            bbox[:, 2] * pad_ratio[1],\n",
    "            bbox[:, 3] * pad_ratio[0],\n",
    "        ],\n",
    "        axis=-1,\n",
    "    )\n",
    "    if mask_obj!=None:\n",
    "        return image, padded_image_shape, ratio, bbox_padded, mask_obj    \n",
    "    return image, padded_image_shape, ratio, bbox_padded\n",
    "\n",
    "\n",
    "def preprocess_data(image, cls_bbox):\n",
    "    \"\"\"Applies preprocessing step to a single sample\n",
    "    Arguments:\n",
    "      sample: A dict representing a single training sample.\n",
    "    Returns:\n",
    "      image: Resized and padded image with random horizontal flipping applied.\n",
    "      bbox: Bounding boxes with the shape `(num_objects, 4)` where each box is\n",
    "        of the format `[x, y, width, height]`.\n",
    "      class_id: An tensor representing the class id of the objects, having\n",
    "        shape `(num_objects,)`.\n",
    "    \"\"\"\n",
    "     \n",
    "    bbox = cls_bbox[:, 1:]\n",
    "    class_id = tf.cast(cls_bbox[:, 0], dtype=tf.int32)    \n",
    "\n",
    "    image, bbox = random_flip_horizontal(image, bbox)    \n",
    "    is_flipped = tf.zeros_like(class_id)\n",
    "    image, image_shape, _, bbox = resize_and_pad_image_bbox(image, bbox)\n",
    "    \n",
    "    margin = 0#new\n",
    "    bbox = tf.stack(\n",
    "        [\n",
    "            bbox[:, 0] * image_shape[1] + margin,\n",
    "            bbox[:, 1] * image_shape[0] + margin,\n",
    "            bbox[:, 2] * image_shape[1] - margin,\n",
    "            bbox[:, 3] * image_shape[0] - margin,\n",
    "        ],\n",
    "        axis=-1,\n",
    "    )\n",
    "    bbox = convert_to_xywh(bbox)    \n",
    "    cls_flip = tf.stack((class_id, is_flipped), -1)\n",
    "    return image, bbox, cls_flip\n",
    "\n",
    "\n",
    "def preprocess_test_data(image, cls_bbox):         \n",
    "    bbox = cls_bbox[:, 1:]\n",
    "    class_id = tf.cast(cls_bbox[:, 0], dtype=tf.int32)        \n",
    "    \n",
    "    image, image_shape, _ = resize_and_pad_image(image)\n",
    "    is_flipped = tf.zeros_like(class_id)\n",
    "    bbox = tf.stack(\n",
    "        [\n",
    "            bbox[:, 0] * image_shape[1],\n",
    "            bbox[:, 1] * image_shape[0],\n",
    "            bbox[:, 2] * image_shape[1],\n",
    "            bbox[:, 3] * image_shape[0],\n",
    "        ],\n",
    "        axis=-1,\n",
    "    )\n",
    "    bbox = convert_to_xywh(bbox)    \n",
    "    cls_flip = tf.stack((class_id, is_flipped), -1)\n",
    "    return image, bbox, cls_flip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_weather_effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_color_augment(x):\n",
    "    if tf.random.uniform(()) < -0.5:\n",
    "        x_max = tf.reduce_max(x, [1, 2], True)\n",
    "        x = x_max - x\n",
    "    if tf.random.uniform(()) < -0.2:\n",
    "        r, g, b = tf.split(x, 3, axis=-1)\n",
    "        x = tf.concat((r, b, g), -1)\n",
    "    elif tf.random.uniform(()) < -0.4:\n",
    "        r, g, b = tf.split(x, 3, axis=-1)\n",
    "        x = tf.concat((b, r, g), -1)\n",
    "    if tf.random.uniform(()) < 0.2:\n",
    "        x = tf.image.random_hue(x, 0.08)\n",
    "        x = tf.image.random_saturation(x, 0.6, 1.6)\n",
    "    if tf.random.uniform(()) < 0.2:\n",
    "        x = tf.image.random_brightness(x, 0.05)\n",
    "        x = tf.image.random_contrast(x, 0.7, 1.3)\n",
    "    if tf.random.uniform(()) < -0.2:\n",
    "        gray = tf.image.rgb_to_grayscale(x)\n",
    "        x = tf.concat((gray, gray, gray), -1)        \n",
    "    if tf.random.uniform(()) < -0.2:\n",
    "        noise = tf.random.normal(tf.shape(x), stddev=tf.pow(tf.reduce_mean(x), 0.3))\n",
    "        x += noise\n",
    "    if tf.random.uniform(()) < 0.2:\n",
    "        x = gaussian_filter2d(x, filter_shape=tuple(np.random.randint(1, 10, (2))), sigma=10)\n",
    "        #x = gaussian_filter2d(x, filter_shape=np.random.randint(3, 10, (2)), sigma=10)\n",
    "    if tf.random.uniform(()) < 0.2:        \n",
    "        x = sharpness(x, factor=10)\n",
    "    if tf.random.uniform(()) < 0.2:        \n",
    "        if use_weather_effect:\n",
    "            weather_k = len(weather_images)                            \n",
    "            h = tf.shape(x)[1]\n",
    "            w = tf.shape(x)[2]\n",
    "            weather_image = weather_images[np.random.randint(weather_k)]\n",
    "            weather_image = tf.image.resize(weather_image, tf.cast((h, w), dtype=tf.int32))\n",
    "            weather_image = tf.expand_dims(weather_image, 0)\n",
    "            x = (x // 3) * 2 + weather_image//3\n",
    "            \n",
    "    #x = tf.image.random_jpeg_quality(x, 0, 1.0)\n",
    "    #x = tf.clip_by_value(x, 0, 1)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## Encoding labels\n",
    "The raw labels, consisting of bounding boxes and class ids need to be\n",
    "transformed into targets for training. This transformation consists of\n",
    "the following steps:\n",
    "- Generating anchor boxes for the given image dimensions\n",
    "- Assigning ground truth boxes to the anchor boxes\n",
    "- The anchor boxes that are not assigned any objects, are either assigned the\n",
    "background class or ignored depending on the IOU\n",
    "- Generating the classification and regression targets using anchor boxes\n",
    "\"\"\"\n",
    "\n",
    "class LabelEncoder:\n",
    "    \"\"\"Transforms the raw labels into targets for training.\n",
    "    This class has operations to generate targets for a batch of samples which\n",
    "    is made up of the input images, bounding boxes for the objects present and\n",
    "    their class ids.\n",
    "    Attributes:\n",
    "      anchor_box: Anchor box generator to encode the bounding boxes.\n",
    "      box_variance: The scaling factors used to scale the bounding box targets.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self._anchor_box = AnchorBox()\n",
    "        self._box_variance = tf.convert_to_tensor(\n",
    "            [0.1, 0.1, 0.2, 0.2], dtype=tf.float32\n",
    "        )    \n",
    "    \n",
    "    def _compute_box_target(self, anchor_boxes, matched_gt_boxes):\n",
    "        \"\"\"Transforms the ground truth boxes into targets for training\"\"\"\n",
    "        box_target = tf.concat(\n",
    "            [\n",
    "                (matched_gt_boxes[:, :2] - anchor_boxes[:, :2]) / anchor_boxes[:, 2:],\n",
    "                tf.math.log(matched_gt_boxes[:, 2:] / anchor_boxes[:, 2:]),\n",
    "            ],\n",
    "            axis=-1,\n",
    "        )\n",
    "        box_target = box_target / self._box_variance\n",
    "        return box_target\n",
    "\n",
    "    def _match_anchor_boxes(\n",
    "        self, anchor_boxes, gt_boxes, match_iou=0.4, ignore_iou=0.1\n",
    "    ):\n",
    "        iou_matrix = compute_iou(anchor_boxes, gt_boxes)\n",
    "        max_iou = tf.reduce_max(iou_matrix, axis=1)#from anchor to object-box        \n",
    "        matched_gt_idx = tf.argmax(iou_matrix, axis=1)    \n",
    "        positive_mask = tf.greater_equal(max_iou, match_iou)# not only this, but also need max iou cell\n",
    "        \n",
    "        positive_proposal_mask = tf.greater_equal(iou_matrix, match_iou)\n",
    "        positive_mask = tf.reduce_any(positive_proposal_mask, axis=1)\n",
    "        \n",
    "        negative_mask = tf.less(max_iou, ignore_iou)\n",
    "        \n",
    "        max_iou_anchor = tf.reduce_max(iou_matrix, axis=0, keepdims=True) \n",
    "        max_iou_anchor_mask = tf.greater_equal(iou_matrix, max_iou_anchor)\n",
    "        positive_max_mask = tf.reduce_any(max_iou_anchor_mask, axis=1)\n",
    "        positive_mask = tf.logical_or(positive_mask, positive_max_mask)#new      \n",
    "        \n",
    "        negative_mask = tf.logical_and(negative_mask, tf.logical_not(positive_mask))\n",
    "        ignore_mask = tf.logical_not(tf.logical_or(positive_mask, negative_mask))        \n",
    "        \n",
    "        return (\n",
    "            matched_gt_idx,            \n",
    "            tf.cast(positive_mask, dtype=tf.float32),\n",
    "            tf.cast(positive_max_mask, dtype=tf.float32),            \n",
    "            tf.cast(ignore_mask, dtype=tf.float32),\n",
    "        )\n",
    "\n",
    "    def _encode_sample(self, image_shape, gt_boxes, cls_ids):\n",
    "        \"\"\"Creates box and classification targets for a single sample\"\"\"\n",
    "        \n",
    "        anchor_boxes = self._anchor_box.get_anchors(image_shape[1], image_shape[2])\n",
    "        cls_ids = tf.cast(cls_ids, dtype=tf.float32)\n",
    "        matched_gt_idx, positive_mask, positive_max_mask, ignore_mask = self._match_anchor_boxes(\n",
    "            anchor_boxes, gt_boxes\n",
    "        )\n",
    "        matched_gt_boxes = tf.gather(gt_boxes, matched_gt_idx)\n",
    "        matched_gt_boxes_size = tf.reduce_prod(matched_gt_boxes[:, 2:], 1)\n",
    "        matched_gt_boxes_size = tf.sqrt(matched_gt_boxes_size)        \n",
    "        \n",
    "        box_target = self._compute_box_target(anchor_boxes, matched_gt_boxes)    \n",
    "        matched_gt_cls_ids = tf.gather(cls_ids, matched_gt_idx)\n",
    "        cls_target = tf.where(\n",
    "            tf.not_equal(positive_mask, 1.0), 0.0, matched_gt_cls_ids\n",
    "        )\n",
    "        cls_target = tf.where(tf.equal(ignore_mask, 1.0), -1.0, cls_target)       \n",
    "        cls_target = tf.expand_dims(cls_target, axis=-1)        \n",
    "        positive_max_mask= tf.expand_dims(positive_max_mask, -1)\n",
    "        label = tf.concat([box_target, cls_target, positive_max_mask], axis=-1)        \n",
    "        return label\n",
    "    \n",
    "    def encode_batch(self, batch_images, gt_boxes, cls_flip):\n",
    "        \"\"\"Creates box and classification targets for a batch\"\"\"\n",
    "        #is_flipped 0 or 1\n",
    "        images_shape = tf.shape(batch_images)\n",
    "        batch_size = images_shape[0]\n",
    "        cls_ids, is_flipped = tf.split(cls_flip, 2, -1)\n",
    "        cls_ids = tf.squeeze(cls_ids, -1)\n",
    "        is_flipped = tf.squeeze(is_flipped, -1)\n",
    "        is_flipped = tf.cast(is_flipped, tf.float32)\n",
    "        \n",
    "        labels = tf.TensorArray(dtype=tf.float32, size=batch_size, dynamic_size=True)\n",
    "        for i in range(batch_size):\n",
    "            label = self._encode_sample(images_shape, gt_boxes[i], cls_ids[i])\n",
    "            labels = labels.write(i, label)\n",
    "        \n",
    "        batch_images = tf.cast(batch_images, tf.float32)\n",
    "        label = labels.stack()\n",
    "        is_flipped_anchor = tf.zeros_like(label[:, :, :1]) + tf.reduce_max(is_flipped)\n",
    "        label = tf.concat((label, is_flipped_anchor), -1)\n",
    "        return batch_images, label\n",
    "      \n",
    "    \n",
    "    def encode_batch_train(self, batch_images, gt_boxes, cls):\n",
    "        \"\"\"Creates box and classification targets for a batch\"\"\"\n",
    "        \n",
    "        batch_images = image_color_augment(batch_images)#new        \n",
    "        return self.encode_batch(batch_images, gt_boxes, cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://towardsdatascience.com/building-a-resnet-in-keras-e8f1322a49ba\n",
    "from tensorflow import Tensor\n",
    "from tensorflow.keras.layers import Input, Conv2D, ReLU, BatchNormalization,\\\n",
    "                                    Add, AveragePooling2D, Flatten, Dense, MaxPool2D\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BifeaturePyramidNet(c345):\n",
    "    filters = 128\n",
    "    a2 = c345[0]\n",
    "    a3 = c345[1]\n",
    "    a4 = c345[2]\n",
    "    a5 = c345[3]\n",
    "    \n",
    "    regulizer  = tf.keras.regularizers.L2(l1)\n",
    "    \n",
    "    #b3 = Conv2D(filters, 1, 1, \"same\", groups=1, activation=activation, kernel_regularizer=regulizer)(a3)\n",
    "    #b4 = Conv2D(filters, 1, 1, \"same\", groups=1, activation=activation, kernel_regularizer=regulizer)(a4)\n",
    "    \n",
    "    a2_0 = Conv2D(filters, 1, 1, \"same\", groups=1, activation=activation, kernel_regularizer=regulizer)(a2)\n",
    "    #a3_0 = Conv2D(filters, 1, 1, \"same\", groups=1, activation=activation, kernel_regularizer=regulizer)(a3)\n",
    "    #a4_0 = Conv2D(filters, 1, 1, \"same\", groups=1, activation=activation, kernel_regularizer=regulizer)(a4)\n",
    "    #a5_0 = Conv2D(filters, 1, 1, \"same\", groups=1, activation=activation, kernel_regularizer=regulizer)(a5)\n",
    "    #a3_1 = Conv2D(filters, 1, 1, \"same\", groups=1, activation=activation, kernel_regularizer=regulizer)(a3)\n",
    "    #a4_1 = Conv2D(filters, 1, 1, \"same\", groups=1, activation=activation, kernel_regularizer=regulizer)(a4)\n",
    "    \n",
    "    a33 = Conv2D(filters*2, 1, 1, \"same\", groups=1, activation=activation, kernel_regularizer=regulizer)(a3)\n",
    "    a44 = Conv2D(filters*2, 1, 1, \"same\", groups=1, activation=activation, kernel_regularizer=regulizer)(a4)\n",
    "    a55 = Conv2D(filters*2, 1, 1, \"same\", groups=1, activation=activation, kernel_regularizer=regulizer)(a5)\n",
    "    a66 = Conv2D(filters*2, 3, 2, \"same\", groups=1, activation=activation, kernel_regularizer=regulizer)(a5)\n",
    "    \n",
    "    a3_0, a3_1 = tf.split(a33, 2, -1)\n",
    "    a4_0, a4_1 = tf.split(a44, 2, -1)\n",
    "    a5_0, a5_1 = tf.split(a55, 2, -1)\n",
    "    a6_0, a6_1 = tf.split(a66, 2, -1)\n",
    "    \n",
    "    b6 = a6_0\n",
    "    \n",
    "    a6_up = keras.layers.UpSampling2D(2)(a6_1)    \n",
    "    b5 = keras.layers.Add()([a5_0, a6_up])  \n",
    "        \n",
    "    a5_up = keras.layers.UpSampling2D(2)(a5_1)    \n",
    "    b4 = keras.layers.Add()([a4_0, a5_up])  \n",
    "    \n",
    "    b4_up = keras.layers.UpSampling2D(2)(b4)\n",
    "    b3 = keras.layers.Add()([a3_0, b4_up])  \n",
    "    \n",
    "    b3_up = keras.layers.UpSampling2D(2)(b3)\n",
    "    b2 = keras.layers.Add()([a2_0, b3_up])\n",
    "    \n",
    "    b2_down = Conv2D(filters, 3, 2, \"same\", groups=1, activation=activation, kernel_regularizer=regulizer)(b2)\n",
    "    b3_1 = Conv2D(filters, 1, 1, \"same\", groups=1, activation=activation, kernel_regularizer=regulizer)(b3)    \n",
    "    c3 = keras.layers.Add()([a3_1, b3_1, b2_down])\n",
    "    \n",
    "    c3_down = Conv2D(filters, 3, 2, \"same\", groups=1, activation=activation, kernel_regularizer=regulizer)(c3)\n",
    "    b4_1 = Conv2D(filters, 1, 1, \"same\", groups=1, activation=activation, kernel_regularizer=regulizer)(b4)    \n",
    "    c4 = keras.layers.Add()([a4_1, b4_1, c3_down])    \n",
    "    \n",
    "    c4_down = Conv2D(filters, 3, 2, \"same\", groups=1, activation=activation, kernel_regularizer=regulizer)(c4)\n",
    "    b5_1 = Conv2D(filters, 1, 1, \"same\", groups=1, activation=activation, kernel_regularizer=regulizer)(b5)    \n",
    "    c5 = keras.layers.Add()([a5_1, b5_1, c4_down])    \n",
    "    \n",
    "    c5_down = Conv2D(filters, 3, 2, \"same\", groups=1, activation=activation, kernel_regularizer=regulizer)(c5)\n",
    "    b6_1 = Conv2D(filters, 1, 1, \"same\", groups=1, activation=activation, kernel_regularizer=regulizer)(b6)    \n",
    "    c6 = keras.layers.Add()([a6_1, b6_1, c5_down])\n",
    "    \n",
    "    return c4, c5, c6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/tensorflow/addons/blob/v0.11.2/tensorflow_addons/image/__init__.py\n",
    "from tensorflow_addons.image.color_ops import sharpness\n",
    "from tensorflow_addons.image.filters import gaussian_filter2d\n",
    "from tensorflow_addons.image.dense_image_warp import dense_image_warp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inputs = Input(shape=(3, 3, 2))  # 18   \n",
    "outputs = Conv2D(10, 3)(inputs)# 18 * 10 + 10 = 190\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "backbone = keras.applications.EfficientNetB2(include_top=False, input_shape=[64, 64, 3])\n",
    "backbone.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inputs = Input(shape=(3, 3, 2))      # 9 + 9\n",
    "outputs = Conv2D(10, 3, groups=2)(inputs) # 9*5 + 5 + 9*5 + 5\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_backbone():\n",
    "    backbone = keras.applications.EfficientNetB2(include_top=False, input_shape=[None, None, 3])\n",
    "    c2_output, c3_output, c4_output, c5_output = [\n",
    "        backbone.get_layer(layer_name).output\n",
    "        for layer_name in [\"block2c_add\", \"block3c_add\", \"block5d_add\", \"top_activation\"]]#block5c_add, block6d_add\n",
    "    #c4_output = (c4_output + c4a_output[:, :, :, :80])/2\n",
    "    return keras.Model(\n",
    "        inputs=[backbone.inputs], outputs=[c2_output, c3_output, c4_output, c5_output]\n",
    "    )\n",
    "backbone = get_backbone()\n",
    "#D0 for layer_name in [\"block2b_add\", \"block3b_add\", \"block5c_add\", \"block6d_add\"]]\n",
    "#D7 for layer_name in [\"block2f_add\", \"block3g_add\", \"block5j_add\", \"block6d_add\"]]\n",
    "#input                           (None, 64, 64, 3)   \n",
    "#block2b_add (Add)               (None, 16, 16, 24) \n",
    "#block3b_add (Add)               (None, 8, 8, 40)    \n",
    "#block4c_add (Add)               (None, 4, 4, 80)\n",
    "#block5c_add (Add)               (None, 4, 4, 112) \n",
    "#block6d_add (Add)               (None, 2, 2, 192)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone.trainable = False #finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def get_backbone():\n",
    "    backbone = keras.applications.MobileNetV2(include_top=False, input_shape=[None, None, 3])\n",
    "    c3_output, c4_output, c5_output = [\n",
    "        backbone.get_layer(layer_name).output\n",
    "        for layer_name in [\"block_6_expand_relu\", \"block_13_expand_relu\", \"out_relu\"]]\n",
    "    return keras.Model(\n",
    "        inputs=[backbone.inputs], outputs=[c3_output, c4_output, c5_output]\n",
    "    )\n",
    "backbone = get_backbone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createRetinaNet(num_classes, anchor_k, is_train=False):\n",
    "    prior_probability = tf.constant_initializer(-np.log((1 - 0.01) / 0.01))\n",
    "    inputs = Input(shape=(None, None, 3))        \n",
    "    \n",
    "    nets_3 = backbone(inputs, training=is_train)    \n",
    "    #nets_3 = create_resnet_backbone(inputs / 255)            \n",
    "    p456 = BifeaturePyramidNet(nets_3)    \n",
    "    \n",
    "    cls_outputs = []\n",
    "    box_outputs = []\n",
    "    \n",
    "    kernel_init = tf.initializers.he_normal()\n",
    "    regulizer = tf.keras.regularizers.L2(l1)\n",
    "    \n",
    "    conv_h0 = keras.layers.Conv2D(anchor_k * (5+num_classes), 3, padding=\"same\", kernel_initializer=kernel_init, bias_initializer=prior_probability, name='head_0')   \n",
    "    conv_h1 = keras.layers.Conv2D(anchor_k * (5+num_classes), 3, padding=\"same\", kernel_initializer=kernel_init, bias_initializer=prior_probability, name='head_1')   \n",
    "    conv_h2 = keras.layers.Conv2D(anchor_k * (5+num_classes), 3, padding=\"same\", kernel_initializer=kernel_init, bias_initializer=prior_probability, name='head_2')\n",
    "    conv_kernels = [conv_h0, conv_h1, conv_h2]\n",
    "    \n",
    "    drop = keras.layers.Dropout(0.1)\n",
    "    N = tf.shape(nets_3[0])[0]\n",
    "    \n",
    "    cbox_outputs = []    \n",
    "    \n",
    "    for i in range(len(p456)):            \n",
    "        feature = p456[i]\n",
    "        conv_kernel = conv_kernels[i]\n",
    "        cls_out = conv_kernel(drop(feature))        \n",
    "        cbox_out = tf.reshape(cls_out, [N, -1, (5+num_classes)])\n",
    "        cbox_outputs.append(cbox_out)\n",
    "    \n",
    "    outputs = tf.concat(cbox_outputs, axis=1)  \n",
    "    \n",
    "    #outputs = tf.reduce_mean(tf.stack(tf.split(outputs, 4, -1),0),0)        \n",
    "    #outputs = keras.layers.Add(name='detect')([outputs, tf.zeros_like(outputs)])    \n",
    "    \n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)#dual    \n",
    "    #model = keras.Model(inputs=inputs, outputs={\"detect_output\": outputs, \"segment_output\": red})#dual    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _decode_box_predictions(anchor_boxes, box_predictions):\n",
    "    _box_variance = tf.convert_to_tensor([0.1, 0.1, 0.2, 0.2], dtype=tf.float32)\n",
    "    boxes = box_predictions * _box_variance\n",
    "    boxes = tf.concat(\n",
    "        [\n",
    "            boxes[:, :, :2] * anchor_boxes[:, :, 2:] + anchor_boxes[:, :, :2],\n",
    "            tf.math.exp(boxes[:, :, 2:]) * anchor_boxes[:, :, 2:],\n",
    "        ],\n",
    "        axis=-1,\n",
    "    )\n",
    "    boxes_transformed = convert_to_corners(boxes)\n",
    "    return boxes_transformed\n",
    "\n",
    "def decodePredictions(images, predictions, \n",
    "                      num_classes=num_classes,\n",
    "                      confidence_threshold=0.5,\n",
    "                      nms_iou_threshold=0.2,\n",
    "                      max_detections_per_class=1000,\n",
    "                      max_detections=1500,\n",
    "                      box_variance=[0.1, 0.1, 0.2, 0.2]):\n",
    "    \n",
    "    _anchor_box = AnchorBox()\n",
    "        \n",
    "    image_shape = tf.cast(tf.shape(images), dtype=tf.float32)\n",
    "    image_h = padded_image_shape[0]\n",
    "    image_w = padded_image_shape[1]\n",
    "    anchor_boxes = _anchor_box.get_anchors(image_shape[1], image_shape[2])#free size    \n",
    "    #anchor_boxes = _anchor_box.get_anchors(image_h, image_w)\n",
    "    box_predictions = predictions[:, :, :4]\n",
    "    objectness = tf.nn.sigmoid(predictions[:, :, 4:5])\n",
    "    cls_score = predictions[:, :, 5:5+num_classes_real]\n",
    "    cls_prob = tf.nn.softmax(cls_score)\n",
    "    cls_prob_max = tf.reduce_max(cls_prob, -1)\n",
    "    #cls_predictions = tf.round(objectness) * cls_predictions         \n",
    "    #cls_predictions = objectness\n",
    "    cls = tf.argmax(cls_score, -1)\n",
    "    cls = tf.cast(cls, tf.float32)\n",
    "    \n",
    "    boxes = _decode_box_predictions(anchor_boxes[None, ...], box_predictions)\n",
    "    boxes_2d = tf.reshape(boxes, [-1, 4])    \n",
    "    scores = tf.reshape(objectness, [-1, 1])#new\n",
    "    #scores = tf.sqrt(scores * tf.reshape(cls_prob_max, [-1, 1]))#new\n",
    "    cls = tf.reshape(cls, [-1, 1])\n",
    "    ccbox = tf.concat((cls, scores, boxes_2d), -1)\n",
    "    \n",
    "    selected_indices, selected_scores = tf.image.non_max_suppression_with_scores(    \n",
    "        ccbox[:, 2:],\n",
    "        ccbox[:, 1],        \n",
    "        max_detections,\n",
    "        nms_iou_threshold,\n",
    "        confidence_threshold,        \n",
    "    )\n",
    "    output = tf.gather(ccbox, selected_indices)        \n",
    "    return output   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetinaNetBoxLoss(tf.losses.Loss):\n",
    "    \"\"\"Implements Smooth L1 loss\"\"\"\n",
    "\n",
    "    def __init__(self, delta):\n",
    "        super(RetinaNetBoxLoss, self).__init__(\n",
    "            reduction=\"none\", name=\"RetinaNetBoxLoss\"\n",
    "        )\n",
    "        self._delta = delta\n",
    "\n",
    "    def call(self, y_true, y_pred):    \n",
    "        \n",
    "        difference = y_true - y_pred\n",
    "        absolute_difference = tf.abs(difference)\n",
    "        squared_difference = difference ** 2\n",
    "        loss = tf.where(\n",
    "            tf.less(absolute_difference, self._delta),\n",
    "            0.5 * squared_difference,\n",
    "            absolute_difference - 0.5,\n",
    "        )\n",
    "        loss = tf.where(loss < 0.01, 0.0, loss)#new marginal loss        \n",
    "        return tf.reduce_sum(loss, axis=-1)\n",
    "\n",
    "\n",
    "class RetinaNetClassificationLoss(tf.losses.Loss):\n",
    "    \"\"\"Implements Focal loss\"\"\"\n",
    "\n",
    "    def __init__(self, alpha, gamma, num_classes):\n",
    "        super(RetinaNetClassificationLoss, self).__init__(\n",
    "            reduction=\"none\", name=\"RetinaNetClassificationLoss\"\n",
    "        )\n",
    "        self._alpha = alpha\n",
    "        self._gamma = gamma\n",
    "        self._num_classes = num_classes\n",
    "\n",
    "        \n",
    "    def call(self, y_cls, y_pred):\n",
    "        y_cls = tf.cast(y_cls, dtype=tf.int32)\n",
    "        y_hot = tf.one_hot(y_cls, depth=self._num_classes, dtype=tf.float32,)\n",
    "        is_exist_non_human_class = tf.reduce_any(y_cls > 1)\n",
    "        y_positive = tf.cast(y_cls > 1, tf.float32)#finetune, 1:unknown          \n",
    "        obj_score = tf.identity(y_pred[:, :, 0], name='obj_score')\n",
    "        objectness = obj_score + tf.reduce_mean(y_pred[:, :, 1:]*0, axis=-1)\n",
    "       \n",
    "        pt = tf.nn.sigmoid(objectness)        \n",
    "        pt = tf.clip_by_value(pt, 1e-7, 1.0 - 1e-7)\n",
    "                \n",
    "        loss_p = - (1.0 - self._alpha) * tf.pow(1.0 - pt, self._gamma) * y_positive * tf.math.log(pt)        \n",
    "        loss_f = - self._alpha * tf.pow(pt, self._gamma) * (1 - y_positive) * tf.math.log(1 - pt)\n",
    "        loss_obj = loss_p + loss_f\n",
    "        #loss_obj = tf.where(loss_obj < 0.01, 0.0, loss_obj)#new\n",
    "        \n",
    "        y_hot = y_hot[:, :, :num_classes_real]\n",
    "        cls_pt = tf.nn.softmax(y_pred[:, :, 1:1+num_classes_real])        \n",
    "        cls_pt = tf.clip_by_value(cls_pt, 1e-7, 1.0 - 1e-7)\n",
    "        loss_cls_p = - tf.pow(1.0 - cls_pt, self._gamma) * y_hot * tf.math.log(cls_pt)\n",
    "        loss_cls_f = - tf.pow(cls_pt, self._gamma) * (1 - y_hot) * tf.math.log(1 - cls_pt)\n",
    "        loss_cls = tf.reduce_sum(loss_cls_p + loss_cls_f, axis=-1)        \n",
    "        loss_cls = y_positive * loss_cls\n",
    "        loss = self._gamma * (loss_obj + loss_cls)\n",
    "        return loss\n",
    "    \n",
    "class RetinaNetFlipLoss(tf.losses.Loss):\n",
    "    def __init__(self):\n",
    "        super(RetinaNetFlipLoss, self).__init__(reduction=\"auto\", name=\"RetinaNetFlipLoss\")\n",
    "    \n",
    "    def call(self, y, h):\n",
    "        pt = tf.nn.sigmoid(h)        \n",
    "        pt = tf.clip_by_value(pt, 1e-7, 1.0 - 1e-7)\n",
    "        gamma = 2.0\n",
    "        loss_p = - tf.pow(1.0 - pt, gamma) * y * tf.math.log(pt)        \n",
    "        loss_f = - tf.pow(pt, gamma) * (1 - y) * tf.math.log(1 - pt)       \n",
    "        loss = loss_p + loss_f\n",
    "        return loss\n",
    "    \n",
    "class RetinaNetLoss(tf.losses.Loss):\n",
    "    \"\"\"Wrapper to combine both the losses\"\"\"\n",
    "\n",
    "    def __init__(self, num_classes=80, alpha=0.25, gamma=2.5, delta=1.0):#alpha=0.25\n",
    "        super(RetinaNetLoss, self).__init__(reduction=\"auto\", name=\"RetinaNetLoss\")\n",
    "        self._clf_loss = RetinaNetClassificationLoss(alpha, gamma, num_classes-1)\n",
    "        self._box_loss = RetinaNetBoxLoss(delta)\n",
    "        self._flip_loss = RetinaNetFlipLoss()        \n",
    "        self._num_classes = num_classes\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        # y_pred : tf.concat([box_outputs, cls_outputs], axis=-1)\n",
    "        y_pred = tf.cast(y_pred, dtype=tf.float32)\n",
    "        \n",
    "        box_labels = y_true[:, :, :4]\n",
    "        y_cls = y_true[:, :, 4]\n",
    "        positive_max_mask = y_true[:, :, 5] > 0#new\n",
    "        \n",
    "        box_predictions = y_pred[:, :, :4]\n",
    "        h_obj = tf.nn.sigmoid(y_pred[:, :, 4])        \n",
    "        cls_predictions = y_pred[:, :, 4:-1]        \n",
    "        \n",
    "        y_flipped = y_true[:, :, 6]\n",
    "        h_flipped = y_pred[:, :, -1]               \n",
    "    \n",
    "        positive_mask = tf.greater(y_cls, 0.0)\n",
    "        ignore_mask = tf.less(y_cls, 0.0)\n",
    "        \n",
    "        clf_loss = self._clf_loss(y_cls, cls_predictions)\n",
    "        box_loss = self._box_loss(box_labels, box_predictions) \n",
    "        #flip_loss = self._flip_loss(y_flipped, h_flipped)\n",
    "                        \n",
    "        clf_loss = tf.where(ignore_mask, 0.0, clf_loss)        \n",
    "        box_loss = tf.where(positive_mask, box_loss, 0.0)\n",
    "        #flip_loss = tf.where(positive_mask, flip_loss, 0.0)\n",
    "        \n",
    "        max_alpha = 0.1\n",
    "        clf_loss = tf.where(positive_max_mask, (1 + max_alpha) * clf_loss, (1 - max_alpha) * clf_loss)        \n",
    "        box_loss = tf.where(positive_max_mask, (1 + max_alpha) * box_loss, (1 - max_alpha) * box_loss)\n",
    "        #box_loss = tf.where(tf.logical_and(h_obj > 0.5, positive_max_mask), box_loss, 0.0)#new    \n",
    "        \n",
    "        positive_mask = tf.cast(positive_mask, tf.float32)\n",
    "        \n",
    "        normalizer = tf.reduce_sum(positive_mask, axis=-1)                \n",
    "        clf_loss = tf.math.divide_no_nan(tf.reduce_sum(clf_loss, axis=-1), normalizer)\n",
    "        box_loss = tf.math.divide_no_nan(tf.reduce_sum(box_loss, axis=-1), normalizer)\n",
    "        #flip_loss = tf.math.divide_no_nan(tf.reduce_sum(flip_loss, axis=-1), normalizer)\n",
    "        loss = (1 + max_alpha) * clf_loss + box_loss# + flip_loss\n",
    "        return loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(y_true, y_pred):    \n",
    "    y_cls = tf.cast(y_true[:, :, 4], tf.int32)\n",
    "    y_positive = tf.cast(y_cls > 0, tf.int32)\n",
    "    y_bg = tf.cast(tf.abs(y_cls)==0, tf.int32)\n",
    "    h_score = y_pred[:, :, 4]\n",
    "    h_prob = tf.nn.sigmoid(h_score)    \n",
    "    h_postive = tf.cast(tf.round(h_prob), tf.int32)\n",
    "    \n",
    "    true_positives = tf.cast(tf.logical_and(y_cls > 0, h_postive>0), tf.float32)\n",
    "    false_negative = y_positive * (1 - h_postive)\n",
    "                \n",
    "    tp = tf.reduce_sum(true_positives, axis=1)# + 0.01\n",
    "    fn = tf.reduce_sum(false_negative, axis=1)\n",
    "    tp = tf.cast(tp, tf.float32)\n",
    "    fn = tf.cast(fn, tf.float32)\n",
    "    \n",
    "    rec = tp / (tp + fn + 1e-8)\n",
    "    return rec\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    \n",
    "    y_cls_symbol = tf.cast(y_true[:, :, 4], dtype=tf.int32)    \n",
    "    y_cls_symbol = tf.cast(y_cls_symbol != 0, tf.int32)\n",
    "    h_obj_prob = tf.nn.sigmoid(y_pred[:, :, 4])\n",
    "    h_cls_symbol = tf.round(h_obj_prob)    \n",
    "    h_cls_symbol = tf.cast(h_cls_symbol, tf.int32)\n",
    "    \n",
    "    true_positives = y_cls_symbol * h_cls_symbol\n",
    "    false_positive = (1 - y_cls_symbol) * h_cls_symbol\n",
    "    \n",
    "    ones = tf.ones_like(true_positives)\n",
    "    zeeros = tf.zeros_like(true_positives)\n",
    "    true_positives = tf.cast(tf.equal(true_positives, ones), tf.float32)\n",
    "    false_positive = tf.cast(tf.equal(false_positive, ones), tf.float32)\n",
    "    \n",
    "    tp = tf.reduce_sum(true_positives, axis=1)# + 0.01\n",
    "    fp = tf.reduce_sum(false_positive, axis=1)\n",
    "    tp = tf.cast(tp, tf.float32)\n",
    "    fp = tf.cast(fp, tf.float32)\n",
    "    prec = tp / (tp + fp + 1e-8)\n",
    "    return prec\n",
    "\n",
    "def accuracy(y_true, y_pred):    \n",
    "    y_cls = tf.cast(y_true[:, :, 4], tf.int32)\n",
    "    y_positive = y_cls > 0\n",
    "    y_bg = tf.cast(tf.abs(y_cls)==0, tf.int32)\n",
    "    h_score = y_pred[:, :, 4]        \n",
    "    h_prob = tf.nn.sigmoid(h_score)\n",
    "    h_postive = tf.cast(tf.round(h_prob), tf.int32)\n",
    "    h_cls = tf.math.argmax(y_pred[:, :, 5:5+num_classes_real], -1, output_type=tf.int32)        \n",
    "    acc = tf.boolean_mask(tf.equal(y_cls, h_cls), y_positive)    \n",
    "    return acc\n",
    "\n",
    "def flip_accuracy(y_true, y_pred):    \n",
    "    y_cls = tf.cast(y_true[:, :, 4], tf.int32)\n",
    "    y_flip = tf.cast(y_true[:, :, 6], tf.int32)\n",
    "    y_positive = y_cls > 0\n",
    "    h_prob = tf.nn.sigmoid(y_pred[:, :, -1])\n",
    "    h_flip = tf.cast(tf.round(h_prob), tf.int32)    \n",
    "    acc = tf.boolean_mask(tf.equal(y_flip, h_flip), y_positive)    \n",
    "    #acc = tf.reduce_mean(tf.cast(acc, tf.float32))\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "anchor_k = len(label_encoder._anchor_box.aspect_ratios)*len(label_encoder._anchor_box.scales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def generator():    \n",
    "    for i in range(len(input_list_train)):\n",
    "        x = input_list_train[i]\n",
    "        y_box = bbox_list_train[i]                \n",
    "        yield (x, y_box)\n",
    "\n",
    "def generator_test():    \n",
    "    for i in range(len(input_list_test)):\n",
    "        x = input_list_test[i]\n",
    "        y_box = bbox_list_test[i]        \n",
    "        yield (x, y_box)\n",
    "\n",
    "dataset = tf.data.Dataset.from_generator(\n",
    "    generator, \n",
    "    output_types=(tf.uint8, tf.float32), \n",
    "    output_shapes=(tf.TensorShape([None, None, 3]), tf.TensorShape([None, 5])))\n",
    "dataset_test = tf.data.Dataset.from_generator(\n",
    "    generator_test, \n",
    "    output_types=(tf.uint8, tf.float32), \n",
    "    output_shapes=(tf.TensorShape([None, None, 3]), tf.TensorShape([None, 5])))\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "for example in tfds.as_numpy(dataset):\n",
    "    image = example[0]\n",
    "    bbox = example[1]\n",
    "    print(image.dtype, image.shape, bbox.shape, bbox[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4 #finetune 2, 12 OOM\n",
    "autotune = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_dataset = dataset.map(preprocess_data, num_parallel_calls=autotune)\n",
    "#train_dataset = train_dataset.shuffle(8 * batch_size)\n",
    "train_dataset = train_dataset.padded_batch(batch_size=batch_size, padding_values=(0.0, 1e-8, -1), drop_remainder=False)\n",
    "#train_dataset = train_dataset.padded_batch(batch_size=batch_size)\n",
    "train_dataset = train_dataset.map(label_encoder.encode_batch_train, num_parallel_calls=autotune)\n",
    "train_dataset = train_dataset.apply(tf.data.experimental.ignore_errors())\n",
    "train_dataset = train_dataset.prefetch(autotune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = dataset_test.map(preprocess_test_data, num_parallel_calls=autotune)\n",
    "val_dataset = val_dataset.padded_batch(batch_size=1, padding_values=(0.0, 1e-8, -1), drop_remainder=False)\n",
    "#val_dataset = val_dataset.padded_batch(batch_size=batch_size)\n",
    "val_dataset = val_dataset.map(label_encoder.encode_batch, num_parallel_calls=autotune)\n",
    "val_dataset = val_dataset.apply(tf.data.experimental.ignore_errors())\n",
    "val_dataset = val_dataset.prefetch(autotune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=3, linewidth=200)\n",
    "image_height, image_width = padded_image_shape\n",
    "\n",
    "img_check = 0\n",
    "for image, output_map in train_dataset:\n",
    "    print('output_map', output_map.shape)\n",
    "    cbbox = output_map    \n",
    "    bbox = cbbox[:, :, :4]\n",
    "    cls_gt = cbbox[:,:,4]\n",
    "    img_m, image_height, image_width, image_ch = image.shape\n",
    "    anchor_feature_size = [(np.ceil(image_height / 2 ** i), np.ceil(image_width / 2 ** i)) \n",
    "                           for i in range(level_start, level_end)]\n",
    "    print('anchor_feature_size', anchor_feature_size)    \n",
    "    m = len(cbbox)    \n",
    "    positive_count = np.sum(cls_gt>0)\n",
    "    print('cbbox', cbbox.shape)\n",
    "    print('cls_sum',np.sum(cls_gt < 0.0), np.sum(cls_gt == 0.0), \n",
    "          np.sum(cls_gt == 1.0), np.sum(cls_gt > 1.0))\n",
    "    print('cls_mean',np.mean(cls_gt < 0.0), np.mean(cls_gt == 0.0), \n",
    "          np.mean(cls_gt == 1.0), np.mean(cls_gt > 0.0))\n",
    "    print('shape',image.shape, cbbox.shape,'unique', np.unique(cls_gt))\n",
    "    print('anchor_feature_size', anchor_feature_size)\n",
    "    offset = 0\n",
    "    positive_maps = []\n",
    "    for anchor_feature_size_1 in anchor_feature_size:        \n",
    "        fm_h, fm_w = anchor_feature_size_1\n",
    "        fm_h = int(fm_h)\n",
    "        fm_w = int(fm_w)        \n",
    "        fm_wh = int(fm_h * fm_w * anchor_k)\n",
    "        cbbox_anchor = cbbox[:, offset:offset+fm_wh, 4]\n",
    "        cbbox_anchor = np.reshape(cbbox_anchor, [m, fm_h, fm_w, anchor_k])\n",
    "        coount_m1 = np.count_nonzero(cbbox_anchor==-1)\n",
    "        coount_0 = np.count_nonzero(cbbox_anchor==0)\n",
    "        coount_1 = np.count_nonzero(cbbox_anchor==1)\n",
    "        coount_1_over = np.count_nonzero(cbbox_anchor>1)\n",
    "        positive_ratio = np.mean(cbbox_anchor>0)\n",
    "        positive_maps.append(cbbox_anchor>0)\n",
    "        print('cbbox_anchor', cbbox_anchor.shape, coount_m1, coount_0, coount_1, coount_1_over, 'ratio', positive_ratio)\n",
    "        sample_0_cbbox = cbbox_anchor[0]\n",
    "        sample_0_cbbox_sum = np.max(sample_0_cbbox, -1).astype(np.int)       \n",
    "      \n",
    "        offset += fm_wh\n",
    "        if False:            \n",
    "            file_name = str(fm_h)+ '_' + str(fm_w)+ '.txt'\n",
    "            np.savetxt(file_name,sample_0_cbbox_sum, fmt='%d',delimiter='')\n",
    "    img_check = image\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.max(positive_maps[0][0], -1))\n",
    "plt.title(str(positive_maps[0].shape)+ str(np.mean(positive_maps[0][0]))+ ' ' + str(np.sum(positive_maps[0][0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmap0 = np.array(Image.fromarray(np.max(positive_maps[0][0],-1)).resize((image_width, image_height)))\n",
    "pmap1 = np.array(Image.fromarray(np.max(positive_maps[1][0],-1)).resize((image_width, image_height)))\n",
    "pmap2 = np.array(Image.fromarray(np.max(positive_maps[2][0],-1)).resize((image_width, image_height)))\n",
    "#pmap3 = np.array(Image.fromarray(np.max(positive_maps[3][0],-1)).resize((image_width, image_height)))\n",
    "#pmap4 = np.array(Image.fromarray(np.max(positive_maps[4][0],-1)).resize((image_width, image_height)))\n",
    "pmap0 = pmap0.astype(np.uint8)\n",
    "pmap1 = pmap1.astype(np.uint8)\n",
    "pmap2 = pmap2.astype(np.uint8)\n",
    "pmap3 = 0#pmap3.astype(np.uint8)\n",
    "pmap4 = 0#pmap4.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pmap_with_img = np.array(img_check)[0]#*255\n",
    "pmap_with_img = pmap_with_img.astype(np.uint8)\n",
    "pmap_add = np.expand_dims(pmap0+pmap1+pmap2+pmap3+pmap4, -1)\n",
    "pmap = (pmap_add>0).astype(np.uint8)*255\n",
    "mix_rgb = np.concatenate((pmap, pmap_with_img[:,:,1:]),-1)\n",
    "plt.figure(figsize=(14,14))\n",
    "plt.imshow(mix_rgb)\n",
    "plt.title(str(np.mean(pmap_add)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_weight():   \n",
    "    weights_dir = path_weight#\"data\"\n",
    "    #latest_checkpoint = tf.train.latest_checkpoint(weights_dir)\n",
    "    latest_checkpoint = weights_dir \n",
    "    print('latest_checkpoint', latest_checkpoint)\n",
    "    model.load_weights(weights_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer = tf.optimizers.SGD(learning_rate=1e-5, momentum=0.1, clipvalue=5.)#warm up clipvalue=10. !\n",
    "optimizer = tf.optimizers.SGD(learning_rate=1e-4, momentum=0.1, clipvalue=10.)\n",
    "loss_detect = RetinaNetLoss(num_classes)\n",
    "model = createRetinaNet(num_classes, anchor_k)\n",
    "metrics = [recall, precision, accuracy]\n",
    "model.compile(loss=loss_detect, optimizer=optimizer, metrics=metrics)#[recall, precision, accuracy]\n",
    "callbacks_list = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=path_weight,\n",
    "        monitor=\"loss\",\n",
    "        save_best_only=False,\n",
    "        save_weights_only=True,\n",
    "        verbose=0,\n",
    "        save_freq=200\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_weight()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(input_list_train), len(input_list_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "resnet-34  81ms/step, resnet-24  53ms/step\n",
    "efficientB0  : 63ms/step - loss: 1.5916 - recall: 0.9155 - precision: 0.9330 - accuracy: 0.8751\n",
    "effi_single  : 50ms/step - loss: 1.7789 - recall: 0.9034 - precision: 0.9472 - accuracy: 0.9202\n",
    "eff-D7 Freeze: 182ms/step - loss: 2.1849 - recall: 0.9470 - precision: 0.9660 - accuracy: 0.9055 - flip_accuracy: 0.0052\n",
    "eff-D2 Freeze: 48ms/step - loss: 7.2216 - recall: 0.9335 - precision: 0.9731 - accuracy: 0.9438 - flip_accuracy: 0.0020\n",
    "eff-D2 finetu: 48ms/step - loss: 2.0763 - recall: 0.8849 - precision: 0.9794 - accuracy: 0.9230 - flip_accuracy: 0.2376\n",
    " 37s 63ms/step - loss: 3.1660 - recall: 0.9925 - precision: 0.9956 - accuracy: 0.9953 - flip_accuracy: 0.6108\n",
    "'''\n",
    "out = model.evaluate(val_dataset.take(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "hist = model.fit(\n",
    "    train_dataset.take(10000),\n",
    "    validation_data=None,#val_dataset.take(2)\n",
    "    epochs=epochs, \n",
    "    callbacks=callbacks_list,#callbacks_list\n",
    "    verbose=1,\n",
    ")\n",
    "'''\n",
    "\n",
    "effD2 Freeze:32s 474ms/step - loss: 14.7472 - recall: 0.4458 - precision: 0.6697 - accuracy: 0.0049\n",
    "effD2 fine :\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(path_weight)\n",
    "path_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#image = tf.keras.Input(shape=[padded_image_shape[0], padded_image_shape[1], 3], name=\"image\")\n",
    "image = tf.keras.Input(shape=[None, None, 3], name=\"image\")\n",
    "predictions = model(image, training=False)\n",
    "\n",
    "detections = decodePredictions(image, predictions, confidence_threshold=0.5, nms_iou_threshold=0.1)\n",
    "inference_model = tf.keras.Model(inputs=image, outputs=detections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def visualize_detections(\n",
    "    image, boxes, classes, scores, figsize=(12, 10), linewidth=1, color=[0, 0, 1], \n",
    "    boxes_gt=None):\n",
    "    \n",
    "    image = np.array(image, dtype=np.uint8)\n",
    "    image = attach_crop_image(image, boxes, max_crop=200)        \n",
    "    \n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(image)\n",
    "    ax = plt.gca()        \n",
    "   \n",
    "    if boxes_gt is not None:\n",
    "        for box in boxes_gt:        \n",
    "            x1, y1, x2, y2 = box\n",
    "            w, h = x2 - x1, y2 - y1\n",
    "            patch = plt.Rectangle(\n",
    "                [x1, y1], w, h, fill=False, edgecolor=[0,1,0], linewidth=2\n",
    "            )\n",
    "            ax.add_patch(patch)\n",
    "            \n",
    "    for box, cls, score in zip(boxes, classes, scores):        \n",
    "        x1, y1, x2, y2 = box\n",
    "        w, h = x2 - x1, y2 - y1\n",
    "        color_text = edgecolors[cls]\n",
    "        color = [0, 0, 1]\n",
    "        patch = plt.Rectangle(\n",
    "            [x1, y1], w, h, fill=False, edgecolor=color, linewidth=linewidth\n",
    "        )\n",
    "        ax.add_patch(patch)\n",
    "        if len(boxes) < 100:\n",
    "            score_txt = str.format('(%d)%.2f' %(cls, score))\n",
    "            ax.text(x1, y1, score_txt, bbox={\"facecolor\": color_text, \"alpha\": 0.4}, clip_box=ax.clipbox, clip_on=True,)\n",
    "          \n",
    "    plt.show()\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_test(test_datas, bboxs_label, step=1):\n",
    "    i = 0\n",
    "    for image, cbbox in test_datas: \n",
    "        if i%step==0:\n",
    "            #cbbox = output_map[\"detect\"]\n",
    "            #mask_obj = output_map[\"segment\"]  \n",
    "            bbox_annotation = bboxs_label[i]\n",
    "            scale = np.array(image.shape[:2])[::-1]\n",
    "            scale = np.reshape(scale, [1, 2])\n",
    "            scale = np.concatenate((scale, scale), 1)\n",
    "            gt_bbox = bbox_annotation[ :, 1:] * scale\n",
    "               \n",
    "            input_image, _, ratio = resize_and_pad_image(image, jitter=None)\n",
    "            input_image = tf.expand_dims(input_image, axis=0)\n",
    "            #input_image = tf.cast(input_image, tf.uint8)\n",
    "            detected_box = inference_model.predict(input_image)        \n",
    "            print(input_image.shape, 'detected_box', detected_box.shape, scale, 'ratio',ratio)\n",
    "            #(1, 1024, 1920, 3) detected_box (7, 6) [[1920 1080 1920 1080]] ratio tf.Tensor(0.94814813, shape=(), dtype=float32)\n",
    "            cls_h = detected_box[:, 0].astype(np.int)\n",
    "            scores = detected_box[:, 1]\n",
    "            box = detected_box[:, 2:] / ratio\n",
    "            #print('box', box)\n",
    "            \n",
    "            visualize_detections(\n",
    "                image,\n",
    "                box,\n",
    "                cls_h,\n",
    "                scores,\n",
    "                boxes_gt=gt_bbox\n",
    "            )\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_test(dataset, bbox_list_train, step=201)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "check_test(dataset_test, bbox_list_test, step=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_pad_32x_arr(image_arr):\n",
    "    #print('image_arr', image_arr.shape, image_arr.dtype)\n",
    "    stride = 32\n",
    "    img_h = image_arr.shape[0]\n",
    "    img_w = image_arr.shape[1]\n",
    "    img_c = image_arr.shape[2]\n",
    "    #print('add_pad_32x', image_arr.shape, img_h, img_w)\n",
    "    pad_h = (stride - (img_h % stride)) % stride\n",
    "    pad_w = (stride - (img_w % stride)) % stride\n",
    "    padded_h = img_h + pad_h\n",
    "    padded_w = img_w + pad_w\n",
    "    #print('pad_h', pad_h, 'pad_w', pad_w)\n",
    "    image_padded = np.zeros((padded_h, padded_w, img_c), dtype=np.uint8)\n",
    "    image_padded[:img_h, :img_w] = image_arr\n",
    "        \n",
    "    return image_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_bg(image, is_display=True, is_save=False, save_path=''):\n",
    "    scale = np.array(image.shape[:2])[::-1]\n",
    "    scale = np.reshape(scale, [1, 2])\n",
    "    scale = np.concatenate((scale, scale), 1)\n",
    "\n",
    "    #input_image, _, ratio = resize_and_pad_image(image, jitter=None)\n",
    "    input_image = add_pad_32x_arr(image)     \n",
    "    ratio = 1\n",
    "    input_image = tf.expand_dims(input_image, axis=0)\n",
    "\n",
    "    detected_box = inference_model.predict(input_image)        \n",
    "    if len(detected_box) > 0:\n",
    "        #print(input_image.shape, 'detected_box', detected_box.shape, scale, 'ratio',ratio)\n",
    "        #(1, 1024, 1920, 3) detected_box (7, 6) [[1920 1080 1920 1080]] ratio tf.Tensor(0.94814813, shape=(), dtype=float32)\n",
    "        if is_display:\n",
    "            cls_h = detected_box[:, 0].astype(np.int)\n",
    "            scores = detected_box[:, 1]\n",
    "            box = detected_box[:, 2:] / ratio\n",
    "\n",
    "            visualize_detections(\n",
    "                image,\n",
    "                box,\n",
    "                cls_h,\n",
    "                scores                \n",
    "            )\n",
    "        if is_save:\n",
    "            img = Image.fromarray(image)\n",
    "            img.save(save_path +'.jpg')\n",
    "        else:\n",
    "            pass\n",
    "    return int(len(detected_box)>0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_jpg = glob(folder_water_bg + '*')\n",
    "print(len(list_jpg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong = 0\n",
    "for i in range(0, len(list_jpg)//1):    \n",
    "    jpg = list_jpg[i]\n",
    "    #print('i', i, jpg.split(os.sep)[-1])\n",
    "    img = Image.open(jpg)\n",
    "    arr = np.array(img)\n",
    "    \n",
    "    n = check_bg(arr, is_display=True)\n",
    "    if n>0:\n",
    "        wrong += n\n",
    "        print(wrong, i, jpg)#414/815, 348/544, 6/242, 43/1522, 59/2043, 64/2375"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "avi Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import time\n",
    "def video_to_frames(input_loc):\n",
    "    \"\"\"Function to extract frames from input video file\n",
    "    and save them as separate frames in an output directory.\n",
    "    Args:\n",
    "        input_loc: Input video file.\n",
    "        output_loc: Output directory to save the frames.\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "        \n",
    "    folder_split = input_loc.split(os.sep)\n",
    "    file_name = folder_split[-1]\n",
    "    file_name = file_name.split('.')[0]\n",
    "    bg_dir = os.sep.join(folder_split[:-1]) + os.sep + 'bg' + os.sep\n",
    "    output_loc = bg_dir#input_loc[:-4]\n",
    "    print('output_loc', output_loc)\n",
    "    list_img = []\n",
    "    list_path = []\n",
    "    \n",
    "    try:\n",
    "        os.mkdir(output_loc)\n",
    "    except OSError:\n",
    "        pass\n",
    "    # Log the time\n",
    "    time_start = time.time()\n",
    "    # Start capturing the feed\n",
    "    cap = cv2.VideoCapture(input_loc)\n",
    "    # Find the number of frames\n",
    "    \n",
    "    video_length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) - 1\n",
    "    print (\"Number of frames: \", video_length)\n",
    "    count = 0\n",
    "    save_count = 0\n",
    "    print (\"Converting video..\\n\")\n",
    "    # Start converting the video\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        # Extract the frame\n",
    "        ret, frame = cap.read()\n",
    "        count = count + 1\n",
    "        if count%10!=0:continue\n",
    "        # Write the results back to output location.\n",
    "        dst_name = output_loc +\"/%s_%#05d.jpg\" % (file_name, count+1)\n",
    "        try:\n",
    "            #cv2.imwrite(file_name, frame)\n",
    "            b, g, r = np.split(frame, 3, -1)\n",
    "            rgb = np.concatenate((r,g,b), -1)        \n",
    "            \n",
    "            list_img.append(rgb)\n",
    "            list_path.append(file_name + '_' + str(save_count))\n",
    "            save_count += 1\n",
    "        except:\n",
    "            print('except')\n",
    "            break\n",
    "        \n",
    "        # If there are no more frames left\n",
    "        if count%100==0:\n",
    "            print('count', video_length, count)\n",
    "        if (count > (video_length*0.9)):\n",
    "            # Log the time again\n",
    "            break\n",
    "    \n",
    "    time_end = time.time()\n",
    "    # Release the feed\n",
    "    cap.release()\n",
    "    # Print stats\n",
    "    print (\"Done extracting frames.\\n%d frames extracted\" % count)\n",
    "    print (\"It took %d seconds to save %d for conversion .\" % (time_end-time_start, save_count))\n",
    "\n",
    "    return list_img, list_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_avi = '/home/mvlab/Downloads/dataset/water_movie_obj/'\n",
    "file_avi = folder_avi +'20201031-100620~20201031-100820_대야리2_332874518.avi'\n",
    "os.path.isdir(folder_avi), os.path.isfile(file_avi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_bg_avi = '/home/mvlab/Downloads/dataset/water_movie_bg/'\n",
    "path_bg_avi = glob(folder_bg_avi + '*.avi' )\n",
    "if len(path_bg_avi)>0:\n",
    "    file_avi = path_bg_avi[0]\n",
    "    print('file_avi', file_avi)\n",
    "os.path.isdir(folder_bg_avi), os.path.isfile(file_avi), len(path_bg_avi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "list_avi_arr, list_avi_path = video_to_frames(file_avi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_water_bg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_new_bg = '/home/mvlab/Downloads/dataset/water_movie_bg/bg0/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "wrong = 0\n",
    "for j in range(34, len(path_bg_avi)):\n",
    "    file_avi = path_bg_avi[j]\n",
    "    print('open_video',len(path_bg_avi), j, file_avi)\n",
    "    list_avi_arr, list_avi_path = video_to_frames(file_avi)\n",
    "    \n",
    "    for i in range(1, len(list_avi_arr), 1):    \n",
    "        arr = list_avi_arr[i]    \n",
    "        filename = folder_new_bg + list_avi_path[i]    \n",
    "        wrong += check_bg(arr, is_display=False, is_save=True, save_path=filename)\n",
    "        if i%100==0:\n",
    "            print(j, len(list_avi_arr), i, wrong)#414 815, 348 544\n",
    "    list_avi_arr.clear()\n",
    "    list_avi_path.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Freeze Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Convert Keras model to ConcreteFunction\n",
    "full_model = tf.function(lambda x: inference_model(x))\n",
    "full_model = full_model.get_concrete_function(\n",
    "    x=tf.TensorSpec(inference_model.inputs[0].shape, inference_model.inputs[0].dtype))\n",
    "\n",
    "# Get frozen ConcreteFunction\n",
    "frozen_func = convert_variables_to_constants_v2(full_model)\n",
    "frozen_func.graph.as_graph_def()\n",
    "\n",
    "layers = [op.name for op in frozen_func.graph.get_operations()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(type(frozen_func.outputs))\n",
    "frozen_list = frozen_func.outputs\n",
    "print(frozen_list)\n",
    "print(len(frozen_list))\n",
    "\n",
    "print(type(frozen_func.inputs))\n",
    "frozen_list = frozen_func.inputs\n",
    "print(frozen_list)\n",
    "print(len(frozen_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.io.write_graph(graph_or_graph_def=frozen_func.graph,\n",
    "                  logdir=\"./frozen_models\",\n",
    "                  name=\"water_efficientDet-D2_frozen_graph.pb\",\n",
    "                  as_text=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_error(X, Y, Path, step=1):\n",
    "    \n",
    "    for i in range(len(X)): \n",
    "        image = X[i]\n",
    "        bbox_annotation = Y[i]\n",
    "        path = Path[i]\n",
    "        \n",
    "        scale = np.array(image.shape[:2])[::-1]\n",
    "        scale = np.reshape(scale, [1, 2])\n",
    "        scale = np.concatenate((scale, scale), 1)\n",
    "        gt_bbox = bbox_annotation[:, 1:] * scale\n",
    "\n",
    "        input_image, _, ratio = resize_and_pad_image(image, jitter=None)\n",
    "        input_image = tf.expand_dims(input_image, axis=0)\n",
    "\n",
    "        detected_box = inference_model.predict(input_image)\n",
    "        detect_k = len(detected_box)\n",
    "        if detect_k!= len(bbox_annotation):\n",
    "            print(path, input_image.shape, 'detected_box', detected_box.shape, scale, 'ratio',ratio.numpy())\n",
    "            #(1, 1024, 1920, 3) detected_box (7, 6) [[1920 1080 1920 1080]] ratio tf.Tensor(0.94814813, shape=(), dtype=float32)\n",
    "            cls_h = detected_box[:, 0].astype(np.int)\n",
    "            scores = detected_box[:, 1]\n",
    "            box = detected_box[:, 2:] / ratio\n",
    "            #print('box', box)\n",
    "\n",
    "            visualize_detections(\n",
    "                image,\n",
    "                box,\n",
    "                cls_h,\n",
    "                scores,\n",
    "                boxes_gt=gt_bbox\n",
    "            )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 1000\n",
    "end = start + 100\n",
    "check_error(input_list[start:end], bbox_list[start:end], path_list[start:end])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load model from .pb file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model_pb = './water_efficientDet-D2_pb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_model.save(saved_model_pb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_objects = {'recall':recall,'precision':precision}\n",
    "model_loaded = keras.models.load_model(saved_model_pb, custom_objects=custom_objects, compile=False)\n",
    "#model_loaded.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_loaded.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for image, cbbox in val_dataset: \n",
    "    \n",
    "    detected_box = model_loaded.predict(image)\n",
    "    print('detected_box', detected_box.shape)\n",
    "    if len(detected_box) > 0:\n",
    "        cls_h = detected_box[:, 0].astype(np.int)\n",
    "        scores = detected_box[:, 1]\n",
    "        box = detected_box[:, 2:]\n",
    "\n",
    "        visualize_detections(\n",
    "            image[0],\n",
    "            box,\n",
    "            cls_h,\n",
    "            scores\n",
    "        )    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_pb)\n",
    "#converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tflite_model = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the TF Lite model.\n",
    "with tf.io.gfile.GFile('model.tflite', 'wb') as f:\n",
    "  f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls *.tflite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Convert the model.\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(inference_model)\n",
    "tflite_model = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the TF Lite model.\n",
    "with tf.io.gfile.GFile('model.tflite', 'wb') as f:\n",
    "  f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load TFLite model and allocate tensors.\n",
    "interpreter = tf.lite.Interpreter(model_content=tflite_model)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensors.\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Test the TensorFlow Lite model on random input data.\n",
    "input_shape = input_details[0]['shape']\n",
    "input_details, output_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = input_list_train[0]\n",
    "input_data = (np.expand_dims(input_data, 0)/255).astype(np.float32)\n",
    "input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = input_data[:, :padded_image_shape[0], :padded_image_shape[1]]\n",
    "input_data.shape, np.max(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)\n",
    "print('input_data', input_data.shape)\n",
    "interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "\n",
    "interpreter.invoke()\n",
    "tflite_results = interpreter.get_tensor(output_details[0]['index'])\n",
    "tflite_results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tflite_results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detected_box = tflite_results\n",
    "cls_h = detected_box[:, 0].astype(np.int)\n",
    "scores = detected_box[:, 1]\n",
    "box = detected_box[:, 2:] / ratio\n",
    "#print('box', box)\n",
    "\n",
    "visualize_detections(\n",
    "    input_data[0]*255,\n",
    "    box,\n",
    "    cls_h,\n",
    "    scores,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
