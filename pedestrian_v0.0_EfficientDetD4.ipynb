{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_datasets as tfds\n",
    "import pandas as pd\n",
    "\n",
    "from PIL import Image\n",
    "from PIL import ImageFilter\n",
    "import ast\n",
    "import json\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://towardsdatascience.com/building-a-resnet-in-keras-e8f1322a49ba\n",
    "from tensorflow import Tensor\n",
    "from tensorflow.keras.layers import Input, Conv2D, ReLU, BatchNormalization,\\\n",
    "                                    Add, AveragePooling2D, Flatten, Dense, MaxPool2D\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/tensorflow/addons/blob/v0.11.2/tensorflow_addons/image/__init__.py\n",
    "from tensorflow_addons.image.color_ops import sharpness\n",
    "from tensorflow_addons.image.filters import gaussian_filter2d\n",
    "from tensorflow_addons.image.dense_image_warp import dense_image_warp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-4-5405bb3c52bd>:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('2.3.1', True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__, tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\" #CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['bg', 'person', 'bicycle', 'lean', 'car']\n",
    "\n",
    "padded_image_shape = (1024, 1920)\n",
    "anchor_k = 6\n",
    "num_classes = 30\n",
    "num_classes_real = len(class_names)\n",
    "max_data_m = 3000#0\n",
    "use_zoom_up_data = False\n",
    "use_pedestrian = True\n",
    "use_visdrone = True\n",
    "level_start = 4\n",
    "level_end = 7\n",
    "l1 = 1e-9#1e-9\n",
    "activation = 'swish'#'selu' is not converted to tflite\n",
    "kernel_init = tf.initializers.he_normal()\n",
    "edgecolors = np.random.rand(num_classes, 3) \n",
    "edgecolors = np.minimum(edgecolors+0.1, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_pedestrian = '/home/mvlab/Downloads/dataset/통영/'\n",
    "forder_P_DESTRE = '/home/mvlab/Downloads/dataset/P-DESTRE/'\n",
    "folder_weather = '/home/sogangori/Downloads/dataset/weather/'\n",
    "\n",
    "names = ['fn','cname','id', 'x0', 'y0', 'w', 'h']\n",
    "edgecolors = np.random.rand(num_classes, 3) \n",
    "edgecolors = np.minimum(edgecolors+0.1, 1.0)\n",
    "path_weight = \"weight/pedestrian_efficientDet-D4\"\n",
    "\n",
    "os.path.isdir(path_pedestrian), os.path.isdir(forder_P_DESTRE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 16, 25945)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths_avi = glob(path_pedestrian+'*.avi')\n",
    "paths_txt = glob(path_pedestrian+'*/*.txt')\n",
    "paths_img = glob(path_pedestrian+'*/*')\n",
    "len(paths_avi), len(paths_txt), len(paths_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16,\n",
       " ['/home/mvlab/Downloads/dataset/통영/정래현/image_label.txt',\n",
       "  '/home/mvlab/Downloads/dataset/통영/미수2/미수2.txt',\n",
       "  '/home/mvlab/Downloads/dataset/통영/IMAGE4/IMAGE2.txt',\n",
       "  '/home/mvlab/Downloads/dataset/통영/IMAGE1/IMAGE fix.txt',\n",
       "  '/home/mvlab/Downloads/dataset/통영/미수1/미수1.txt',\n",
       "  '/home/mvlab/Downloads/dataset/통영/IMAGE3/IMAGE3.txt',\n",
       "  '/home/mvlab/Downloads/dataset/통영/조윤선/image_label.txt',\n",
       "  '/home/mvlab/Downloads/dataset/통영/도천1/도천1.txt',\n",
       "  '/home/mvlab/Downloads/dataset/통영/bridge_img_01/image_label.txt',\n",
       "  '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_.txt',\n",
       "  '/home/mvlab/Downloads/dataset/통영/도천2/도천2.txt',\n",
       "  '/home/mvlab/Downloads/dataset/통영/서영재/image_label.txt',\n",
       "  '/home/mvlab/Downloads/dataset/통영/IMAGE2/IMAGE2.txt',\n",
       "  '/home/mvlab/Downloads/dataset/통영/이예진/image_label.txt',\n",
       "  '/home/mvlab/Downloads/dataset/통영/IMAGE5/IMAGE1.txt',\n",
       "  '/home/mvlab/Downloads/dataset/통영/박성욱/image_label.txt'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(paths_txt), paths_txt#14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, '/home/sogangori/Downloads/dataset/weather/')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_weather_effect = os.path.isdir(folder_weather)\n",
    "use_weather_effect, folder_weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_images = []\n",
    "if use_weather_effect:\n",
    "    path_weathers = glob(folder_weather + '*.*')\n",
    "    if len(path_weathers)>0:\n",
    "        for path_weather in path_weathers:            \n",
    "            image = Image.open(path_weather) \n",
    "            weather_images.append(image)            \n",
    "            plt.imshow(image)\n",
    "            plt.show()\n",
    "    else:\n",
    "        use_weather_effect = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### video label load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/mvlab/Downloads/dataset/통영/정래현/image_label.txt',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수2/미수2.txt',\n",
       " '/home/mvlab/Downloads/dataset/통영/IMAGE4/IMAGE2.txt',\n",
       " '/home/mvlab/Downloads/dataset/통영/IMAGE1/IMAGE fix.txt',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수1/미수1.txt',\n",
       " '/home/mvlab/Downloads/dataset/통영/IMAGE3/IMAGE3.txt',\n",
       " '/home/mvlab/Downloads/dataset/통영/조윤선/image_label.txt',\n",
       " '/home/mvlab/Downloads/dataset/통영/도천1/도천1.txt',\n",
       " '/home/mvlab/Downloads/dataset/통영/bridge_img_01/image_label.txt',\n",
       " '/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_.txt',\n",
       " '/home/mvlab/Downloads/dataset/통영/도천2/도천2.txt',\n",
       " '/home/mvlab/Downloads/dataset/통영/서영재/image_label.txt',\n",
       " '/home/mvlab/Downloads/dataset/통영/IMAGE2/IMAGE2.txt',\n",
       " '/home/mvlab/Downloads/dataset/통영/이예진/image_label.txt',\n",
       " '/home/mvlab/Downloads/dataset/통영/IMAGE5/IMAGE1.txt',\n",
       " '/home/mvlab/Downloads/dataset/통영/박성욱/image_label.txt']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mvlab/Downloads/dataset/통영/정래현/ image_label.txt (363, 7)\n",
      "1.0\n",
      "/home/mvlab/Downloads/dataset/통영/미수2/ 미수2.txt (2291, 7)\n",
      "1.0\n",
      "/home/mvlab/Downloads/dataset/통영/IMAGE4/ IMAGE2.txt (283, 7)\n",
      "1.0\n",
      "/home/mvlab/Downloads/dataset/통영/IMAGE1/ IMAGE fix.txt (84, 7)\n",
      "1.0\n",
      "/home/mvlab/Downloads/dataset/통영/미수1/ 미수1.txt (1085, 7)\n",
      "1.0\n",
      "/home/mvlab/Downloads/dataset/통영/IMAGE3/ IMAGE3.txt (520, 7)\n",
      "1.0\n",
      "/home/mvlab/Downloads/dataset/통영/조윤선/ image_label.txt (748, 7)\n",
      "1.0\n",
      "/home/mvlab/Downloads/dataset/통영/도천1/ 도천1.txt (978, 7)\n",
      "0.8353783231083844\n",
      "/home/mvlab/Downloads/dataset/통영/bridge_img_01/ image_label.txt (758, 7)\n",
      "0.8456464379947229\n",
      "/home/mvlab/Downloads/dataset/통영/미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_/ 미수_스마트시티_통영대교_도천방향1(Ch 01)_[20201215]162700-[20201215]163530(20201215_16270_.txt (1327, 7)\n",
      "1.0\n",
      "/home/mvlab/Downloads/dataset/통영/도천2/ 도천2.txt (2793, 7)\n",
      "0.8918725384890799\n",
      "/home/mvlab/Downloads/dataset/통영/서영재/ image_label.txt (620, 7)\n",
      "1.0\n",
      "/home/mvlab/Downloads/dataset/통영/IMAGE2/ IMAGE2.txt (703, 7)\n",
      "0.9857752489331437\n",
      "/home/mvlab/Downloads/dataset/통영/이예진/ image_label.txt (511, 7)\n",
      "1.0\n",
      "/home/mvlab/Downloads/dataset/통영/IMAGE5/ IMAGE1.txt (262, 7)\n",
      "1.0\n",
      "/home/mvlab/Downloads/dataset/통영/박성욱/ image_label.txt (249, 7)\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "list_df_TongYoung = []\n",
    "\n",
    "for path_txt in paths_txt:\n",
    "    df = pd.read_csv(path_txt, header=None, names=names)    \n",
    "    index_last_sep = path_txt.rindex(os.sep)\n",
    "    path_self_folder = path_txt[:index_last_sep+1]\n",
    "    annot_file_name = path_txt.split(os.sep)[-1]\n",
    "    print(path_self_folder, annot_file_name, df.shape)\n",
    "    for i in range(len(df)):\n",
    "        file_name = df.loc[i, 'fn']\n",
    "        \n",
    "        if '.' not in str(file_name):\n",
    "            file_name = '0' + str(file_name) + '.jpg'\n",
    "        \n",
    "        full_path = path_self_folder + file_name        \n",
    "        df.loc[i, 'exist'] = os.path.isfile(full_path)\n",
    "        df.loc[i, 'path'] = full_path        \n",
    "    \n",
    "    exist = df['exist'].astype(int)\n",
    "    print(exist.mean())\n",
    "    df = df[exist > 0].drop(columns='exist')\n",
    "    if len(df) > 0:\n",
    "        list_df_TongYoung.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12985, 8)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_TongYoung = pd.concat(list_df_TongYoung, axis=0)\n",
    "df_TongYoung.shape#3566, 4315, 4644, 6052, 7843, 8652, 10312, 11479"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['lean', 'bicycle', 'person', 'bike'], dtype=object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_TongYoung.cname.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fn</th>\n",
       "      <th>cname</th>\n",
       "      <th>id</th>\n",
       "      <th>x0</th>\n",
       "      <th>y0</th>\n",
       "      <th>w</th>\n",
       "      <th>h</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>73860.jpg</td>\n",
       "      <td>lean</td>\n",
       "      <td>-1</td>\n",
       "      <td>1183</td>\n",
       "      <td>599</td>\n",
       "      <td>61</td>\n",
       "      <td>189</td>\n",
       "      <td>/home/mvlab/Downloads/dataset/통영/정래현/73860.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>73920.jpg</td>\n",
       "      <td>lean</td>\n",
       "      <td>-1</td>\n",
       "      <td>1188</td>\n",
       "      <td>605</td>\n",
       "      <td>69</td>\n",
       "      <td>181</td>\n",
       "      <td>/home/mvlab/Downloads/dataset/통영/정래현/73920.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>73980.jpg</td>\n",
       "      <td>lean</td>\n",
       "      <td>-1</td>\n",
       "      <td>1186</td>\n",
       "      <td>607</td>\n",
       "      <td>71</td>\n",
       "      <td>178</td>\n",
       "      <td>/home/mvlab/Downloads/dataset/통영/정래현/73980.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>74040.jpg</td>\n",
       "      <td>lean</td>\n",
       "      <td>-1</td>\n",
       "      <td>1186</td>\n",
       "      <td>605</td>\n",
       "      <td>77</td>\n",
       "      <td>182</td>\n",
       "      <td>/home/mvlab/Downloads/dataset/통영/정래현/74040.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>74100.jpg</td>\n",
       "      <td>lean</td>\n",
       "      <td>-1</td>\n",
       "      <td>1175</td>\n",
       "      <td>607</td>\n",
       "      <td>89</td>\n",
       "      <td>179</td>\n",
       "      <td>/home/mvlab/Downloads/dataset/통영/정래현/74100.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>34020.jpg</td>\n",
       "      <td>lean</td>\n",
       "      <td>-1</td>\n",
       "      <td>1211</td>\n",
       "      <td>527</td>\n",
       "      <td>52</td>\n",
       "      <td>148</td>\n",
       "      <td>/home/mvlab/Downloads/dataset/통영/박성욱/34020.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>34080.jpg</td>\n",
       "      <td>lean</td>\n",
       "      <td>-1</td>\n",
       "      <td>1211</td>\n",
       "      <td>527</td>\n",
       "      <td>52</td>\n",
       "      <td>148</td>\n",
       "      <td>/home/mvlab/Downloads/dataset/통영/박성욱/34080.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>34140.jpg</td>\n",
       "      <td>lean</td>\n",
       "      <td>-1</td>\n",
       "      <td>1211</td>\n",
       "      <td>527</td>\n",
       "      <td>52</td>\n",
       "      <td>148</td>\n",
       "      <td>/home/mvlab/Downloads/dataset/통영/박성욱/34140.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>34200.jpg</td>\n",
       "      <td>lean</td>\n",
       "      <td>-1</td>\n",
       "      <td>1211</td>\n",
       "      <td>527</td>\n",
       "      <td>52</td>\n",
       "      <td>148</td>\n",
       "      <td>/home/mvlab/Downloads/dataset/통영/박성욱/34200.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>34260.jpg</td>\n",
       "      <td>lean</td>\n",
       "      <td>-1</td>\n",
       "      <td>1211</td>\n",
       "      <td>527</td>\n",
       "      <td>52</td>\n",
       "      <td>148</td>\n",
       "      <td>/home/mvlab/Downloads/dataset/통영/박성욱/34260.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12985 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            fn cname  id    x0   y0   w    h  \\\n",
       "0    73860.jpg  lean  -1  1183  599  61  189   \n",
       "1    73920.jpg  lean  -1  1188  605  69  181   \n",
       "2    73980.jpg  lean  -1  1186  607  71  178   \n",
       "3    74040.jpg  lean  -1  1186  605  77  182   \n",
       "4    74100.jpg  lean  -1  1175  607  89  179   \n",
       "..         ...   ...  ..   ...  ...  ..  ...   \n",
       "244  34020.jpg  lean  -1  1211  527  52  148   \n",
       "245  34080.jpg  lean  -1  1211  527  52  148   \n",
       "246  34140.jpg  lean  -1  1211  527  52  148   \n",
       "247  34200.jpg  lean  -1  1211  527  52  148   \n",
       "248  34260.jpg  lean  -1  1211  527  52  148   \n",
       "\n",
       "                                               path  \n",
       "0    /home/mvlab/Downloads/dataset/통영/정래현/73860.jpg  \n",
       "1    /home/mvlab/Downloads/dataset/통영/정래현/73920.jpg  \n",
       "2    /home/mvlab/Downloads/dataset/통영/정래현/73980.jpg  \n",
       "3    /home/mvlab/Downloads/dataset/통영/정래현/74040.jpg  \n",
       "4    /home/mvlab/Downloads/dataset/통영/정래현/74100.jpg  \n",
       "..                                              ...  \n",
       "244  /home/mvlab/Downloads/dataset/통영/박성욱/34020.jpg  \n",
       "245  /home/mvlab/Downloads/dataset/통영/박성욱/34080.jpg  \n",
       "246  /home/mvlab/Downloads/dataset/통영/박성욱/34140.jpg  \n",
       "247  /home/mvlab/Downloads/dataset/통영/박성욱/34200.jpg  \n",
       "248  /home/mvlab/Downloads/dataset/통영/박성욱/34260.jpg  \n",
       "\n",
       "[12985 rows x 8 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_TongYoung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['lean', 'bicycle', 'person', 'bike'], dtype=object)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_TongYoung['cname'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fb7347c49d0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_TongYoung['cname'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12985, 8)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_img = df_TongYoung\n",
    "df_img.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P-DESTRE load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PDESTRE_columns = ['frame', 'ID', 'x', 'y', 'w', 'h', 'head', 'yaw', 'pitch', 'roll',          \n",
    "          'gender', 'age', 'height', 'body volume', 'ethnicity', 'hair color', 'hairstyle', 'beard', 'mustache', 'glasses', \n",
    "           'head accessories', 'upper cloth', 'lower cloth', 'feet', 'accessories', 'action']\n",
    "len(PDESTRE_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/mvlab/Downloads/dataset/P-DESTRE/annotation',\n",
       " '/home/mvlab/Downloads/dataset/P-DESTRE/videos']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glob(forder_P_DESTRE + '*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#glob(forder_P_DESTRE + 'annotation/*.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1477385, 28)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_dataframe(forder_P_DESTRE):\n",
    "    list_df = []\n",
    "    \n",
    "    list_annotation_path = glob(forder_P_DESTRE + 'annotation/*.txt')\n",
    "    for i in range(len(list_annotation_path)):\n",
    "        path_anno = list_annotation_path[i]\n",
    "        anno_file_name = path_anno.split(os.sep)[-1]\n",
    "        anno_file_name = anno_file_name[:-4]\n",
    "        df = pd.read_csv(path_anno, header=None, names=PDESTRE_columns)\n",
    "        df['video'] = anno_file_name\n",
    "        list_df.append(df)\n",
    "        #print(i, anno_file_name, df.shape)\n",
    "    \n",
    "    df_all = pd.concat(list_df, axis=0)\n",
    "    df_all = df_all.reset_index()\n",
    "    return df_all \n",
    "\n",
    "df_P_DESTRE = get_dataframe(forder_P_DESTRE)    \n",
    "df_P_DESTRE.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['index', 'frame', 'ID', 'x', 'y', 'w', 'h', 'head', 'yaw', 'pitch',\n",
       "       'roll', 'gender', 'age', 'height', 'body volume', 'ethnicity',\n",
       "       'hair color', 'hairstyle', 'beard', 'mustache', 'glasses',\n",
       "       'head accessories', 'upper cloth', 'lower cloth', 'feet', 'accessories',\n",
       "       'action', 'video'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_P_DESTRE.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gender               3\n",
       "age                  8\n",
       "height               4\n",
       "body volume          3\n",
       "ethnicity            4\n",
       "hair color           6\n",
       "hairstyle            6\n",
       "beard                3\n",
       "mustache             3\n",
       "glasses              4\n",
       "head accessories     4\n",
       "upper cloth         10\n",
       "lower cloth          7\n",
       "feet                 6\n",
       "accessories          6\n",
       "dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_P_DESTRE.iloc[:, 11:-2].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 16 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = df_P_DESTRE.iloc[:, 11:-2].hist(figsize=(10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fb6f07dc700>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_P_DESTRE['ID'].hist() #2:unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fb6f027d220>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_P_DESTRE['gender'].hist() #2:unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fb6f07b9f70>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_P_DESTRE['body volume'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fb6f01b43d0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_P_DESTRE['action'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fb6f07133d0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_P_DESTRE['upper cloth'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_P_DESTRE['gender_'] = df_P_DESTRE['gender'].max() - df_P_DESTRE['gender']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_P_DESTRE['body volume_'] = df_P_DESTRE['body volume'].max() - df_P_DESTRE['body volume']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_P_DESTRE['upper cloth_'] = df_P_DESTRE['upper cloth'].max() - df_P_DESTRE['upper cloth']\n",
    "df_P_DESTRE['upper cloth'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_k = 2\n",
    "body_volumn_k = 3\n",
    "upper_cloth_k = 12\n",
    "label_attribute_k = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(274, 262)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_P_DESTRE['ID'].max(), df_P_DESTRE['ID'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#num_classes_real = df_P_DESTRE['ID'].max() + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'00011'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str.format('%#05d' % 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = []\n",
    "for fr in df_P_DESTRE['frame'].values:\n",
    "    file_names.append(str.format('%#05d.jpg' % fr))\n",
    "    \n",
    "df_P_DESTRE['file_name'] = file_names\n",
    "df_P_DESTRE['path'] = forder_P_DESTRE + 'videos/' + df_P_DESTRE['video'] + os.sep + df_P_DESTRE['file_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>frame</th>\n",
       "      <th>ID</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>w</th>\n",
       "      <th>h</th>\n",
       "      <th>head</th>\n",
       "      <th>yaw</th>\n",
       "      <th>pitch</th>\n",
       "      <th>...</th>\n",
       "      <th>lower cloth</th>\n",
       "      <th>feet</th>\n",
       "      <th>accessories</th>\n",
       "      <th>action</th>\n",
       "      <th>video</th>\n",
       "      <th>gender_</th>\n",
       "      <th>body volume_</th>\n",
       "      <th>upper cloth_</th>\n",
       "      <th>file_name</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "      <td>2593.3</td>\n",
       "      <td>1094.3</td>\n",
       "      <td>143.78</td>\n",
       "      <td>386.91</td>\n",
       "      <td>1</td>\n",
       "      <td>-46.228</td>\n",
       "      <td>-12.39</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>18-07-2019-1-1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>00001.jpg</td>\n",
       "      <td>/home/mvlab/Downloads/dataset/P-DESTRE/videos/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>2060.2</td>\n",
       "      <td>1052.2</td>\n",
       "      <td>108.69</td>\n",
       "      <td>342.89</td>\n",
       "      <td>1</td>\n",
       "      <td>-46.228</td>\n",
       "      <td>-12.39</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>18-07-2019-1-1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>00001.jpg</td>\n",
       "      <td>/home/mvlab/Downloads/dataset/P-DESTRE/videos/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>36</td>\n",
       "      <td>1897.5</td>\n",
       "      <td>1077.2</td>\n",
       "      <td>126.69</td>\n",
       "      <td>331.07</td>\n",
       "      <td>1</td>\n",
       "      <td>-46.228</td>\n",
       "      <td>-12.39</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>18-07-2019-1-1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>00001.jpg</td>\n",
       "      <td>/home/mvlab/Downloads/dataset/P-DESTRE/videos/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>2733.7</td>\n",
       "      <td>1086.1</td>\n",
       "      <td>104.15</td>\n",
       "      <td>315.39</td>\n",
       "      <td>1</td>\n",
       "      <td>-46.228</td>\n",
       "      <td>-12.39</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>18-07-2019-1-1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>00001.jpg</td>\n",
       "      <td>/home/mvlab/Downloads/dataset/P-DESTRE/videos/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>2540.7</td>\n",
       "      <td>1080.6</td>\n",
       "      <td>91.90</td>\n",
       "      <td>338.05</td>\n",
       "      <td>1</td>\n",
       "      <td>-46.228</td>\n",
       "      <td>-12.39</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>18-07-2019-1-1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>00001.jpg</td>\n",
       "      <td>/home/mvlab/Downloads/dataset/P-DESTRE/videos/...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  frame  ID       x       y       w       h  head     yaw  pitch  ...  \\\n",
       "0      0      1  35  2593.3  1094.3  143.78  386.91     1 -46.228 -12.39  ...   \n",
       "1      1      1  25  2060.2  1052.2  108.69  342.89     1 -46.228 -12.39  ...   \n",
       "2      2      1  36  1897.5  1077.2  126.69  331.07     1 -46.228 -12.39  ...   \n",
       "3      3      1  28  2733.7  1086.1  104.15  315.39     1 -46.228 -12.39  ...   \n",
       "4      4      1  37  2540.7  1080.6   91.90  338.05     1 -46.228 -12.39  ...   \n",
       "\n",
       "   lower cloth  feet  accessories  action           video  gender_  \\\n",
       "0            0     0            1       2  18-07-2019-1-1        2   \n",
       "1            0     0            1       2  18-07-2019-1-1        2   \n",
       "2            0     0            1       2  18-07-2019-1-1        2   \n",
       "3            0     0            1       2  18-07-2019-1-1        2   \n",
       "4            0     0            1       2  18-07-2019-1-1        2   \n",
       "\n",
       "   body volume_  upper cloth_  file_name  \\\n",
       "0             1             2  00001.jpg   \n",
       "1             1            12  00001.jpg   \n",
       "2             1            12  00001.jpg   \n",
       "3             1            12  00001.jpg   \n",
       "4             1            12  00001.jpg   \n",
       "\n",
       "                                                path  \n",
       "0  /home/mvlab/Downloads/dataset/P-DESTRE/videos/...  \n",
       "1  /home/mvlab/Downloads/dataset/P-DESTRE/videos/...  \n",
       "2  /home/mvlab/Downloads/dataset/P-DESTRE/videos/...  \n",
       "3  /home/mvlab/Downloads/dataset/P-DESTRE/videos/...  \n",
       "4  /home/mvlab/Downloads/dataset/P-DESTRE/videos/...  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_P_DESTRE.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.027727369642984057"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remove not exist file\n",
    "is_exist_file = []\n",
    "for path_pdestre in df_P_DESTRE['path']:\n",
    "    is_exist_file.append(os.path.isfile(path_pdestre))\n",
    "\n",
    "np.mean(is_exist_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40964, 33)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_P_DESTRE_exist = df_P_DESTRE[is_exist_file]\n",
    "df_P_DESTRE_exist.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_P_DESTRE_cut = df_P_DESTRE_exist[\n",
    "    ['ID', 'ID', 'x', 'y', 'w', 'h', 'path','gender_','body volume_','upper cloth_']]\n",
    "df_P_DESTRE_cut.columns = [\n",
    "    'cname', 'id', 'x0', 'y0', 'w', 'h', 'path','gender_','body volume_','upper cloth_']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cname</th>\n",
       "      <th>id</th>\n",
       "      <th>x0</th>\n",
       "      <th>y0</th>\n",
       "      <th>w</th>\n",
       "      <th>h</th>\n",
       "      <th>path</th>\n",
       "      <th>gender_</th>\n",
       "      <th>body volume_</th>\n",
       "      <th>upper cloth_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1618</th>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>2200.2</td>\n",
       "      <td>2042.80</td>\n",
       "      <td>110.42</td>\n",
       "      <td>118.37</td>\n",
       "      <td>/home/mvlab/Downloads/dataset/P-DESTRE/videos/...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1619</th>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>2028.9</td>\n",
       "      <td>761.69</td>\n",
       "      <td>86.19</td>\n",
       "      <td>270.58</td>\n",
       "      <td>/home/mvlab/Downloads/dataset/P-DESTRE/videos/...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1620</th>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>1804.4</td>\n",
       "      <td>1283.60</td>\n",
       "      <td>125.83</td>\n",
       "      <td>466.63</td>\n",
       "      <td>/home/mvlab/Downloads/dataset/P-DESTRE/videos/...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1621</th>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>2111.7</td>\n",
       "      <td>757.48</td>\n",
       "      <td>69.43</td>\n",
       "      <td>254.57</td>\n",
       "      <td>/home/mvlab/Downloads/dataset/P-DESTRE/videos/...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1622</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2011.8</td>\n",
       "      <td>1380.10</td>\n",
       "      <td>162.78</td>\n",
       "      <td>476.97</td>\n",
       "      <td>/home/mvlab/Downloads/dataset/P-DESTRE/videos/...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1477190</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2500.8</td>\n",
       "      <td>1276.40</td>\n",
       "      <td>135.73</td>\n",
       "      <td>385.91</td>\n",
       "      <td>/home/mvlab/Downloads/dataset/P-DESTRE/videos/...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1477191</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>3499.4</td>\n",
       "      <td>1034.10</td>\n",
       "      <td>100.55</td>\n",
       "      <td>152.40</td>\n",
       "      <td>/home/mvlab/Downloads/dataset/P-DESTRE/videos/...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1477192</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2830.9</td>\n",
       "      <td>1202.50</td>\n",
       "      <td>105.56</td>\n",
       "      <td>276.40</td>\n",
       "      <td>/home/mvlab/Downloads/dataset/P-DESTRE/videos/...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1477193</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2711.3</td>\n",
       "      <td>1171.20</td>\n",
       "      <td>105.94</td>\n",
       "      <td>320.12</td>\n",
       "      <td>/home/mvlab/Downloads/dataset/P-DESTRE/videos/...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1477194</th>\n",
       "      <td>151</td>\n",
       "      <td>151</td>\n",
       "      <td>2729.9</td>\n",
       "      <td>1150.10</td>\n",
       "      <td>73.46</td>\n",
       "      <td>114.48</td>\n",
       "      <td>/home/mvlab/Downloads/dataset/P-DESTRE/videos/...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40964 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         cname   id      x0       y0       w       h  \\\n",
       "1618        25   25  2200.2  2042.80  110.42  118.37   \n",
       "1619        38   38  2028.9   761.69   86.19  270.58   \n",
       "1620        30   30  1804.4  1283.60  125.83  466.63   \n",
       "1621        13   13  2111.7   757.48   69.43  254.57   \n",
       "1622        -1   -1  2011.8  1380.10  162.78  476.97   \n",
       "...        ...  ...     ...      ...     ...     ...   \n",
       "1477190     -1   -1  2500.8  1276.40  135.73  385.91   \n",
       "1477191     -1   -1  3499.4  1034.10  100.55  152.40   \n",
       "1477192     -1   -1  2830.9  1202.50  105.56  276.40   \n",
       "1477193     -1   -1  2711.3  1171.20  105.94  320.12   \n",
       "1477194    151  151  2729.9  1150.10   73.46  114.48   \n",
       "\n",
       "                                                      path  gender_  \\\n",
       "1618     /home/mvlab/Downloads/dataset/P-DESTRE/videos/...        2   \n",
       "1619     /home/mvlab/Downloads/dataset/P-DESTRE/videos/...        2   \n",
       "1620     /home/mvlab/Downloads/dataset/P-DESTRE/videos/...        2   \n",
       "1621     /home/mvlab/Downloads/dataset/P-DESTRE/videos/...        2   \n",
       "1622     /home/mvlab/Downloads/dataset/P-DESTRE/videos/...        2   \n",
       "...                                                    ...      ...   \n",
       "1477190  /home/mvlab/Downloads/dataset/P-DESTRE/videos/...        1   \n",
       "1477191  /home/mvlab/Downloads/dataset/P-DESTRE/videos/...        2   \n",
       "1477192  /home/mvlab/Downloads/dataset/P-DESTRE/videos/...        1   \n",
       "1477193  /home/mvlab/Downloads/dataset/P-DESTRE/videos/...        1   \n",
       "1477194  /home/mvlab/Downloads/dataset/P-DESTRE/videos/...        2   \n",
       "\n",
       "         body volume_  upper cloth_  \n",
       "1618                1            12  \n",
       "1619                1            12  \n",
       "1620                1            12  \n",
       "1621                2             4  \n",
       "1622                1            12  \n",
       "...               ...           ...  \n",
       "1477190             2            11  \n",
       "1477191             2            12  \n",
       "1477192             2            11  \n",
       "1477193             1             6  \n",
       "1477194             2             4  \n",
       "\n",
       "[40964 rows x 10 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_P_DESTRE_cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5480, 40964)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cond = df_P_DESTRE_cut['id']==-1\n",
    "cond.sum(), len(cond)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-47-25745db2f8fa>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_P_DESTRE_cut['id'][cond] = 1\n",
      "/home/mvlab/anaconda3/lib/python3.8/site-packages/pandas/core/generic.py:8765: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._update_inplace(new_data)\n"
     ]
    }
   ],
   "source": [
    "df_P_DESTRE_cut['id'][cond] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((12985, 8), (40964, 10))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_img.shape, df_P_DESTRE_cut.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Index(['fn', 'cname', 'id', 'x0', 'y0', 'w', 'h', 'path'], dtype='object'),\n",
       " Index(['cname', 'id', 'x0', 'y0', 'w', 'h', 'path', 'gender_', 'body volume_',\n",
       "        'upper cloth_'],\n",
       "       dtype='object'))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_img.columns, df_P_DESTRE_cut.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1,  0])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_img['id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12985, 10)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df_img.drop(columns='fn')\n",
    "df['id']= 1 #finetune\n",
    "df['gender_'] = 0\n",
    "df['body volume_'] = 0\n",
    "df['upper cloth_'] = 0\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 7956)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum().sum(), df['path'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12985, 10)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.dropna()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7956"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['path'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['bg', 'person', 'bicycle', 'lean', 'car'], 1)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names, class_names.index('person')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def parsing_annotation(df, is_PEDESTRE=False):\n",
    "    annotation = dict()\n",
    "    for i in range(len(df)):\n",
    "      \n",
    "        row = df.iloc[i].values\n",
    "        cname, iden, x0, y0, w, h, path, gender, body_volume, upper_cloth = row\n",
    "        x1 = x0 + w\n",
    "        y1 = y0 + h\n",
    "        if i%10000==0:\n",
    "            print(i, row)\n",
    "            \n",
    "        cls = 1\n",
    "        if cname=='bike':\n",
    "            cname = 'bicycle'\n",
    "        if cname in class_names:            \n",
    "            cls = class_names.index(cname)\n",
    "        \n",
    "        if is_PEDESTRE:\n",
    "            cls = iden # land \n",
    "            cls = iden * 0 + 1\n",
    "            \n",
    "        bbox = np.array([x0, y0, x1, y1, cls, gender, body_volume, upper_cloth]).reshape((1, -1))        \n",
    "\n",
    "        path_image = path#finetune\n",
    "        if not os.path.isfile(path_image):\n",
    "            print('not exist', path_image)\n",
    "            continue\n",
    "            \n",
    "        if path_image in annotation.keys():\n",
    "            pre_bbox = annotation[path_image]\n",
    "            new_bbox = np.concatenate((pre_bbox, bbox), axis=0)\n",
    "            #cls_bbox = np.stack(cls_bbox, 0)#.reshape([-1, 6])\n",
    "            #annotation[path_image].extend(new_bbox)\n",
    "            annotation[path_image] = new_bbox\n",
    "        else:\n",
    "            annotation[path_image] = bbox        \n",
    "    return annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((12985, 10), (40964, 10))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape, df_P_DESTRE_cut.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ['lean' 1 1183 599 61 189 '/home/mvlab/Downloads/dataset/통영/정래현/73860.jpg'\n",
      " 0 0 0]\n",
      "10000 ['person' 1 1088 326 16 31\n",
      " '/home/mvlab/Downloads/dataset/통영/도천2/02244.png' 0 0 0]\n",
      "0 [25 25 2200.2 2042.8 110.42 118.37\n",
      " '/home/mvlab/Downloads/dataset/P-DESTRE/videos/18-07-2019-1-1/00150.jpg'\n",
      " 2 1 12]\n",
      "10000 [234 234 565.59 845.25 208.75 490.46\n",
      " '/home/mvlab/Downloads/dataset/P-DESTRE/videos/12-11-2019-3-3/01071.jpg'\n",
      " 1 2 6]\n",
      "20000 [140 140 2448.2 495.42 167.71 424.89\n",
      " '/home/mvlab/Downloads/dataset/P-DESTRE/videos/13-11-2019-1-5/00771.jpg'\n",
      " 2 2 4]\n",
      "30000 [163 163 1916.2 1451.5 90.07 281.36\n",
      " '/home/mvlab/Downloads/dataset/P-DESTRE/videos/12-11-2019-2-1/00622.jpg'\n",
      " 2 2 12]\n",
      "40000 [14 14 2213.6 1225.7 124.83 350.28\n",
      " '/home/mvlab/Downloads/dataset/P-DESTRE/videos/10-07-2019-1-3/00150.jpg'\n",
      " 2 1 2]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(7956, 2901)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotation = parsing_annotation(df)\n",
    "annotation_PDESTRE = parsing_annotation(df_P_DESTRE_cut, is_PEDESTRE=True)\n",
    "\n",
    "len(annotation), len(annotation_PDESTRE) #basic:1530"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VisDrone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "visdrone_categories = ['bg','pedestrian', 'person', 'car', 'van', 'bus', 'truck', 'motor', 'bicycle', 'awning-tricycle', 'tricycle','empty_0','empty_1']\n",
    "path_visDrone = '/home/mvlab/Downloads/dataset/VisDrone2019/VisDrone2019-DET-train/'\n",
    "path_visDrone_annotation = path_visDrone + 'annotations/'\n",
    "path_visDrone_image = path_visDrone + 'images/'\n",
    "os.path.isdir(path_visDrone_annotation), os.path.isdir(path_visDrone_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_visdrone_data(path_visDrone, pedestrian_only=True):    \n",
    "    annotation = dict()\n",
    "    path_visDrone_annotation = path_visDrone + 'annotations/'\n",
    "    path_visDrone_image = path_visDrone + 'images/'\n",
    "    os.path.isdir(path_visDrone_annotation), os.path.isdir(path_visDrone_image)\n",
    "    list_annotation = glob(path_visDrone_annotation+'*.*')\n",
    "    list_image_path = glob(path_visDrone_image+'*.*')\n",
    "    \n",
    "    print('len', len(list_annotation), len(list_image_path), list_annotation[0])\n",
    "    \n",
    "    for i in range(len(list_annotation)):\n",
    "        path_annotation = list_annotation[i]\n",
    "        \n",
    "        df = pd.read_csv(path_annotation, header=None)\n",
    "                \n",
    "        cls = np.array(df.iloc[:, 5])\n",
    "        bbox_xywh = np.array(df.iloc[:, :4])        \n",
    "        \n",
    "        file_name_annotation = path_annotation.split('/')[-1].split('.')[0]\n",
    "        file_name_image = path_visDrone_image+file_name_annotation+'.jpg'\n",
    "        \n",
    "        if not os.path.isfile(file_name_image):\n",
    "            print('not exist', file_name_image)\n",
    "            continue\n",
    "                \n",
    "        if pedestrian_only:            \n",
    "            human_mask = np.logical_and(cls > 0, cls < 3)            \n",
    "            car_mask = cls > 2.5\n",
    "            is_human_contain = np.any(human_mask)\n",
    "            human_count = np.sum(human_mask)\n",
    "            if not is_human_contain or human_count < 20:#30:651\n",
    "                continue\n",
    "            \n",
    "            cls = np.where(human_mask, 1.0, 0.0)\n",
    "            cls = np.where(car_mask, 4.0, cls)#new            \n",
    "            use_mask = np.logical_or(human_mask, car_mask)\n",
    "            cls = cls[use_mask]\n",
    "            #print('cls', cls)\n",
    "            bbox_xywh = bbox_xywh[use_mask]            \n",
    "                      \n",
    "        x0 = bbox_xywh[:, 0]\n",
    "        y0 = bbox_xywh[:, 1]\n",
    "        w = bbox_xywh[:, 2]\n",
    "        h = bbox_xywh[:, 3]        \n",
    "        \n",
    "        #annotation[file_name_image] = np.stack((x0, y0, x0 + w, y0 + h, cls), axis=1)\n",
    "        zero = np.zeros_like(w)\n",
    "        annotation[file_name_image] = np.stack((x0, y0, x0 + w, y0 + h, cls, zero, zero, zero), axis=1)\n",
    "        \n",
    "        if len(file_name_image)>max_data_m:\n",
    "            break\n",
    "        if i%100==0:\n",
    "            print(len(list_annotation), i, file_name_annotation, len(file_name_image))\n",
    "    return annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len 6471 6471 /home/mvlab/Downloads/dataset/VisDrone2019/VisDrone2019-DET-train/annotations/9999984_00000_d_0000058.txt\n",
      "6471 1200 0000050_00500_d_0000104 100\n",
      "6471 1400 9999999_00165_d_0000067 100\n",
      "6471 1500 9999937_00000_d_0000179 100\n",
      "6471 1800 9999966_00000_d_0000116 100\n",
      "6471 1900 9999955_00000_d_0000320 100\n",
      "6471 2200 0000351_02941_d_0000534 100\n",
      "6471 2900 9999982_00000_d_0000081 100\n",
      "6471 3000 0000036_00500_d_0000046 100\n",
      "6471 3300 0000288_03601_d_0000802 100\n",
      "6471 3700 9999955_00000_d_0000337 100\n",
      "6471 3800 0000358_02353_d_0000692 100\n",
      "6471 4500 9999955_00000_d_0000375 100\n",
      "6471 4800 0000309_04201_d_0000354 100\n",
      "6471 5100 9999999_00556_d_0000250 100\n",
      "6471 5500 9999943_00000_d_0000068 100\n",
      "6471 5700 0000205_01665_d_0000200 100\n"
     ]
    }
   ],
   "source": [
    "if use_visdrone:\n",
    "    annotation_visdrone = load_visdrone_data(path_visDrone)\n",
    "    print(len(annotation_visdrone))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(annotation), len(annotation_PDESTRE)#, len(annotation_visdrone)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### image load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_bbox_image(image, boxes):\n",
    "    img_objects = []\n",
    "    image = np.array(image)\n",
    "    for box in boxes:        \n",
    "        box = box.astype(np.int)\n",
    "        x1, y1, x2, y2 = box\n",
    "        w = x2 - x1\n",
    "        h = y2 - y1        \n",
    "        crop_image_arr = image[y1:y2, x1:x2]\n",
    "        ch, cw, cc = crop_image_arr.shape\n",
    "        if ch>1 and cw>1:\n",
    "            img_objects.append(crop_image_arr)\n",
    "        else:\n",
    "            print('crop_bbox_image', x2-x1, y2-y1, 'crop_image_arr.shape', crop_image_arr.shape)\n",
    "        \n",
    "    return img_objects\n",
    "    \n",
    "    \n",
    "def attach_crop_image(image, boxes, max_crop=200):\n",
    "        \n",
    "    crop_bbox_arr = crop_bbox_image(image, np.array(boxes)[:max_crop])\n",
    "    bbox_k = len(crop_bbox_arr)\n",
    "    max_col = 30\n",
    "    \n",
    "    if bbox_k > 0:\n",
    "        img_h, img_w, img_c = image.shape\n",
    "        object_img_w = img_w//bbox_k        \n",
    "        resize_h = img_h // 8\n",
    "        resize_w = img_w // bbox_k  \n",
    "        resize_w = min(max(resize_w, img_w//max_col), img_w//8)\n",
    "        \n",
    "        footer_h = resize_h * (1 + (bbox_k-1)//max_col)\n",
    "        footer = np.zeros((footer_h, img_w, img_c), np.uint8)\n",
    "        \n",
    "        for i in range(min(bbox_k, max_crop)):\n",
    "            crop_arr = crop_bbox_arr[i]\n",
    "            crop_img = Image.fromarray(crop_arr)                \n",
    "            crop_img = crop_img.resize((resize_w, resize_h))\n",
    "            crop_arr_resized = np.array(crop_img)\n",
    "            offset_y = (i//max_col) * resize_h\n",
    "            offset_x = (i%max_col) * resize_w\n",
    "            footer[offset_y:offset_y+resize_h, offset_x:offset_x+resize_w] = crop_arr_resized\n",
    "\n",
    "        seperate_line = np.zeros_like(footer[:2])\n",
    "        image = np.concatenate((image, seperate_line, footer), axis=0)    \n",
    "    return image    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_detections_simple(\n",
    "    image, boxes, classes, figsize=(12, 12), linewidth=1, color=[0, 0, 1]\n",
    "):\n",
    "    \"\"\"Visualize Detections\"\"\"\n",
    "    image = np.array(image, dtype=np.uint8)    \n",
    "    \n",
    "    img_h, img_w, img_c = image.shape\n",
    "    \n",
    "    image = attach_crop_image(image, boxes, max_crop=100)\n",
    "    \n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(image)\n",
    "    ax = plt.gca()\n",
    "    boxes_width = boxes[:, 2] - boxes[:, 0]\n",
    "    boxes_height = boxes[:, 3] - boxes[:, 1]\n",
    "    box_min_width = np.min(boxes_width)\n",
    "    box_max_width = np.max(boxes_width)\n",
    "    box_min_height = np.min(boxes_height)\n",
    "    box_max_height = np.max(boxes_height)\n",
    "    title = str.format('(%dx%d) %d box, width:%d ~ %d, height:%d ~ %d' \n",
    "                       %(img_h, img_w, len(boxes), box_min_width, box_max_width, box_min_height, box_max_height))\n",
    "    plt.title(title)\n",
    "    for box, cls in zip(boxes, classes):\n",
    "        x1, y1, x2, y2 = box        \n",
    "        w, h = x2 - x1, y2 - y1\n",
    "        \n",
    "        color = edgecolors[min(len(edgecolors)-1,int(cls))]\n",
    "        patch = plt.Rectangle(\n",
    "            [x1, y1], w, h, fill=False, edgecolor=color, linewidth=linewidth\n",
    "        )\n",
    "        ax.add_patch(patch)\n",
    "        if len(boxes) < 170:\n",
    "            #score_txt = class_names[int(cls)]\n",
    "            score_txt = str(int(cls))\n",
    "            ax.text(x1, y1, score_txt, bbox={\"facecolor\": [1,1,0], \"alpha\": 0.4}, clip_box=ax.clipbox, clip_on=True,)\n",
    "        \n",
    "    plt.show()\n",
    "    return ax\n",
    "\n",
    "def display_data(X, BBOX, stride=1):\n",
    "    for i in range(len(X)):\n",
    "        if i%stride==0:\n",
    "            img_arr = X[i]\n",
    "            sample_box = BBOX[i]            \n",
    "            bbox = sample_box[:, :4]\n",
    "            label = sample_box[:, 4]\n",
    "\n",
    "            h, w, c = img_arr.shape\n",
    "            scale = np.array((w, h, w, h))\n",
    "            scale = np.reshape(scale, (1, 4))\n",
    "            bbox_norm = bbox.astype(np.float) * scale.astype(np.float)\n",
    "            #print('bbox_norm', bbox, bbox_norm)\n",
    "            print(i, np.unique(label))\n",
    "            ax = visualize_detections_simple(img_arr,bbox_norm,label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_xy(annotation, rescale=1, stride=1, size_cut=False):\n",
    "    input_list = []\n",
    "    bbox_list = []\n",
    "    path_list = []\n",
    "    i = 0\n",
    "    for path_image in annotation:\n",
    "        i+=1\n",
    "        if stride!=1 and np.random.randint(1, 1+stride)%stride!=0:\n",
    "            continue\n",
    "            \n",
    "        cls_bbox = annotation[path_image]                \n",
    "        bbox = np.array(cls_bbox[:, :4])\n",
    "        attribute = cls_bbox[:, 4:]\n",
    "\n",
    "        img = Image.open(path_image)    \n",
    "        scale = np.array((img.width, img.height, img.width, img.height))\n",
    "        scale = np.reshape(scale, (1, 4))\n",
    "\n",
    "        if rescale!=1:\n",
    "            img = img.resize((img.width//rescale, img.height//rescale))\n",
    "\n",
    "        img_arr = np.array(img)\n",
    "        try:\n",
    "            std_v = np.std(img_arr)\n",
    "            if std_v < 3:\n",
    "                print('std_v', std_v)\n",
    "                continue\n",
    "        except:\n",
    "            print('error', path_image)\n",
    "            continue\n",
    "        \n",
    "        box_width = bbox[:, 3] - bbox[:, 1]\n",
    "        box_height = bbox[:, 3] - bbox[:, 1]\n",
    "        \n",
    "        if np.min(box_width) < 1 or np.min(box_height) < 1:\n",
    "            print('box_size < 1', box_width, box_height)#check\n",
    "            continue\n",
    "            \n",
    "        bbox_norm = bbox.astype(np.float) / scale.astype(np.float)\n",
    "        cls_bbox_norm = np.concatenate((bbox_norm, attribute), axis=1)\n",
    "\n",
    "        if size_cut:                \n",
    "            box_height = cls_bbox_norm[:, 3] - cls_bbox_norm[:, 1]\n",
    "            box_height_max = np.max(box_height)\n",
    "            if box_height_max < 0.05 or box_height_max > 0.2:\n",
    "                continue\n",
    "\n",
    "        input_list.append(img_arr)\n",
    "        bbox_list.append(cls_bbox_norm)\n",
    "        path_list.append(path_image)\n",
    "        if len(input_list)%100==0:        \n",
    "            print(len(annotation), i, len(input_list))   \n",
    "        if len(input_list) > max_data_m:\n",
    "            break       \n",
    "\n",
    "    print(len(input_list), len(bbox_list))\n",
    "    return input_list, bbox_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(len(annotation))#6258, 7165, 7467\n",
    "input_list, bbox_list = load_xy(annotation, stride=2)\n",
    "#error /home/mvlab/Downloads/dataset/통영/도천2/01748.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(input_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbbox = np.concatenate(bbox_list, 0)\n",
    "print(cbbox.shape, np.max(cbbox[:, 4])) # basic:1736 + P-DESTRE > 9757\n",
    "h = plt.hist(cbbox[:, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display_data(input_list, bbox_list, stride=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if use_pedestrian:\n",
    "    input_list_PDESTRE, bbox_list_PDESTRE = load_xy(annotation_PDESTRE, rescale=4, stride=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if use_visdrone:\n",
    "    list_visdrone_x, list_visdrone_y = load_xy(annotation_visdrone, size_cut=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(list_visdrone_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_visdrone:\n",
    "    cbbox = np.concatenate(list_visdrone_y, 0)\n",
    "    print(cbbox.shape, np.max(cbbox[:, 4])) # basic:1736 + P-DESTRE > 9757\n",
    "    h = plt.hist(cbbox[:, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(input_list)#, len(input_list_PDESTRE)#, len(list_visdrone_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "list_visdrone_x, list_visdrone_y = load_visdrone_data(path_visDrone)\n",
    "print(len(list_visdrone_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#max_data_m, np.unique(np.concatenate(list_visdrone_y, 0)[:, -1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "display_data(list_visdrone_x, list_visdrone_y, stride=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbbox = np.concatenate(bbox_list, 0)\n",
    "print(cbbox.shape, np.max(cbbox[:, 4])) # basic:1736 + P-DESTRE > 9757\n",
    "h = plt.hist(cbbox[:, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "h = plt.hist(cbbox[:, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbbox = np.concatenate(bbox_list, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "box_w = cbbox[:, 2] - cbbox[:, 0]\n",
    "box_h = cbbox[:, 3] - cbbox[:, 1]\n",
    "h = plt.hist(box_h, label='h')\n",
    "h = plt.hist(box_w, label='w')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(box_w, box_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbbox.shape, 1/9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(a<2, 10, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tile_object_biggest_center(X, BBOX, scope=0.5, min_cls=1):\n",
    "    \n",
    "    crop_xs = []\n",
    "    crop_bboxs = []\n",
    "    for i in range(len(X)):\n",
    "        x = X[i]\n",
    "        img_h, img_w, img_c = x.shape\n",
    "        bbox = BBOX[i].copy()\n",
    "        #print('len', len(x), len(bbox), x.shape, bbox.shape)\n",
    "        \n",
    "        x0 = bbox[:, 0]\n",
    "        x1 = bbox[:, 2]\n",
    "        cls = bbox[:, 4]\n",
    "        \n",
    "        if not np.any(cls - 0.1 > min_cls):\n",
    "            continue            \n",
    "    \n",
    "        box_size = (bbox[:, 2] - bbox[:, 0]) * (bbox[:, 3] - bbox[:, 1])\n",
    "        box_max_index = np.argmax(box_size)\n",
    "        if np.max(cls) > 1:\n",
    "            box_max_index = np.argmax(cls)\n",
    "            \n",
    "        #print('box_max_index', box_max_index)\n",
    "        box_center = bbox[box_max_index]\n",
    "        #print('box_center', box_center)\n",
    "                    \n",
    "        cx = (box_center[0] + box_center[2])/2        \n",
    "\n",
    "        if cx < scope:\n",
    "            tx0 = np.maximum(0, cx - scope/2)\n",
    "            tx1 = tx0 + scope\n",
    "        elif cx > 1 - scope:\n",
    "            tx1 = np.minimum(1.0, cx + scope/2)\n",
    "            tx0 = tx1 - scope\n",
    "        else:\n",
    "            tx0 = cx - scope/2\n",
    "            tx1 = cx + scope/2\n",
    "        \n",
    "        epsilon = 1e-8 \n",
    "        bbox[:, 0] = np.where(np.logical_and(x0 < tx0, x1 < tx1) , tx0, x0)        \n",
    "        bbox[:, 2] = np.where(np.logical_and(x1 > tx1, x0 > tx0) , tx1, x1)                \n",
    "        cond = (bbox[:, 2] - bbox[:, 0]) > epsilon\n",
    "        \n",
    "        if not np.any(cond):\n",
    "            print('no valid center box', bbox)\n",
    "            continue\n",
    "            \n",
    "        bbox = bbox[cond]\n",
    "        cls = bbox[:, 4]\n",
    "        attributes = bbox[:, 4:]\n",
    "        \n",
    "        x0 = bbox[:, 0]\n",
    "        y0 = bbox[:, 1]\n",
    "        x1 = bbox[:, 2]\n",
    "        y1 = bbox[:, 3]\n",
    "        tbox = np.stack(((x0 - tx0)/scope, y0, (x1 - tx0)/scope, y1), axis=1)            \n",
    "        tbox = np.concatenate((tbox, attributes), -1)\n",
    "\n",
    "        img_x0 = int(tx0 * img_w)\n",
    "        img_x1 = img_x0 + int(img_w*scope)\n",
    "        timg = x[:, img_x0:img_x1]\n",
    "\n",
    "        img = Image.fromarray(timg)\n",
    "        dst_w = int(padded_image_shape[1]*scope)\n",
    "        img_resized = img.resize((dst_w, padded_image_shape[0]))\n",
    "        arr_resized = np.array(img_resized)            \n",
    "\n",
    "        crop_xs.append(arr_resized)\n",
    "        crop_bboxs.append(tbox)\n",
    "        \n",
    "    return crop_xs, crop_bboxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def attach_tiled_data(X, Y, col=2, row = 1):\n",
    "    \n",
    "    m = len(X)\n",
    "    use_m = (m // (row * col)) * (row * col)    \n",
    "    print('use_m', use_m)\n",
    "    X = np.array(X[:use_m])\n",
    "    Y = np.array(Y[:use_m])\n",
    "    \n",
    "    x_split = np.split(X, col, axis=0)\n",
    "    y_split = np.split(Y, col, axis=0)\n",
    "        \n",
    "    print('y_split', X[0].shape,  x_split[0].shape, x_split[1].shape, y_split[0].shape, y_split[1].shape)\n",
    "        \n",
    "    stride_i = len(x_split[0])\n",
    "    scale_x = 1.0 / col\n",
    "    \n",
    "    list_Y = []    \n",
    "    for i in range(stride_i):\n",
    "        list_y = []\n",
    "                \n",
    "        for j in range(col):\n",
    "            y = Y[i + stride_i * j]\n",
    "            y_box = y[:, :4]\n",
    "            y_attr = y[:, 4:]\n",
    "            x0, y0, x1, y1 = np.split(y_box, 4, -1) \n",
    "            \n",
    "            x0 = x0 * scale_x + j * scale_x\n",
    "            x1 = x1 * scale_x + j * scale_x\n",
    "\n",
    "            y_box = np.concatenate((x0, y0, x1, y1), -1)\n",
    "            new_y = np.concatenate((y_box, y_attr), axis=-1)\n",
    "            \n",
    "            list_y.append(new_y)\n",
    "        \n",
    "        list_Y.append(np.concatenate(list_y, axis=0))\n",
    "       \n",
    "    \n",
    "    x_concat = np.concatenate(x_split, axis=2)\n",
    "    \n",
    "    print('x_concat', x_concat.shape, 'type y',type(y_split))\n",
    "        \n",
    "    return list(x_concat), list_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attach_tiled_data_row(X, Y, row=2):\n",
    "    \n",
    "    m = len(X)\n",
    "    use_m = (m // row) * row    \n",
    "    print('use_m', use_m)\n",
    "    X = np.array(X[:use_m])\n",
    "    Y = np.array(Y[:use_m])\n",
    "    \n",
    "    x_split = np.split(X, row, axis=0)\n",
    "    y_split = np.split(Y, row, axis=0)\n",
    "    \n",
    "    print('len()', len(x_split), len(y_split), len(x_split[0]))\n",
    "    print('y_split', x_split[0].shape, x_split[1].shape, y_split[0].shape, y_split[1].shape)\n",
    "        \n",
    "    stride_i = len(x_split[0])\n",
    "    scale_y = 1.0 / row\n",
    "    \n",
    "    list_Y = []    \n",
    "    for i in range(stride_i):\n",
    "        list_y = []\n",
    "                \n",
    "        for j in range(row):\n",
    "            y = Y[i + stride_i * j]\n",
    "            y_box = y[:, :4]\n",
    "            y_attr = y[:, 4:]\n",
    "            x0, y0, x1, y1 = np.split(y_box, 4, -1) \n",
    "            \n",
    "            y0 = y0 * scale_y + j * scale_y\n",
    "            y1 = y1 * scale_y + j * scale_y\n",
    "\n",
    "            y_box = np.concatenate((x0, y0, x1, y1), -1)\n",
    "            new_y = np.concatenate((y_box, y_attr), axis=-1)\n",
    "            \n",
    "            list_y.append(new_y)\n",
    "        \n",
    "        list_Y.append(np.concatenate(list_y, axis=0))\n",
    "       \n",
    "    \n",
    "    x_concat = np.concatenate(x_split, axis=1)\n",
    "    \n",
    "    print('x_concat', x_concat.shape, 'type y',type(y_split))\n",
    "        \n",
    "    return list(x_concat), list_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bg_paths():\n",
    "    list_bg_jpg = glob(folder_water_bg + '*')\n",
    "    list_bg_jpg0 = glob(folder_water_bg[:-1] + '0/*')\n",
    "    print(len(list_bg_jpg), len(list_bg_jpg0))\n",
    "    return list_bg_jpg0[1::2]#+ list_bg_jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def box_convert_cxy(box):\n",
    "    y0, x0, y1, x1 = np.split(box, 4, axis=-1)    \n",
    "    return np.concatenate(( (y0+y1)/2, (x0+x1)/2 ), axis=1)\n",
    "\n",
    "def box_swap_xy(box):\n",
    "    y0, x0, y1, x1 = np.split(box, 4, axis=-1)    \n",
    "    return np.concatenate((x0, y0, x1, y1), axis=1)\n",
    "\n",
    "def box_convert_to_xywh(boxes):\n",
    "    return np.concatenate(\n",
    "        [(boxes[..., :2] + boxes[..., 2:]) / 2.0, boxes[..., 2:] - boxes[..., :2]], axis=-1,)\n",
    "\n",
    "def box_convert_to_corners(boxes):    \n",
    "    return np.concatenate(\n",
    "        [boxes[..., :2] - boxes[..., 2:] / 2.0, boxes[..., :2] + boxes[..., 2:] / 2.0], axis=-1,)\n",
    "\n",
    "def angle_to_radian(angle):\n",
    "    return angle * np.pi/180\n",
    "\n",
    "def rotate_images(X, angle):\n",
    "    rotate_X = []\n",
    "    for i in range(len(X)):        \n",
    "        x = X[i]\n",
    "        img_h = x.shape[0]\n",
    "        img_w = x.shape[1]\n",
    "        img = Image.fromarray(x)                \n",
    "        img_rotated = img.rotate(angle)\n",
    "        rotate_X.append(np.array(img_rotated))\n",
    "\n",
    "    return rotate_X\n",
    "    \n",
    "def gen_rotate_data(X, BBOX, angle):\n",
    "    rotate_xs = []\n",
    "    rotate_bboxs = []\n",
    "    m = len(X)\n",
    "    for i in range(m):        \n",
    "        x = X[i]\n",
    "        \n",
    "        bbox = BBOX[i]       \n",
    "        attributes = bbox[:, 4:]        \n",
    "        box = bbox[:, :4]\n",
    "        box_xywh = box_convert_to_xywh(box)\n",
    "        box_xy = box_xywh[:, :2] \n",
    "        box_wh = box_xywh[:, 2:] \n",
    "        box_uv = (np.reshape(box_xy, [-1, 2]) - 0.5) * 2\n",
    "        \n",
    "        img_h, img_w, img_c = x.shape\n",
    "        img = Image.fromarray(x)        \n",
    "        scale_mat = np.array([1, 0, 0, 1.0*img_h/img_w]).reshape((2,2))\n",
    "        scale_mat_rev = np.array([1, 0, 0, 1.0*img_w/img_h]).reshape((2,2))\n",
    "        \n",
    "        if np.random.rand() > 0.5:\n",
    "            angle = -angle\n",
    "        radian = angle_to_radian(angle)        \n",
    "        rotate_mat = np.array([np.cos(radian), -np.sin(radian), np.sin(radian), np.cos(radian)])        \n",
    "        rotate_mat = np.reshape(rotate_mat, (2, 2))\n",
    "        box_uv_trans = np.matmul(box_uv, scale_mat)\n",
    "        box_uv_trans = np.matmul(box_uv_trans, rotate_mat)\n",
    "        box_uv_trans = np.matmul(box_uv_trans, scale_mat_rev)\n",
    "        box_trans = (box_uv_trans + 1)/2\n",
    "        box_trans_xy = np.reshape(box_trans, [-1, 2])\n",
    "        box_trans_xywh = np.concatenate((box_trans_xy, box_wh), axis=1)\n",
    "        box_trans = box_convert_to_corners(box_trans_xywh)\n",
    "   \n",
    "        #if np.min(box_trans)<0 or np.max(box_trans)>1: continue               \n",
    "        \n",
    "        bbox_trans = np.concatenate((box_trans, attributes), -1)\n",
    "   \n",
    "        img_rotated = img.rotate(angle)\n",
    "        #plt.imshow(img_rotated)\n",
    "        rotate_xs.append(np.array(img_rotated))\n",
    "        rotate_bboxs.append(bbox_trans)\n",
    "\n",
    "    return rotate_xs, rotate_bboxs        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "display_data(input_list, bbox_list, stride=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "m = len(input_list)\n",
    "print('m', m)\n",
    "s = 1\n",
    "input_list_train = input_list[::s]\n",
    "input_list_test = input_list[1::2]\n",
    "bbox_list_train = bbox_list[::s]\n",
    "bbox_list_test = bbox_list[1::2]\n",
    "print('bbox_list_train', len(bbox_list), len(bbox_list_train), len(bbox_list_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(input_list_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attach_col = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 1\n",
    "crop_xs, crop_bboxs = tile_object_biggest_center(input_list_train[::s], bbox_list_train[::s], scope=1/attach_col, min_cls=0)\n",
    "print('crop_xs', len(crop_xs), len(crop_bboxs), crop_xs[0].shape, crop_bboxs[0])\n",
    "#display_data(crop_xs, crop_bboxs, stride=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for box in crop_bboxs:\n",
    "    w = box[:, 2] - box[:, 0]\n",
    "    w_min = np.min(w)\n",
    "    if w_min < 0.01:\n",
    "        print(w_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_important_class_resample = True\n",
    "is_rotate_attach = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_important_class_resample:    \n",
    "    crop_xs_i, crop_bboxs_i = tile_object_biggest_center(input_list_train[::s], bbox_list_train[::s], scope=1/attach_col, min_cls=2)\n",
    "    print('crop_xs_i', len(crop_xs_i), len(crop_bboxs_i))\n",
    "    rotate_xs_i0, rotate_bboxs_i0 = gen_rotate_data(crop_xs_i, crop_bboxs_i, angle=0.5)\n",
    "    \n",
    "    print('rotate_xs', len(rotate_xs_i0), len(rotate_bboxs_i0))\n",
    "    crop_xs.extend(rotate_xs_i0)\n",
    "    crop_bboxs.extend(rotate_bboxs_i0)    \n",
    "    print(len(crop_xs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(crop_xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_rotate_attach:\n",
    "    rotate_xs, rotate_bboxs = gen_rotate_data(crop_xs, crop_bboxs, angle=0.5)    \n",
    "    rotate_xs.extend(crop_xs)\n",
    "    rotate_bboxs.extend(crop_bboxs)\n",
    "\n",
    "    print('rotate_xs', len(rotate_xs), len(rotate_bboxs))\n",
    "    print('crop_xs', len(crop_xs), len(rotate_xs))\n",
    "else:\n",
    "    rotate_xs = crop_xs\n",
    "    rotate_bboxs = crop_bboxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_permutation(lst, p):\n",
    "    return [lst[x] for x in p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rotate_ind = np.arange(len(rotate_bboxs))\n",
    "np.random.shuffle(rotate_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rotate_xs = apply_permutation(rotate_xs, rotate_ind)\n",
    "rotate_bboxs = apply_permutation(rotate_bboxs, rotate_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attach_x, attach_bbox = attach_tiled_data(rotate_xs, rotate_bboxs, col=attach_col)\n",
    "len(attach_x), len(attach_bbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_data(attach_x, attach_bbox, stride=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "insert_stride = len(input_list_train)//len(attach_x)\n",
    "\n",
    "for i in range(len(attach_x)):\n",
    "    input_list_train.insert(i*insert_stride, attach_x.pop())\n",
    "    bbox_list_train.insert(i*insert_stride, attach_bbox.pop())\n",
    "    \n",
    "print('bbox_list_train', len(input_list_train), len(bbox_list_train), len(bbox_list_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input_list_train.extend(rotate_xs)\n",
    "bbox_list_train.extend(rotate_bboxs)\n",
    "len(input_list_train), len(rotate_xs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "attach_x, attach_bbox = attach_tiled_data(rotate_xs, rotate_bboxs, row=1, col=2)\n",
    "len(attach_x), len(attach_bbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_pedestrian:\n",
    "    display_data(input_list_PDESTRE, bbox_list_PDESTRE, stride=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "display_data(list_visdrone_x, list_visdrone_y, stride=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_pedestrian:    \n",
    "    print(len(input_list_PDESTRE), len(bbox_list_PDESTRE), input_list_PDESTRE[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rotate_xs.extend(crop_xs)\n",
    "rotate_bboxs.extend(crop_bboxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if use_pedestrian:\n",
    "    \n",
    "    m = len(input_list_PDESTRE)\n",
    "    for i in range(m):\n",
    "        x = input_list_PDESTRE[i]\n",
    "        y = bbox_list_PDESTRE[i]\n",
    "        if not x.shape==(540, 960, 3):\n",
    "            print(i, x.shape)\n",
    "            if x.shape[1]<500:\n",
    "                print(x.shape)\n",
    "                input_list_PDESTRE.pop(i)\n",
    "                bbox_list_PDESTRE.pop(i)\n",
    "        m = len(input_list_PDESTRE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('crop_xs', len(crop_xs), len(rotate_xs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_pedestrian:\n",
    "    display_data(input_list_PDESTRE, bbox_list_PDESTRE, stride=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_pedestrian:\n",
    "    attach_xs, attach_bboxs = attach_tiled_data(np.stack(input_list_PDESTRE, 0), bbox_list_PDESTRE, col=2)\n",
    "    \n",
    "    print(len(attach_xs), len(attach_bboxs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_pedestrian:\n",
    "    print(len(attach_xs), len(attach_bboxs))\n",
    "    display_data(attach_xs, attach_bboxs, stride=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_pedestrian:\n",
    "    attach_x_row, attach_bbox_row = attach_tiled_data_row(attach_xs, attach_bboxs, row=2)  \n",
    "    print(len(attach_x_row), len(attach_bbox_row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_pedestrian:\n",
    "    display_data(attach_x_row, attach_bbox_row, stride=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_pedestrian:\n",
    "    print('attach_x', len(attach_x), 'attach_x_row', len(attach_x_row))\n",
    "    attach_x.extend(attach_x_row)\n",
    "    attach_bbox.extend(attach_bbox_row)\n",
    "    print('attach_x', len(attach_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_pedestrian:\n",
    "    insert_stride = len(attach_x)//len(attach_x_row)\n",
    "\n",
    "    for i in range(len(attach_x_row)):\n",
    "        attach_x.insert(i*insert_stride, attach_x_row.pop())\n",
    "        attach_bbox.insert(i*insert_stride, attach_bbox_row.pop())\n",
    "\n",
    "    print('bbox_list_train', len(attach_x), len(attach_x_row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insert_stride = len(input_list_train)//len(attach_x)\n",
    "\n",
    "for i in range(len(attach_x)):\n",
    "    input_list_train.insert(i*insert_stride, attach_x.pop())\n",
    "    bbox_list_train.insert(i*insert_stride, attach_bbox.pop())\n",
    "\n",
    "print('bbox_list_train', len(input_list_train), len(attach_x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input_list_train = attach_x\n",
    "bbox_list_train = attach_bbox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "attach_xs.extend(attach_x)\n",
    "attach_bboxs.extend(attach_bbox)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "len(attach_xs), len(attach_bboxs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "display_data(attach_xs, attach_bboxs, stride=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input_list_train.extend(attach_xs)\n",
    "bbox_list_train.extend(attach_bboxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if use_visdrone:\n",
    "    display_data(list_visdrone_x, list_visdrone_y, stride=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_visdrone:\n",
    "    \n",
    "    insert_stride = len(input_list_train)//len(list_visdrone_x)\n",
    "    print('insert_stride', insert_stride)\n",
    "\n",
    "    for i in range(len(list_visdrone_x)):\n",
    "        input_list_train.insert(i*insert_stride, list_visdrone_x.pop())\n",
    "        bbox_list_train.insert(i*insert_stride, list_visdrone_y.pop())\n",
    "\n",
    "    print('bbox_list_train', len(input_list_train), len(list_visdrone_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('len', len(input_list_train), len(input_list_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_data(input_list_test, bbox_list_test, stride=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_data(input_list_train[:946*2], bbox_list_train[:946*2], stride=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swap_xy(boxes):\n",
    "    \"\"\"Swaps order the of x and y coordinates of the boxes.\n",
    "    Arguments:\n",
    "      boxes: A tensor with shape `(num_boxes, 4)` representing bounding boxes.\n",
    "    Returns:\n",
    "      swapped boxes with shape same as that of boxes.\n",
    "    \"\"\"\n",
    "    return tf.stack([boxes[:, 1], boxes[:, 0], boxes[:, 3], boxes[:, 2]], axis=-1)\n",
    "\n",
    "\n",
    "def convert_to_xywh(boxes):\n",
    "    \"\"\"Changes the box format to center, width and height.\n",
    "    Arguments:\n",
    "      boxes: A tensor of rank 2 or higher with a shape of `(..., num_boxes, 4)`\n",
    "        representing bounding boxes where each box is of the format\n",
    "        `[xmin, ymin, xmax, ymax]`.\n",
    "    Returns:\n",
    "      converted boxes with shape same as that of boxes.\n",
    "    \"\"\"\n",
    "    return tf.concat(\n",
    "        [(boxes[..., :2] + boxes[..., 2:]) / 2.0, boxes[..., 2:] - boxes[..., :2]],\n",
    "        axis=-1,\n",
    "    )\n",
    "\n",
    "\n",
    "def convert_to_corners(boxes):\n",
    "    \"\"\"Changes the box format to corner coordinates\n",
    "    Arguments:\n",
    "      boxes: A tensor of rank 2 or higher with a shape of `(..., num_boxes, 4)`\n",
    "        representing bounding boxes where each box is of the format\n",
    "        `[x, y, width, height]`.\n",
    "    Returns:\n",
    "      converted boxes with shape same as that of boxes.\n",
    "    \"\"\"\n",
    "    return tf.concat(\n",
    "        [boxes[..., :2] - boxes[..., 2:] / 2.0, boxes[..., :2] + boxes[..., 2:] / 2.0],\n",
    "        axis=-1,\n",
    "    )\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "## Computing pairwise Intersection Over Union (IOU)\n",
    "As we will see later in the example, we would be assigning ground truth boxes\n",
    "to anchor boxes based on the extent of overlapping. This will require us to\n",
    "calculate the Intersection Over Union (IOU) between all the anchor\n",
    "boxes and ground truth boxes pairs.\n",
    "\"\"\"\n",
    "\n",
    "def compute_iou(boxes1, boxes2):#compute_iou(anchor_boxes, gt_boxes)\n",
    "    \"\"\"Computes pairwise IOU matrix for given two sets of boxes\n",
    "    Arguments:\n",
    "      boxes1: A tensor with shape `(N, 4)` representing bounding boxes\n",
    "        where each box is of the format `[x, y, width, height]`.\n",
    "        boxes2: A tensor with shape `(M, 4)` representing bounding boxes\n",
    "        where each box is of the format `[x, y, width, height]`.\n",
    "    Returns:\n",
    "      pairwise IOU matrix with shape `(N, M)`, where the value at ith row\n",
    "        jth column holds the IOU between ith box and jth box from\n",
    "        boxes1 and boxes2 respectively.\n",
    "    \"\"\"\n",
    "    boxes1_corners = convert_to_corners(boxes1)\n",
    "    boxes2_corners = convert_to_corners(boxes2)\n",
    "    lu = tf.maximum(boxes1_corners[:, None, :2], boxes2_corners[:, :2])\n",
    "    rd = tf.minimum(boxes1_corners[:, None, 2:], boxes2_corners[:, 2:])\n",
    "    intersection = tf.maximum(0.0, rd - lu)\n",
    "    intersection_area = intersection[:, :, 0] * intersection[:, :, 1]\n",
    "    boxes1_area = boxes1[:, 2] * boxes1[:, 3]\n",
    "    boxes2_area = boxes2[:, 2] * boxes2[:, 3]\n",
    "    union_area = tf.maximum(\n",
    "        boxes1_area[:, None] + boxes2_area - intersection_area, 1e-8\n",
    "    )\n",
    "    return tf.clip_by_value(intersection_area / union_area, 0.0, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "level_end - level_start, anchor_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "## Implementing Anchor generator\n",
    "Anchor boxes are fixed sized boxes that the model uses to predict the bounding\n",
    "box for an object. It does this by regressing the offset between the location\n",
    "of the object's center and the center of an anchor box, and then uses the width\n",
    "and height of the anchor box to predict a relative scale of the object. In the\n",
    "case of RetinaNet, each location on a given feature map has nine anchor boxes\n",
    "(at three scales and three ratios).\n",
    "\"\"\"\n",
    "class AnchorBox:\n",
    "    \"\"\"Generates anchor boxes.\n",
    "    This class has operations to generate anchor boxes for feature maps at\n",
    "    strides `[8, 16, 32, 64, 128]`. Where each anchor each box is of the\n",
    "    format `[x, y, width, height]`.\n",
    "    Attributes:\n",
    "      aspect_ratios: A list of float values representing the aspect ratios of\n",
    "        the anchor boxes at each location on the feature map\n",
    "      scales: A list of float values representing the scale of the anchor boxes\n",
    "        at each location on the feature map.\n",
    "      num_anchors: The number of anchor boxes at each location on feature map\n",
    "      areas: A list of float values representing the areas of the anchor\n",
    "        boxes for each feature map in the feature pyramid.\n",
    "      strides: A list of float value representing the strides for each feature\n",
    "        map in the feature pyramid.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.level_start = level_start\n",
    "        self.level_end = level_end\n",
    "        \n",
    "        if anchor_k==9:\n",
    "            self.aspect_ratios = [0.5, 1.0, 2.0]        \n",
    "            self.scales = [2 ** x for x in [0, 1 / 3, 2 / 3]]\n",
    "        elif anchor_k==6:\n",
    "            self.aspect_ratios = [0.5, 1.0] #area / aspect_ratios. = height\n",
    "            self.scales = [2 ** x for x in [0, 1 / 3, 2 / 3]]        \n",
    "        else:\n",
    "            self.aspect_ratios = [1.0]#area / ratio = height\n",
    "            self.scales = [2 ** x for x in [0]]\n",
    "                \n",
    "        self._num_anchors = len(self.aspect_ratios) * len(self.scales)\n",
    "        self._strides = [2 ** i for i in range(self.level_start, self.level_end)]\n",
    "        self._areas = [x ** 2 for x in [32.0, 64.0, 128.0, 196.0, 256.0]]                        \n",
    "        self._areas = self._areas[:level_end - level_start]\n",
    "        \n",
    "        self._anchor_dims = self._compute_dims()\n",
    "\n",
    "    def _compute_dims(self):\n",
    "        \"\"\"Computes anchor box dimensions for all ratios and scales at all levels\n",
    "        of the feature pyramid.\n",
    "        \"\"\"\n",
    "        anchor_dims_all = []\n",
    "        for area in self._areas:\n",
    "            anchor_dims = []\n",
    "            for ratio in self.aspect_ratios:\n",
    "                anchor_height = tf.math.sqrt(area / ratio)\n",
    "                anchor_width = area / anchor_height\n",
    "                dims = tf.reshape(\n",
    "                    tf.stack([anchor_width, anchor_height], axis=-1), [1, 1, 2]\n",
    "                )\n",
    "                for scale in self.scales:\n",
    "                    anchor_dims.append(scale * dims)\n",
    "            anchor_dims_all.append(tf.stack(anchor_dims, axis=-2))\n",
    "        return anchor_dims_all\n",
    "\n",
    "    def _get_anchors(self, feature_height, feature_width, level):\n",
    "        \"\"\"Generates anchor boxes for a given feature map size and level\n",
    "        Arguments:\n",
    "          feature_height: An integer representing the height of the feature map.\n",
    "          feature_width: An integer representing the width of the feature map.\n",
    "          level: An integer representing the level of the feature map in the\n",
    "            feature pyramid.\n",
    "        Returns:\n",
    "          anchor boxes with the shape\n",
    "          `(feature_height * feature_width * num_anchors, 4)`\n",
    "        \"\"\"\n",
    "        rx = tf.range(feature_width, dtype=tf.float32) + 0.5\n",
    "        ry = tf.range(feature_height, dtype=tf.float32) + 0.5\n",
    "        centers = tf.stack(tf.meshgrid(rx, ry), axis=-1) * self._strides[level - self.level_start]\n",
    "        centers = tf.expand_dims(centers, axis=-2)\n",
    "        centers = tf.tile(centers, [1, 1, self._num_anchors, 1])\n",
    "        dims = tf.tile(\n",
    "            self._anchor_dims[level - self.level_start], [feature_height, feature_width, 1, 1]\n",
    "        )\n",
    "        anchors = tf.concat([centers, dims], axis=-1)\n",
    "        return tf.reshape(\n",
    "            anchors, [feature_height * feature_width * self._num_anchors, 4]\n",
    "        )\n",
    "\n",
    "    def get_anchors(self, image_height, image_width):\n",
    "        \"\"\"Generates anchor boxes for all the feature maps of the feature pyramid.\n",
    "        Arguments:\n",
    "          image_height: Height of the input image.\n",
    "          image_width: Width of the input image.\n",
    "        Returns:\n",
    "          anchor boxes for all the feature maps, stacked as a single tensor\n",
    "            with shape `(total_anchors, 4)`\n",
    "        \"\"\"\n",
    "        anchors = [\n",
    "            self._get_anchors(\n",
    "                tf.math.ceil(image_height / 2 ** i),\n",
    "                tf.math.ceil(image_width / 2 ** i),\n",
    "                i,\n",
    "            )\n",
    "            for i in range(self.level_start, self.level_end)\n",
    "        ]\n",
    "        return tf.concat(anchors, axis=0)\n",
    "    \n",
    "    def get_anchors_check(self, image_height, image_width):\n",
    "        \"\"\"Generates anchor boxes for all the feature maps of the feature pyramid.\n",
    "        Arguments:\n",
    "          image_height: Height of the input image.\n",
    "          image_width: Width of the input image.\n",
    "        Returns:\n",
    "          anchor boxes for all the feature maps, stacked as a single tensor\n",
    "            with shape `(total_anchors, 4)`\n",
    "        \"\"\"\n",
    "        anchors = [\n",
    "            self._get_anchors(\n",
    "                tf.math.ceil(image_height / 2 ** i),\n",
    "                tf.math.ceil(image_width / 2 ** i),\n",
    "                i,\n",
    "            )\n",
    "            for i in range(self.level_start, self.level_end)\n",
    "        ]\n",
    "        return anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "level_start, level_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_check = AnchorBox()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "anchors = anchor_check.get_anchors_check(128,128)\n",
    "for anchor in anchors:\n",
    "    print(anchor.shape, anchor[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_image_shape, 128*8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## Preprocessing data\n",
    "Preprocessing the images involves two steps:\n",
    "- Resizing the image: Images are resized such that the shortest size is equal\n",
    "to 800 px, after resizing if the longest side of the image exceeds 1333 px,\n",
    "the image is resized such that the longest size is now capped at 1333 px.\n",
    "- Applying augmentation: Random scale jittering  and random horizontal flipping\n",
    "are the only augmentations applied to the images.\n",
    "Along with the images, bounding boxes are rescaled and flipped if required.\n",
    "\"\"\"\n",
    "\n",
    "def random_flip_horizontal(image, boxes):\n",
    "    \"\"\"Flips image and boxes horizontally with 50% chance\n",
    "    Arguments:\n",
    "      image: A 3-D tensor of shape `(height, width, channels)` representing an\n",
    "        image.\n",
    "      boxes: A tensor with shape `(num_boxes, 4)` representing bounding boxes,\n",
    "        having normalized coordinates.\n",
    "    Returns:\n",
    "      Randomly flipped image and boxes\n",
    "    \"\"\"\n",
    "    if tf.random.uniform(()) > 0.5:\n",
    "        image = tf.image.flip_left_right(image)\n",
    "        boxes = tf.stack([1 - boxes[:, 2], boxes[:, 1], 1 - boxes[:, 0], boxes[:, 3]], axis=-1)\n",
    "   \n",
    "    return image, boxes\n",
    "\n",
    "def resize_and_pad_image(\n",
    "    image, mask_obj=None, min_side=1024.0, max_side=11333.0, jitter=[128*8, 128*8+1], stride=128.0\n",
    "):\n",
    "   \n",
    "    image_shape = tf.cast(tf.shape(image)[:2], dtype=tf.float32)\n",
    "    \n",
    "    ratio = min_side / tf.reduce_min(image_shape)\n",
    "    \n",
    "    image_shape = ratio * image_shape\n",
    "    image = tf.image.resize(image, tf.cast(image_shape, dtype=tf.int32))\n",
    "    if mask_obj!=None:\n",
    "        mask_obj = tf.image.resize(mask_obj, tf.cast(image_shape, dtype=tf.int32))\n",
    "    \n",
    "    image = tf.image.pad_to_bounding_box(image, 0, 0, padded_image_shape[0], padded_image_shape[1]) \n",
    "    if mask_obj!=None:\n",
    "        mask_obj = tf.image.pad_to_bounding_box(mask_obj, 0, 0, padded_image_shape[0], padded_image_shape[1])        \n",
    "    if mask_obj!=None:\n",
    "        return image, image_shape, ratio, mask_obj\n",
    "    return image, image_shape, ratio\n",
    "\n",
    "def resize_and_pad_image_bbox(\n",
    "    image, bbox, mask_obj=None, min_side=1024.0, max_side=1024.0*4, jitter=[128*7+32, 128*8-32], stride=128.0\n",
    "):\n",
    "    #image, min_side=800.0, max_side=1333.0, jitter=[640, 1024], stride=128.0\n",
    "    \"\"\"Resizes and pads image while preserving aspect ratio.\n",
    "    1. Resizes images so that the shorter side is equal to `min_side`\n",
    "    2. If the longer side is greater than `max_side`, then resize the image\n",
    "      with longer side equal to `max_side`\n",
    "    3. Pad with zeros on right and bottom to make the image shape divisible by\n",
    "    `stride`\n",
    "    Arguments:\n",
    "      image: A 3-D tensor of shape `(height, width, channels)` representing an\n",
    "        image.\n",
    "      min_side: The shorter side of the image is resized to this value, if\n",
    "        `jitter` is set to None.\n",
    "      max_side: If the longer side of the image exceeds this value after\n",
    "        resizing, the image is resized such that the longer side now equals to\n",
    "        this value.\n",
    "      jitter: A list of floats containing minimum and maximum size for scale\n",
    "        jittering. If available, the shorter side of the image will be\n",
    "        resized to a random value in this range.\n",
    "      stride: The stride of the smallest feature map in the feature pyramid.\n",
    "        Can be calculated using `image_size / feature_map_size`.\n",
    "    Returns:\n",
    "      image: Resized and padded image.\n",
    "      image_shape: Shape of the image before padding.\n",
    "      ratio: The scaling factor used to resize the image\n",
    "    \"\"\"\n",
    "    image_shape = tf.cast(tf.shape(image)[:2], dtype=tf.float32)\n",
    "    if jitter is not None:\n",
    "        min_side = tf.random.uniform((), jitter[0], jitter[1], dtype=tf.float32)\n",
    "    ratio = min_side / tf.reduce_min(image_shape)\n",
    "    if ratio * tf.reduce_max(image_shape) > max_side:\n",
    "        ratio = max_side / tf.reduce_max(image_shape)\n",
    "    image_shape = ratio * image_shape\n",
    "    ratio_jitter = tf.random.uniform(tf.shape(image_shape), -32, 32, dtype=tf.float32)\n",
    "    image_shape += ratio_jitter      \n",
    "    image = tf.image.resize(image, tf.cast(image_shape, dtype=tf.int32))\n",
    "    if mask_obj!=None:\n",
    "        mask_obj = tf.image.resize(mask_obj, tf.cast(image_shape, dtype=tf.int32))\n",
    "    padded_image_shape = tf.cast(\n",
    "        tf.math.ceil(image_shape / stride) * stride, dtype=tf.int32\n",
    "    )\n",
    "    image = tf.image.pad_to_bounding_box(image, 0, 0, padded_image_shape[0], padded_image_shape[1])\n",
    "    if mask_obj!=None:\n",
    "        mask_obj = tf.image.pad_to_bounding_box(mask_obj, 0, 0, padded_image_shape[0], padded_image_shape[1])        \n",
    "    padded_image_shape = tf.cast(padded_image_shape, tf.float32)              \n",
    "    pad_ratio = tf.cast(image_shape, tf.float32) / padded_image_shape\n",
    "    bbox_padded = tf.stack(\n",
    "        [\n",
    "            bbox[:, 0] * pad_ratio[1],\n",
    "            bbox[:, 1] * pad_ratio[0],\n",
    "            bbox[:, 2] * pad_ratio[1],\n",
    "            bbox[:, 3] * pad_ratio[0],\n",
    "        ],\n",
    "        axis=-1,\n",
    "    )\n",
    "    if mask_obj!=None:\n",
    "        return image, padded_image_shape, ratio, bbox_padded, mask_obj    \n",
    "    return image, padded_image_shape, ratio, bbox_padded\n",
    "\n",
    "\n",
    "def unnormalize_box(bbox, image_shape):\n",
    "    bbox = tf.stack(\n",
    "        [\n",
    "            bbox[:, 0] * image_shape[1],\n",
    "            bbox[:, 1] * image_shape[0],\n",
    "            bbox[:, 2] * image_shape[1],\n",
    "            bbox[:, 3] * image_shape[0],\n",
    "        ],\n",
    "        axis=-1,\n",
    "    )\n",
    "    bbox = convert_to_xywh(bbox)    \n",
    "    return bbox    \n",
    "\n",
    "\n",
    "def preprocess_data(image, cls_bbox):\n",
    "    \"\"\"Applies preprocessing step to a single sample\n",
    "    Arguments:\n",
    "      sample: A dict representing a single training sample.\n",
    "    Returns:\n",
    "      image: Resized and padded image with random horizontal flipping applied.\n",
    "      bbox: Bounding boxes with the shape `(num_objects, 4)` where each box is\n",
    "        of the format `[x, y, width, height]`.\n",
    "      class_id: An tensor representing the class id of the objects, having\n",
    "        shape `(num_objects,)`.\n",
    "    \"\"\"\n",
    "     \n",
    "    bbox = cls_bbox[:, :4]    \n",
    "    attribute = tf.cast(cls_bbox[:, 4:], dtype=tf.int32)\n",
    "\n",
    "    image, bbox = random_flip_horizontal(image, bbox)        \n",
    "    image, image_shape, _, bbox = resize_and_pad_image_bbox(image, bbox)    \n",
    "    bbox = unnormalize_box(bbox, image_shape)\n",
    "    \n",
    "    return image, bbox, attribute\n",
    "\n",
    "def preprocess_test_data(image, cls_bbox):         \n",
    "    bbox = cls_bbox[:, :4]    \n",
    "    attribute = tf.cast(cls_bbox[:, 4:], dtype=tf.int32)\n",
    "    \n",
    "    image, image_shape, _ = resize_and_pad_image(image)\n",
    "    bbox = unnormalize_box(bbox, image_shape)    \n",
    "    \n",
    "    return image, bbox, attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_weather_effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_color_augment(x):\n",
    "    if tf.random.uniform(()) < -0.5:\n",
    "        x_max = tf.reduce_max(x, [1, 2], True)\n",
    "        x = x_max - x\n",
    "    if tf.random.uniform(()) < -0.2:\n",
    "        r, g, b = tf.split(x, 3, axis=-1)\n",
    "        x = tf.concat((r, b, g), -1)\n",
    "    elif tf.random.uniform(()) < -0.4:\n",
    "        r, g, b = tf.split(x, 3, axis=-1)\n",
    "        x = tf.concat((b, r, g), -1)\n",
    "    if tf.random.uniform(()) < 0.2:\n",
    "        x = tf.image.random_hue(x, 0.08)\n",
    "        x = tf.image.random_saturation(x, 0.6, 1.6)\n",
    "    if tf.random.uniform(()) < 0.2:\n",
    "        x = tf.image.random_brightness(x, 0.05)\n",
    "        x = tf.image.random_contrast(x, 0.7, 1.3)\n",
    "    if tf.random.uniform(()) < -0.2:\n",
    "        gray = tf.image.rgb_to_grayscale(x)\n",
    "        x = tf.concat((gray, gray, gray), -1)        \n",
    "    if tf.random.uniform(()) < -0.2:\n",
    "        noise = tf.random.normal(tf.shape(x), stddev=tf.pow(tf.reduce_mean(x), 0.3))\n",
    "        x += noise\n",
    "    if tf.random.uniform(()) < 0.2:\n",
    "        x = gaussian_filter2d(x, filter_shape=tuple(np.random.randint(1, 10, (2))), sigma=10)\n",
    "        #x = gaussian_filter2d(x, filter_shape=np.random.randint(3, 10, (2)), sigma=10)\n",
    "    if tf.random.uniform(()) < 0.2:        \n",
    "        x = sharpness(x, factor=10)\n",
    "    if tf.random.uniform(()) < 0.2:        \n",
    "        if use_weather_effect:\n",
    "            weather_k = len(weather_images)                            \n",
    "            h = tf.shape(x)[1]\n",
    "            w = tf.shape(x)[2]\n",
    "            weather_image = weather_images[np.random.randint(weather_k)]\n",
    "            weather_image = tf.image.resize(weather_image, tf.cast((h, w), dtype=tf.int32))\n",
    "            weather_image = tf.expand_dims(weather_image, 0)\n",
    "            x = (x // 3) * 2 + weather_image//3\n",
    "            \n",
    "    #x = tf.image.random_jpeg_quality(x, 0, 1.0)\n",
    "    #x = tf.clip_by_value(x, 0, 1)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## Encoding labels\n",
    "The raw labels, consisting of bounding boxes and class ids need to be\n",
    "transformed into targets for training. This transformation consists of\n",
    "the following steps:\n",
    "- Generating anchor boxes for the given image dimensions\n",
    "- Assigning ground truth boxes to the anchor boxes\n",
    "- The anchor boxes that are not assigned any objects, are either assigned the\n",
    "background class or ignored depending on the IOU\n",
    "- Generating the classification and regression targets using anchor boxes\n",
    "\"\"\"\n",
    "\n",
    "class LabelEncoder:\n",
    "    \"\"\"Transforms the raw labels into targets for training.\n",
    "    This class has operations to generate targets for a batch of samples which\n",
    "    is made up of the input images, bounding boxes for the objects present and\n",
    "    their class ids.\n",
    "    Attributes:\n",
    "      anchor_box: Anchor box generator to encode the bounding boxes.\n",
    "      box_variance: The scaling factors used to scale the bounding box targets.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self._anchor_box = AnchorBox()\n",
    "        self._box_variance = tf.convert_to_tensor(\n",
    "            [0.1, 0.1, 0.2, 0.2], dtype=tf.float32\n",
    "        )    \n",
    "    \n",
    "    def _compute_box_target(self, anchor_boxes, matched_gt_boxes):\n",
    "        \"\"\"Transforms the ground truth boxes into targets for training\"\"\"\n",
    "        box_target = tf.concat(\n",
    "            [\n",
    "                (matched_gt_boxes[:, :2] - anchor_boxes[:, :2]) / anchor_boxes[:, 2:],\n",
    "                tf.math.log(matched_gt_boxes[:, 2:] / anchor_boxes[:, 2:]),\n",
    "            ],\n",
    "            axis=-1,\n",
    "        )\n",
    "        box_target = box_target / self._box_variance\n",
    "        return box_target\n",
    "\n",
    "    def _match_anchor_boxes(\n",
    "        self, anchor_boxes, gt_boxes, match_iou=0.35, ignore_iou=0.1\n",
    "    ):\n",
    "        iou_matrix = compute_iou(anchor_boxes, gt_boxes)\n",
    "        max_iou = tf.reduce_max(iou_matrix, axis=1)#from anchor to object-box        \n",
    "        matched_gt_idx = tf.argmax(iou_matrix, axis=1)    \n",
    "        positive_mask = tf.greater_equal(max_iou, match_iou)# not only this, but also need max iou cell\n",
    "        \n",
    "        positive_proposal_mask = tf.greater_equal(iou_matrix, match_iou)\n",
    "        positive_mask = tf.reduce_any(positive_proposal_mask, axis=1)\n",
    "        \n",
    "        negative_mask = tf.less(max_iou, ignore_iou)\n",
    "        \n",
    "        max_iou_anchor = tf.reduce_max(iou_matrix, axis=0, keepdims=True) \n",
    "        max_iou_anchor_mask = tf.greater_equal(iou_matrix, max_iou_anchor)\n",
    "        positive_max_mask = tf.reduce_any(max_iou_anchor_mask, axis=1)\n",
    "        positive_mask = tf.logical_or(positive_mask, positive_max_mask)#new      \n",
    "        \n",
    "        negative_mask = tf.logical_and(negative_mask, tf.logical_not(positive_mask))\n",
    "        ignore_mask = tf.logical_not(tf.logical_or(positive_mask, negative_mask))        \n",
    "        \n",
    "        return (\n",
    "            matched_gt_idx,            \n",
    "            tf.cast(positive_mask, dtype=tf.float32),\n",
    "            tf.cast(positive_max_mask, dtype=tf.float32),            \n",
    "            tf.cast(ignore_mask, dtype=tf.float32),\n",
    "        )\n",
    "\n",
    "    def _encode_sample(self, image_shape, gt_boxes, box_label):\n",
    "        \"\"\"Creates box and classification targets for a single sample\"\"\"\n",
    "        \n",
    "        anchor_boxes = self._anchor_box.get_anchors(image_shape[1], image_shape[2])\n",
    "        box_label = tf.cast(box_label, dtype=tf.float32)\n",
    "        \n",
    "        bx, by, bw, bh = tf.split(gt_boxes, 4, -1)#finetune xywh original size\n",
    "        \n",
    "        bw = tf.squeeze(bw * bh, -1)\n",
    "        bw = tf.sqrt(bw)\n",
    "        \n",
    "        cls_ids, cls_gender, cls_body, upper_cloth = tf.split(box_label, 4, -1)\n",
    "        cls_ids = tf.squeeze(cls_ids, -1)\n",
    "        cls_gender = tf.squeeze(cls_gender, -1)\n",
    "        cls_body = tf.squeeze(cls_body, -1)\n",
    "        upper_cloth = tf.squeeze(upper_cloth, -1)\n",
    "        \n",
    "        matched_gt_idx, positive_mask, positive_max_mask, ignore_mask = self._match_anchor_boxes(\n",
    "            anchor_boxes, gt_boxes\n",
    "        )\n",
    "        matched_gt_boxes = tf.gather(gt_boxes, matched_gt_idx)\n",
    "        matched_gt_boxes_size = tf.reduce_prod(matched_gt_boxes[:, 2:], 1)\n",
    "        matched_gt_boxes_size = tf.sqrt(matched_gt_boxes_size)        \n",
    "        \n",
    "        box_target = self._compute_box_target(anchor_boxes, matched_gt_boxes)    \n",
    "        matched_bw = tf.gather(bw, matched_gt_idx)\n",
    "        matched_gt_cls_ids = tf.gather(cls_ids, matched_gt_idx)\n",
    "        matched_gt_gender = tf.gather(cls_gender, matched_gt_idx)\n",
    "        matched_cls_body = tf.gather(cls_body, matched_gt_idx)\n",
    "        matched_upper_cloth = tf.gather(upper_cloth, matched_gt_idx)\n",
    "        \n",
    "        cls_target = tf.where(tf.not_equal(positive_mask, 1.0), 0.0, matched_gt_cls_ids)        \n",
    "        cls_target = tf.where(tf.equal(ignore_mask, 1.0), -1.0, cls_target)\n",
    "        #bw_thresh = tf.minimum(tf.reduce_min(matched_bw)+1, 10)\n",
    "        #cls_target = tf.where(matched_bw < bw_thresh, -1.0, cls_target) #finetune\n",
    "        \n",
    "        attribute_target = tf.stack((cls_target, matched_gt_gender, matched_cls_body, matched_upper_cloth), -1)\n",
    "        targets = tf.concat([box_target, attribute_target], axis=-1)        \n",
    "        return targets\n",
    "    \n",
    "    def encode_batch(self, batch_images, gt_boxes, box_label):\n",
    "        \"\"\"Creates box and classification targets for a batch\"\"\"\n",
    "        \n",
    "        images_shape = tf.shape(batch_images)\n",
    "        batch_size = images_shape[0]\n",
    "       \n",
    "        labels = tf.TensorArray(dtype=tf.float32, size=batch_size, dynamic_size=True)\n",
    "        for i in range(batch_size):\n",
    "            label = self._encode_sample(images_shape, gt_boxes[i], box_label[i])\n",
    "            labels = labels.write(i, label)\n",
    "        \n",
    "        batch_images = tf.cast(batch_images, tf.float32)\n",
    "        labels = labels.stack()\n",
    "        \n",
    "        #label = tf.concat((label, is_flipped_anchor), -1)\n",
    "        return batch_images, labels      \n",
    "    \n",
    "    def encode_batch_train(self, batch_images, gt_boxes, cls):\n",
    "        \"\"\"Creates box and classification targets for a batch\"\"\"\n",
    "        \n",
    "        batch_images = image_color_augment(batch_images)#finetune        \n",
    "        return self.encode_batch(batch_images, gt_boxes, cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BifeaturePyramidNet(c345):\n",
    "    filters = 128\n",
    "    a2 = c345[0]\n",
    "    a3 = c345[1]\n",
    "    a4 = c345[2]\n",
    "    a5 = c345[3]\n",
    "    \n",
    "    regulizer  = tf.keras.regularizers.L2(l1)\n",
    "    \n",
    "    #b3 = Conv2D(filters, 1, 1, \"same\", groups=1, activation=activation, kernel_regularizer=regulizer)(a3)\n",
    "    #b4 = Conv2D(filters, 1, 1, \"same\", groups=1, activation=activation, kernel_regularizer=regulizer)(a4)\n",
    "    \n",
    "    a2_0 = Conv2D(filters, 1, 1, \"same\", groups=1, activation=activation, kernel_regularizer=regulizer, name='a2_0')(a2)\n",
    "    #a3_0 = Conv2D(filters, 1, 1, \"same\", groups=1, activation=activation, kernel_regularizer=regulizer)(a3)\n",
    "    #a4_0 = Conv2D(filters, 1, 1, \"same\", groups=1, activation=activation, kernel_regularizer=regulizer)(a4)\n",
    "    #a5_0 = Conv2D(filters, 1, 1, \"same\", groups=1, activation=activation, kernel_regularizer=regulizer)(a5)\n",
    "    #a3_1 = Conv2D(filters, 1, 1, \"same\", groups=1, activation=activation, kernel_regularizer=regulizer)(a3)\n",
    "    #a4_1 = Conv2D(filters, 1, 1, \"same\", groups=1, activation=activation, kernel_regularizer=regulizer)(a4)\n",
    "    \n",
    "    a33 = Conv2D(filters*2, 1, 1, \"same\", groups=1, activation=activation, kernel_regularizer=regulizer, name='a33')(a3)\n",
    "    a44 = Conv2D(filters*2, 1, 1, \"same\", groups=1, activation=activation, kernel_regularizer=regulizer, name='a44')(a4)\n",
    "    a55 = Conv2D(filters*2, 1, 1, \"same\", groups=1, activation=activation, kernel_regularizer=regulizer, name='a55')(a5)\n",
    "    a66 = Conv2D(filters*2, 3, 2, \"same\", groups=1, activation=activation, kernel_regularizer=regulizer, name='a66')(a5)\n",
    "    \n",
    "    a3_0, a3_1 = tf.split(a33, 2, -1)\n",
    "    a4_0, a4_1 = tf.split(a44, 2, -1)\n",
    "    a5_0, a5_1 = tf.split(a55, 2, -1)\n",
    "    a6_0, a6_1 = tf.split(a66, 2, -1)\n",
    "    \n",
    "    b6 = a6_0\n",
    "    \n",
    "    a6_up = keras.layers.UpSampling2D(2)(a6_1)    \n",
    "    b5 = keras.layers.Add()([a5_0, a6_up])  \n",
    "        \n",
    "    a5_up = keras.layers.UpSampling2D(2)(a5_1)    \n",
    "    b4 = keras.layers.Add()([a4_0, a5_up])  \n",
    "    \n",
    "    b4_up = keras.layers.UpSampling2D(2)(b4)\n",
    "    b3 = keras.layers.Add()([a3_0, b4_up])  \n",
    "    \n",
    "    b3_up = keras.layers.UpSampling2D(2)(b3)\n",
    "    b2 = keras.layers.Add()([a2_0, b3_up])\n",
    "    \n",
    "    b2_down = Conv2D(filters, 3, 2, \"same\", groups=1, activation=activation, kernel_regularizer=regulizer, name='b2_d')(b2)\n",
    "    b3_1 = Conv2D(filters, 1, 1, \"same\", groups=1, activation=activation, kernel_regularizer=regulizer, name='b3_1')(b3)    \n",
    "    c3 = keras.layers.Add()([a3_1, b3_1, b2_down])\n",
    "    \n",
    "    c3_down = Conv2D(filters, 3, 2, \"same\", groups=1, activation=activation, kernel_regularizer=regulizer, name='c3_d')(c3)\n",
    "    b4_1 = Conv2D(filters, 1, 1, \"same\", groups=1, activation=activation, kernel_regularizer=regulizer, name='b4_1')(b4)    \n",
    "    c4 = keras.layers.Add()([a4_1, b4_1, c3_down])    \n",
    "    \n",
    "    c4_down = Conv2D(filters, 3, 2, \"same\", groups=1, activation=activation, kernel_regularizer=regulizer, name='c4_d')(c4)\n",
    "    b5_1 = Conv2D(filters, 1, 1, \"same\", groups=1, activation=activation, kernel_regularizer=regulizer, name='b5_1')(b5)    \n",
    "    c5 = keras.layers.Add()([a5_1, b5_1, c4_down])    \n",
    "    \n",
    "    c5_down = Conv2D(filters, 3, 2, \"same\", groups=1, activation=activation, kernel_regularizer=regulizer, name='c5_d')(c5)\n",
    "    b6_1 = Conv2D(filters, 1, 1, \"same\", groups=1, activation=activation, kernel_regularizer=regulizer, name='b6_1')(b6)    \n",
    "    c6 = keras.layers.Add()([a6_1, b6_1, c5_down])\n",
    "    \n",
    "    return c4, c5, c6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inputs = Input(shape=(3, 3, 2))  # 18   \n",
    "outputs = Conv2D(10, 3)(inputs)# 18 * 10 + 10 = 190\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone = keras.applications.EfficientNetB4(include_top=False, input_shape=[64, 64, 3])\n",
    "backbone.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inputs = Input(shape=(3, 3, 2))      # 9 + 9\n",
    "outputs = Conv2D(10, 3, groups=2)(inputs) # 9*5 + 5 + 9*5 + 5\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_backbone():\n",
    "    backbone = keras.applications.EfficientNetB4(include_top=False, input_shape=[None, None, 3])\n",
    "    c2_output, c3_output, c4_output, c5_output = [\n",
    "        backbone.get_layer(layer_name).output\n",
    "        for layer_name in [\"block2d_add\", \"block3d_add\", \"block5f_add\", \"block6c_add\"]]#block5c_add, block6d_add\n",
    "    #c4_output = (c4_output + c4a_output[:, :, :, :80])/2\n",
    "    return keras.Model(\n",
    "        inputs=[backbone.inputs], outputs=[c2_output, c3_output, c4_output, c5_output]\n",
    "    )\n",
    "\n",
    "\n",
    "#D0 for layer_name in [\"block2b_add\", \"block3b_add\", \"block5c_add\", \"block6d_add\"]]\n",
    "#D7 for layer_name in [\"block2f_add\", \"block3g_add\", \"block5j_add\", \"block6d_add\"]]\n",
    "#input                           (None, 64, 64, 3)   \n",
    "#block2b_add (Add)               (None, 16, 16, 24) \n",
    "#block3b_add (Add)               (None, 8, 8, 40)    \n",
    "#block4c_add (Add)               (None, 4, 4, 80)\n",
    "#block5c_add (Add)               (None, 4, 4, 112) \n",
    "#block6d_add (Add)               (None, 2, 2, 192)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def get_backbone():\n",
    "    backbone = keras.applications.MobileNetV2(include_top=False, input_shape=[None, None, 3])\n",
    "    c3_output, c4_output, c5_output = [\n",
    "        backbone.get_layer(layer_name).output\n",
    "        for layer_name in [\"block_6_expand_relu\", \"block_13_expand_relu\", \"out_relu\"]]\n",
    "    return keras.Model(\n",
    "        inputs=[backbone.inputs], outputs=[c3_output, c4_output, c5_output]\n",
    "    )\n",
    "backbone = get_backbone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coordinate_map_uv(h, w):\n",
    "    #return (6, 18, 256)\n",
    "    x = tf.range(0.5, w, 1) / tf.cast(w, tf.float32) * 2.0 -1\n",
    "    y = tf.range(0.5, h, 1) / tf.cast(h, tf.float32) * 2.0 -1    \n",
    "    X, Y = tf.meshgrid(x, y)\n",
    "    xy = tf.stack((X, Y), -1)\n",
    "    xy = tf.expand_dims(xy, axis=0)   \n",
    "    return xy \n",
    "\n",
    "def coordinate_map(h, w):\n",
    "    #return (6, 18, 256)    \n",
    "    x = tf.range(0.0, w, 1) / tf.cast(w, tf.float32) * 2.0 -1\n",
    "    y = tf.linspace(0, 1, h)\n",
    "    y = tf.cast(y, tf.float32)\n",
    "    x = tf.zeros_like(x)#finetune\n",
    "    X, Y = tf.meshgrid(x, y)\n",
    "    xy = tf.stack((X, Y), -1)\n",
    "    xy = tf.expand_dims(xy, axis=0)   \n",
    "    return xy \n",
    "\n",
    "def add_map(net):\n",
    "    shape = tf.shape(net)\n",
    "    map_norm = coordinate_map(shape[1], shape[2])        \n",
    "    map_square = tf.sqrt(map_norm)\n",
    "    net = tf.concat((map_norm + net[:, :, :, :2], map_square + net[:, :, :, 2:4], net[:, :, :, 4:]), -1)    \n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.sqrt(coordinate_map(8, 3))[0,:,:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createRetinaNet(num_classes, anchor_k, is_train=False):\n",
    "    prior_probability = tf.constant_initializer(-np.log((1 - 0.01) / 0.01))\n",
    "    inputs = Input(shape=(None, None, 3))   \n",
    "    \n",
    "    backbone = get_backbone()\n",
    "    backbone.trainable = False #finetune\n",
    "    nets_3 = backbone(inputs, training=is_train)            \n",
    "    p456 = BifeaturePyramidNet(nets_3)    \n",
    "    \n",
    "    cls_outputs = []\n",
    "    box_outputs = []\n",
    "    \n",
    "    kernel_init = tf.initializers.he_normal()\n",
    "    regulizer = tf.keras.regularizers.L2(l1)\n",
    "    \n",
    "    filters = 5 + num_classes\n",
    "    conv_h0 = keras.layers.Conv2D(anchor_k * filters, 3, padding=\"same\", kernel_initializer=kernel_init, bias_initializer=prior_probability, kernel_regularizer=regulizer, name='head_0')   \n",
    "    conv_h1 = keras.layers.Conv2D(anchor_k * filters, 3, padding=\"same\", kernel_initializer=kernel_init, bias_initializer=prior_probability, kernel_regularizer=regulizer, name='head_1')   \n",
    "    conv_h2 = keras.layers.Conv2D(anchor_k * filters, 3, padding=\"same\", kernel_initializer=kernel_init, bias_initializer=prior_probability, kernel_regularizer=regulizer, name='head_2')\n",
    "    conv_kernels = [conv_h0, conv_h1, conv_h2]\n",
    "    \n",
    "    drop = keras.layers.Dropout(0.05)\n",
    "    N = tf.shape(nets_3[0])[0]\n",
    "    \n",
    "    cbox_outputs = []    \n",
    "    \n",
    "    for i in range(len(p456)):            \n",
    "        feature = drop(p456[i])        \n",
    "        feature = add_map(feature)\n",
    "        conv_kernel = conv_kernels[i]\n",
    "        cls_out = conv_kernel(feature)        \n",
    "        cbox_out = tf.reshape(cls_out, [N, -1, filters])\n",
    "        cbox_outputs.append(cbox_out[:,:,:])\n",
    "    \n",
    "    outputs = tf.concat(cbox_outputs, axis=1)          \n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _decode_box_predictions(anchor_boxes, box_predictions):\n",
    "    _box_variance = tf.convert_to_tensor([0.1, 0.1, 0.2, 0.2], dtype=tf.float32)\n",
    "    boxes = box_predictions * _box_variance\n",
    "    boxes = tf.concat(\n",
    "        [\n",
    "            boxes[:, :, :2] * anchor_boxes[:, :, 2:] + anchor_boxes[:, :, :2],\n",
    "            tf.math.exp(boxes[:, :, 2:]) * anchor_boxes[:, :, 2:],\n",
    "        ],\n",
    "        axis=-1,\n",
    "    )\n",
    "    boxes_transformed = convert_to_corners(boxes)\n",
    "    return boxes_transformed\n",
    "\n",
    "def decodePredictions(images, predictions, \n",
    "                      num_classes=num_classes,\n",
    "                      confidence_threshold=0.5,\n",
    "                      nms_iou_threshold=0.2,\n",
    "                      max_detections_per_class=1000,\n",
    "                      max_detections=1500,\n",
    "                      box_variance=[0.1, 0.1, 0.2, 0.2]):\n",
    "    \n",
    "    _anchor_box = AnchorBox()\n",
    "        \n",
    "    image_shape = tf.cast(tf.shape(images), dtype=tf.float32)\n",
    "    image_h = padded_image_shape[0]\n",
    "    image_w = padded_image_shape[1]\n",
    "    anchor_boxes = _anchor_box.get_anchors(image_shape[1], image_shape[2])#free size    \n",
    "    #anchor_boxes = _anchor_box.get_anchors(image_h, image_w)\n",
    "    box_predictions = predictions[:, :, :4]\n",
    "    objectness = tf.nn.sigmoid(predictions[:, :, 4:5])\n",
    "    cls_score = predictions[:, :, 5:5+num_classes_real]\n",
    "    cls_prob = tf.nn.softmax(cls_score)\n",
    "    cls_prob_max = tf.reduce_max(cls_prob, -1)\n",
    "    #cls_predictions = tf.round(objectness) * cls_predictions         \n",
    "    #cls_predictions = objectness\n",
    "    cls = tf.argmax(cls_score, -1)\n",
    "    cls = tf.cast(cls, tf.float32)\n",
    "    \n",
    "    boxes = _decode_box_predictions(anchor_boxes[None, ...], box_predictions)\n",
    "    boxes_2d = tf.reshape(boxes, [-1, 4])    \n",
    "    scores = tf.reshape(objectness, [-1, 1])#new\n",
    "    #scores = tf.sqrt(scores * tf.reshape(cls_prob_max, [-1, 1]))#new\n",
    "    cls = tf.reshape(cls, [-1, 1])\n",
    "    ccbox = tf.concat((cls, scores, boxes_2d), -1)\n",
    "    \n",
    "    selected_indices, selected_scores = tf.image.non_max_suppression_with_scores(    \n",
    "        ccbox[:, 2:],\n",
    "        ccbox[:, 1],        \n",
    "        max_detections,\n",
    "        nms_iou_threshold,\n",
    "        confidence_threshold,        \n",
    "    )\n",
    "    output = tf.gather(ccbox, selected_indices)        \n",
    "    return output   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenderLoss(tf.losses.Loss):\n",
    "    \"\"\"Implements Focal loss\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(GenderLoss, self).__init__(\n",
    "            reduction=\"none\", name=\"GenderLoss\"\n",
    "        )\n",
    "        self._alpha = 0.5\n",
    "        self._gamma = 1.0\n",
    "        \n",
    "    def call(self, y_cls, y_pred):\n",
    "        #y : 0:unknown, 1, 2        \n",
    "        y_positive = tf.cast(y_cls > 0, tf.float32)\n",
    "        y_cls = tf.cast(y_cls - 1, dtype=tf.int32)\n",
    "        y_cls = tf.maximum(y_cls, 0)\n",
    "        y_hot = tf.one_hot(y_cls, depth=2, dtype=tf.float32,)\n",
    "        \n",
    "        pt = tf.nn.sigmoid(y_pred)        \n",
    "        pt = tf.clip_by_value(pt, 1e-7, 1.0 - 1e-7)\n",
    "                \n",
    "        loss_p = - (1.0 - self._alpha) * tf.pow(1.0 - pt, self._gamma) * y_positive * tf.math.log(pt)        \n",
    "        loss_f = - self._alpha * tf.pow(pt, self._gamma) * (1 - y_positive) * tf.math.log(1 - pt)\n",
    "        loss_obj = y_positive * (loss_p + loss_f)\n",
    "        return loss_obj\n",
    "\n",
    "class BodyLoss(tf.losses.Loss):\n",
    "    def __init__(self):\n",
    "        super(BodyLoss, self).__init__(reduction=\"none\", name=\"BodyLoss\")\n",
    "        self._alpha = 0.5\n",
    "        self._gamma = 3.0\n",
    "\n",
    "    def call(self, y_cls, y_pred):\n",
    "        #y : 0:unknown, 1:thin, 2:medium, 3:fat         \n",
    "        y_positive = tf.cast(y_cls > 0, tf.float32)\n",
    "        y_cls = tf.cast(y_cls - 1, dtype=tf.int32)\n",
    "        y_cls = tf.maximum(y_cls, 0)\n",
    "        y_hot = tf.one_hot(y_cls, depth=body_volumn_k, dtype=tf.float32,)\n",
    "\n",
    "        pt = tf.nn.softmax(y_pred)        \n",
    "        pt = tf.clip_by_value(pt, 1e-7, 1.0 - 1e-7)\n",
    "\n",
    "        loss_p = - (1.0 - self._alpha) * tf.pow(1.0 - pt, self._gamma) * y_hot * tf.math.log(pt)        \n",
    "        loss_f = - self._alpha * tf.pow(pt, self._gamma) * (1 - y_hot) * tf.math.log(1 - pt)\n",
    "        loss_obj = y_positive * tf.reduce_sum(loss_p + loss_f, -1)\n",
    "        return loss_obj\n",
    "\n",
    "class UpperClothLoss(tf.losses.Loss):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(UpperClothLoss, self).__init__(reduction=\"none\", name=\"UpperClothLoss\")\n",
    "        self._alpha = 0.5\n",
    "        self._gamma = 3.0\n",
    "\n",
    "    def call(self, y_cls, y_pred):\n",
    "        #y : 0:unknown, 1:thin, 2:medium, 3:fat         \n",
    "        y_positive = tf.cast(y_cls > 0, tf.float32)        \n",
    "        y_cls = tf.cast(y_cls - 1, dtype=tf.int32)\n",
    "        y_cls = tf.maximum(y_cls, 0)\n",
    "        y_hot = tf.one_hot(y_cls, depth=upper_cloth_k, dtype=tf.float32,)\n",
    "\n",
    "        pt = tf.nn.softmax(y_pred)        \n",
    "        pt = tf.clip_by_value(pt, 1e-7, 1.0 - 1e-7)\n",
    "\n",
    "        loss_p = - (1.0 - self._alpha) * tf.pow(1.0 - pt, self._gamma) * y_hot * tf.math.log(pt)        \n",
    "        loss_f = - self._alpha * tf.pow(pt, self._gamma) * (1 - y_hot) * tf.math.log(1 - pt)\n",
    "        loss_obj = y_positive * tf.reduce_sum(loss_p + loss_f, -1)\n",
    "        return loss_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetinaNetBoxLoss(tf.losses.Loss):\n",
    "    \"\"\"Implements Smooth L1 loss\"\"\"\n",
    "\n",
    "    def __init__(self, delta):\n",
    "        super(RetinaNetBoxLoss, self).__init__(\n",
    "            reduction=\"none\", name=\"RetinaNetBoxLoss\"\n",
    "        )\n",
    "        self._delta = delta\n",
    "\n",
    "    def call(self, y_true, y_pred):    \n",
    "        \n",
    "        difference = y_true - y_pred\n",
    "        absolute_difference = tf.abs(difference)        \n",
    "        loss = tf.where(\n",
    "            tf.less(absolute_difference, self._delta),\n",
    "            0.5 * (difference ** 2),\n",
    "            absolute_difference - 0.5,\n",
    "        )\n",
    "        loss = tf.where(loss < 0.02, 0.0, loss)#new marginal loss        \n",
    "        return tf.reduce_sum(loss, axis=-1)\n",
    "\n",
    "\n",
    "class RetinaNetClassificationLoss(tf.losses.Loss):\n",
    "    \"\"\"Implements Focal loss\"\"\"\n",
    "\n",
    "    def __init__(self, alpha, gamma, num_classes):\n",
    "        super(RetinaNetClassificationLoss, self).__init__(\n",
    "            reduction=\"none\", name=\"RetinaNetClassificationLoss\"\n",
    "        )\n",
    "        self._alpha = alpha\n",
    "        self._gamma = gamma\n",
    "        self._num_classes = num_classes\n",
    "        print('RetinaNetClassificationLoss', num_classes)\n",
    "        \n",
    "    def call(self, y_cls, y_pred):\n",
    "        y_cls_int = tf.cast(y_cls, dtype=tf.int32)\n",
    "        y_hot = tf.one_hot(y_cls_int, depth=self._num_classes, dtype=tf.float32,)\n",
    "        \n",
    "        y_positive = tf.cast(y_cls > 0, tf.float32)#finetune, 1:unknown\n",
    "        y_positive_identity = tf.cast(y_cls > 1, tf.float32)# 1:unknown\n",
    "        \n",
    "        obj_score = tf.identity(y_pred[:, :, 0], name='obj_score')\n",
    "        cls_score = y_pred[:, :, 1:1+self._num_classes]\n",
    "        h_cls_num = tf.argmax(cls_score, -1)\n",
    "        #objectness = obj_score + tf.reduce_mean(y_pred[:, :, 1:]*0, axis=-1)\n",
    "       \n",
    "        pt = tf.nn.sigmoid(obj_score)        \n",
    "        pt = tf.clip_by_value(pt, 1e-7, 1.0 - 1e-7)\n",
    "                \n",
    "        loss_p = - (1.0 - self._alpha) * tf.pow(1.0 - pt, self._gamma) * y_positive * tf.math.log(pt)        \n",
    "        loss_f = - self._alpha * tf.pow(pt, self._gamma) * (1 - y_positive) * tf.math.log(1 - pt)\n",
    "        loss_obj = loss_p + loss_f \n",
    "      \n",
    "        cls_pt = tf.nn.softmax(cls_score)        \n",
    "        cls_pt = tf.clip_by_value(cls_pt, 1e-7, 1.0 - 1e-7)\n",
    "        loss_cls_p = - tf.pow(1.0 - cls_pt, self._gamma) * y_hot * tf.math.log(cls_pt)\n",
    "        loss_cls_f = - tf.pow(cls_pt, self._gamma) * (1 - y_hot) * tf.math.log(1 - cls_pt)\n",
    "        loss_cls = tf.reduce_sum(loss_cls_p + loss_cls_f, axis=-1)        \n",
    "                \n",
    "        normalizer = tf.reduce_sum(y_positive_identity, axis=-1, keepdims=True)\n",
    "        loss_cls = tf.math.divide_no_nan(loss_cls, normalizer)\n",
    "        #weight_cls = 1.0 + y_positive * (y_cls - 1)/2\n",
    "        #loss = loss_obj + .1 * y_positive * tf.round(pt) * loss_cls#when not stable\n",
    "        y_cls_positive = tf.boolean_mask(y_cls, y_cls > 0)\n",
    "        is_various_cls_exist = tf.cast(tf.math.reduce_std(y_cls_positive) > 0, tf.float32)        \n",
    "                \n",
    "        loss = loss_obj + y_positive * is_various_cls_exist * loss_cls#when not stable\n",
    "        car_dataset_sample = tf.reduce_any(y_cls > 3, axis=1, keepdims=True)\n",
    "        mask_ignore_loss = tf.logical_and(y_cls_int == 0, h_cls_num >= 3)\n",
    "        mask_ignore_loss = tf.logical_and(tf.logical_not(car_dataset_sample), mask_ignore_loss)\n",
    "        loss = tf.where(mask_ignore_loss, 0.0, loss)        \n",
    "        return loss    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_k, body_volumn_k, upper_cloth_k, num_classes_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetinaNetLoss(tf.losses.Loss):\n",
    "    \"\"\"Wrapper to combine both the losses\"\"\"\n",
    "\n",
    "    def __init__(self, num_classes=80, alpha=0.3, gamma=3.0, delta=1.0):#alpha=0.25\n",
    "        super(RetinaNetLoss, self).__init__(reduction=\"auto\", name=\"RetinaNetLoss\")\n",
    "        self._clf_loss = RetinaNetClassificationLoss(alpha, gamma, num_classes_real)\n",
    "        self._box_loss = RetinaNetBoxLoss(delta)\n",
    "        self._gender_loss = GenderLoss()\n",
    "        self._body_loss = BodyLoss()\n",
    "        self._upper_cloth_loss = UpperClothLoss()\n",
    "        self._num_classes = num_classes\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        # y_pred : tf.concat([box_outputs, cls_outputs], axis=-1)\n",
    "        #y_pred = tf.cast(y_pred, dtype=tf.float32)\n",
    "        \n",
    "        y_box = y_true[:, :, :4]\n",
    "        y_cls = y_true[:, :, 4]\n",
    "        y_gender = y_true[:, :, 5]\n",
    "        y_body = y_true[:, :, 6]\n",
    "        y_upper_cloth = y_true[:, :, 7]\n",
    "        \n",
    "        h_box = y_pred[:, :, :4]\n",
    "        h_obj = tf.nn.sigmoid(y_pred[:, :, 4])        \n",
    "        h_cls = y_pred[:, :, 4:4+1+num_classes_real]        \n",
    "        h_gender = y_pred[:, :, -1]#k2\n",
    "        h_body = y_pred[:, :, -body_volumn_k-1:-1]#k3\n",
    "        h_upper_cloth = y_pred[:, :, -upper_cloth_k-body_volumn_k-1:-body_volumn_k-1]\n",
    "      \n",
    "        positive_mask = tf.greater(y_cls, 0.0)\n",
    "        ignore_mask = tf.less(y_cls, 0.0)\n",
    "        \n",
    "        clf_loss = self._clf_loss(y_cls, h_cls)\n",
    "        box_loss = self._box_loss(y_box, h_box) \n",
    "        if use_pedestrian:\n",
    "            gender_loss = self._gender_loss(y_gender, h_gender)\n",
    "            body_loss = self._body_loss(y_body, h_body)\n",
    "            upper_cloth_loss = self._upper_cloth_loss(y_upper_cloth, h_upper_cloth)\n",
    "            pass\n",
    "        \n",
    "        attribute_loss = box_loss\n",
    "        if use_pedestrian:\n",
    "            attribute_loss = box_loss + .1 * (gender_loss + body_loss + upper_cloth_loss)\n",
    "            pass\n",
    "                \n",
    "        clf_loss = tf.where(ignore_mask, 0.0, clf_loss)\n",
    "        #attribute_loss = tf.where(ignore_mask, 0.0, attribute_loss)\n",
    "        attribute_loss = tf.where(positive_mask, attribute_loss, 0.0)\n",
    "                \n",
    "        positive_mask = tf.cast(positive_mask, tf.float32)        \n",
    "        normalizer = tf.sqrt(tf.reduce_sum(positive_mask, axis=-1, keepdims=False))#fine     \n",
    "         \n",
    "        loss = (1 + tf.cast(y_cls > 1, tf.float32)) * (clf_loss + attribute_loss)\n",
    "        loss = tf.math.divide_no_nan(tf.reduce_sum(loss, axis=-1), normalizer)\n",
    "        \n",
    "        return loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(y_true, y_pred):    \n",
    "    y_cls = tf.cast(y_true[:, :, 4], tf.int32)\n",
    "    y_positive = tf.cast(y_cls > 0, tf.int32)\n",
    "    y_bg = tf.cast(tf.abs(y_cls)==0, tf.int32)\n",
    "    h_score = y_pred[:, :, 4]\n",
    "    h_prob = tf.nn.sigmoid(h_score)    \n",
    "    h_postive = tf.cast(tf.round(h_prob), tf.int32)\n",
    "    \n",
    "    true_positives = tf.cast(tf.logical_and(y_cls > 0, h_postive>0), tf.float32)\n",
    "    false_negative = y_positive * (1 - h_postive)\n",
    "                \n",
    "    tp = tf.reduce_sum(true_positives, axis=1)# + 0.01\n",
    "    fn = tf.reduce_sum(false_negative, axis=1)\n",
    "    tp = tf.cast(tp, tf.float32)\n",
    "    fn = tf.cast(fn, tf.float32)\n",
    "    \n",
    "    rec = tp / (tp + fn + 1e-8)\n",
    "    return rec\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    \n",
    "    y_cls_symbol = tf.cast(y_true[:, :, 4], dtype=tf.int32)    \n",
    "    y_cls_symbol = tf.cast(y_cls_symbol != 0, tf.int32)\n",
    "    h_obj_prob = tf.nn.sigmoid(y_pred[:, :, 4])\n",
    "    h_cls_symbol = tf.round(h_obj_prob)    \n",
    "    h_cls_symbol = tf.cast(h_cls_symbol, tf.int32)\n",
    "    \n",
    "    true_positives = y_cls_symbol * h_cls_symbol\n",
    "    false_positive = (1 - y_cls_symbol) * h_cls_symbol\n",
    "    \n",
    "    ones = tf.ones_like(true_positives)\n",
    "    zeeros = tf.zeros_like(true_positives)\n",
    "    true_positives = tf.cast(tf.equal(true_positives, ones), tf.float32)\n",
    "    false_positive = tf.cast(tf.equal(false_positive, ones), tf.float32)\n",
    "    \n",
    "    tp = tf.reduce_sum(true_positives, axis=1)# + 0.01\n",
    "    fp = tf.reduce_sum(false_positive, axis=1)\n",
    "    tp = tf.cast(tp, tf.float32)\n",
    "    fp = tf.cast(fp, tf.float32)\n",
    "    prec = tp / (tp + fp + 1e-8)\n",
    "    return prec\n",
    "\n",
    "def acc(y_true, y_pred):    \n",
    "    y_cls = tf.cast(y_true[:, :, 4], tf.int32)\n",
    "    y_positive = y_cls > 0    \n",
    "    h_cls = tf.math.argmax(y_pred[:, :, 5:5+num_classes_real], -1, output_type=tf.int32)        \n",
    "    acc = tf.boolean_mask(tf.equal(y_cls, h_cls), y_positive)    \n",
    "    #acc = tf.equal(y_cls, h_cls)\n",
    "    return acc\n",
    "\n",
    "def gender_acc(y_true, y_pred):    \n",
    "    y_cls = tf.cast(y_true[:, :, 4], tf.int32)\n",
    "    y_gender = tf.cast(y_true[:, :, 5], tf.int32)\n",
    "    y_positive = tf.logical_and(y_cls > 0, y_gender > 0)\n",
    "    \n",
    "    y_gender = tf.maximum(y_gender - 1, 0)    \n",
    "    h_prob = tf.nn.sigmoid(y_pred[:, :, -1])\n",
    "    h = tf.cast(tf.round(h_prob), tf.int32)    \n",
    "    acc = tf.boolean_mask(tf.equal(y_gender, h), y_positive)        \n",
    "    return acc\n",
    "\n",
    "def body_acc(y_true, y_pred):    \n",
    "    y_cls = tf.cast(y_true[:, :, 4], tf.int32)\n",
    "    y_gender = tf.cast(y_true[:, :, 6], tf.int32)\n",
    "    y_positive = tf.logical_and(y_cls > 0, y_gender > 0)\n",
    "    \n",
    "    y_gender = tf.maximum(y_gender - 1, 0)    \n",
    "    h_prob = tf.nn.softmax(y_pred[:, :, -3-1:-1])\n",
    "    h = tf.cast(tf.argmax(h_prob, -1), tf.int32)    \n",
    "    acc = tf.boolean_mask(tf.equal(y_gender, h), y_positive)        \n",
    "    return acc\n",
    "\n",
    "def up_cloth_acc(y_true, y_pred):    \n",
    "    y_cls = tf.cast(y_true[:, :, 4], tf.int32)\n",
    "    y_gender = tf.cast(y_true[:, :, 7], tf.int32)\n",
    "    y_positive = tf.logical_and(y_cls > 0, y_gender > 0)\n",
    "    \n",
    "    y_gender = tf.maximum(y_gender - 1, 0)\n",
    "    \n",
    "    h_score = y_pred[:, :, -upper_cloth_k-body_volumn_k-1:-body_volumn_k-1]    \n",
    "    h_prob = tf.nn.softmax(h_score)\n",
    "    h = tf.cast(tf.argmax(h_prob, -1), tf.int32)    \n",
    "    acc = tf.boolean_mask(tf.equal(y_gender, h), y_positive)        \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "anchor_k = len(label_encoder._anchor_box.aspect_ratios)*len(label_encoder._anchor_box.scales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bbox_list_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(bbox_list_train), np.concatenate(bbox_list_train, 0).shape\n",
    "cls = np.concatenate(bbox_list_train, 0)[:, 4]\n",
    "print('cls', np.unique(cls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def generator():    \n",
    "    for i in range(0, (len(input_list_train)//2)*2):\n",
    "        x = input_list_train[i]\n",
    "        y_box = bbox_list_train[i]\n",
    "        #box_height = y_box[:, 3] - y_box[:, 1]\n",
    "        #y_box = y_box[box_height > 0]#height:0 box exist\n",
    "        if y_box.shape[1] < label_attribute_k:\n",
    "            z = np.zeros_like(y_box[:, :label_attribute_k - y_box.shape[1]])\n",
    "            y_box = np.concatenate((y_box, z), -1)\n",
    "        yield (x, y_box)\n",
    "\n",
    "def generator_test():    \n",
    "    for i in range(len(input_list_test)):\n",
    "        x = input_list_test[i]\n",
    "        y_box = bbox_list_test[i]\n",
    "        if y_box.shape[1] < label_attribute_k:\n",
    "            z = np.zeros_like(y_box[:, :label_attribute_k - y_box.shape[1]])\n",
    "            y_box = np.concatenate((y_box, z), -1)\n",
    "        yield (x, y_box)\n",
    "\n",
    "dataset = tf.data.Dataset.from_generator(\n",
    "    generator, \n",
    "    output_types=(tf.uint8, tf.float32), \n",
    "    output_shapes=(tf.TensorShape([None, None, 3]), tf.TensorShape([None, label_attribute_k])))\n",
    "dataset_test = tf.data.Dataset.from_generator(\n",
    "    generator_test, \n",
    "    output_types=(tf.uint8, tf.float32), \n",
    "    output_shapes=(tf.TensorShape([None, None, 3]), tf.TensorShape([None, label_attribute_k])))\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "for example in tfds.as_numpy(dataset):\n",
    "    image = example[0]\n",
    "    bbox = example[1]\n",
    "    print(image.dtype, image.shape, bbox.shape, bbox[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2*4 #finetune 1 or 4\n",
    "autotune = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_dataset = dataset.map(preprocess_data, num_parallel_calls=autotune)\n",
    "#train_dataset = train_dataset.shuffle(8 * batch_size)\n",
    "train_dataset = train_dataset.padded_batch(batch_size=batch_size, padding_values=(0.0, 1e-8, -1), drop_remainder=False)\n",
    "#train_dataset = train_dataset.padded_batch(batch_size=batch_size)\n",
    "train_dataset = train_dataset.map(label_encoder.encode_batch_train, num_parallel_calls=autotune)\n",
    "train_dataset = train_dataset.prefetch(autotune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = dataset_test.map(preprocess_test_data, num_parallel_calls=autotune)\n",
    "val_dataset = val_dataset.padded_batch(batch_size=batch_size, padding_values=(0.0, 1e-8, -1), drop_remainder=False)\n",
    "#val_dataset = val_dataset.padded_batch(batch_size=batch_size)\n",
    "val_dataset = val_dataset.map(label_encoder.encode_batch, num_parallel_calls=autotune)\n",
    "val_dataset = val_dataset.prefetch(autotune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=3, linewidth=200)\n",
    "image_height, image_width = padded_image_shape\n",
    "\n",
    "img_check = 0\n",
    "for image, output_map in train_dataset:\n",
    "    print('output_map', output_map.shape)\n",
    "    cbbox = output_map    \n",
    "    bbox = cbbox[:, :, :4]\n",
    "    cls_gt = cbbox[:,:,4]\n",
    "    img_m, image_height, image_width, image_ch = image.shape\n",
    "    anchor_feature_size = [(np.ceil(image_height / 2 ** i), np.ceil(image_width / 2 ** i)) \n",
    "                           for i in range(level_start, level_end)]\n",
    "    print('anchor_feature_size', anchor_feature_size)    \n",
    "    m = len(cbbox)    \n",
    "    positive_count = np.sum(cls_gt>0)\n",
    "    print('cbbox', cbbox.shape)\n",
    "    print('cls_sum',np.sum(cls_gt < 0.0), np.sum(cls_gt == 0.0), \n",
    "          np.sum(cls_gt == 1.0), np.sum(cls_gt > 1.0))\n",
    "    print('cls_mean',np.mean(cls_gt < 0.0), np.mean(cls_gt == 0.0), \n",
    "          np.mean(cls_gt == 1.0), np.mean(cls_gt > 0.0))\n",
    "    print('shape',image.shape, cbbox.shape,'unique', np.unique(cls_gt))\n",
    "    print('anchor_feature_size', anchor_feature_size)\n",
    "    offset = 0\n",
    "    positive_maps = []\n",
    "    for anchor_feature_size_1 in anchor_feature_size:        \n",
    "        fm_h, fm_w = anchor_feature_size_1\n",
    "        fm_h = int(fm_h)\n",
    "        fm_w = int(fm_w)        \n",
    "        fm_wh = int(fm_h * fm_w * anchor_k)\n",
    "        cbbox_anchor = cbbox[:, offset:offset+fm_wh, 4]\n",
    "        cbbox_anchor = np.reshape(cbbox_anchor, [m, fm_h, fm_w, anchor_k])\n",
    "        coount_m1 = np.count_nonzero(cbbox_anchor==-1)\n",
    "        coount_0 = np.count_nonzero(cbbox_anchor==0)\n",
    "        coount_1 = np.count_nonzero(cbbox_anchor==1)\n",
    "        coount_1_over = np.count_nonzero(cbbox_anchor>1)\n",
    "        positive_ratio = np.mean(cbbox_anchor>0)\n",
    "        positive_maps.append(cbbox_anchor>0)\n",
    "        print('cbbox_anchor', cbbox_anchor.shape, coount_m1, coount_0, coount_1, coount_1_over, 'ratio', positive_ratio)\n",
    "        sample_0_cbbox = cbbox_anchor[0]\n",
    "        sample_0_cbbox_sum = np.max(sample_0_cbbox, -1).astype(np.int)       \n",
    "      \n",
    "        offset += fm_wh\n",
    "        if False:            \n",
    "            file_name = str(fm_h)+ '_' + str(fm_w)+ '.txt'\n",
    "            np.savetxt(file_name,sample_0_cbbox_sum, fmt='%d',delimiter='')\n",
    "    img_check = image\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(np.max(positive_maps[0][0], -1))\n",
    "plt.title(str(positive_maps[0].shape)+ str(np.mean(positive_maps[0][0]))+ ' ' + str(np.sum(positive_maps[0][0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmap0 = np.array(Image.fromarray(np.max(positive_maps[0][0],-1)).resize((image_width, image_height)))\n",
    "pmap1 = np.array(Image.fromarray(np.max(positive_maps[1][0],-1)).resize((image_width, image_height)))\n",
    "pmap2 = np.array(Image.fromarray(np.max(positive_maps[2][0],-1)).resize((image_width, image_height)))\n",
    "#pmap3 = np.array(Image.fromarray(np.max(positive_maps[3][0],-1)).resize((image_width, image_height)))\n",
    "#pmap4 = np.array(Image.fromarray(np.max(positive_maps[4][0],-1)).resize((image_width, image_height)))\n",
    "pmap0 = pmap0.astype(np.uint8)\n",
    "pmap1 = pmap1.astype(np.uint8)\n",
    "pmap2 = pmap2.astype(np.uint8)\n",
    "pmap3 = 0#pmap3.astype(np.uint8)\n",
    "pmap4 = 0#pmap4.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pmap_with_img = np.array(img_check)[0]#*255\n",
    "pmap_with_img = pmap_with_img.astype(np.uint8)\n",
    "pmap_add = np.expand_dims(pmap0+pmap1+pmap2+pmap3+pmap4, -1)\n",
    "pmap = (pmap_add>0).astype(np.uint8)*255\n",
    "mix_rgb = np.concatenate((pmap, pmap_with_img[:,:,1:]),-1)\n",
    "plt.figure(figsize=(14,14))\n",
    "plt.imshow(mix_rgb)\n",
    "plt.title(str(np.mean(pmap_add)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_weight():   \n",
    "    weights_dir = path_weight#\"data\"\n",
    "    #latest_checkpoint = tf.train.latest_checkpoint(weights_dir)\n",
    "    latest_checkpoint = weights_dir \n",
    "    print('latest_checkpoint', latest_checkpoint)\n",
    "    model.load_weights(weights_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes, num_classes_real, len(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = tf.distribute.MirroredStrategy()\n",
    "#strategy = tf.distribute.OneDeviceStrategy(device='/gpu:0')\n",
    "print(\"Number of devices: {}\".format(strategy.num_replicas_in_sync))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes, use_pedestrian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#optimizer = tf.optimizers.SGD(learning_rate=1e-5, momentum=0.1, clipvalue=5.)#warm up clipvalue=10. !\n",
    "\n",
    "with strategy.scope():\n",
    "    optimizer = tf.optimizers.SGD(learning_rate=1e-3, momentum=0.01)\n",
    "    loss_detect = RetinaNetLoss(num_classes)\n",
    "    model = createRetinaNet(num_classes, anchor_k)\n",
    "    metrics = [recall, precision, acc]#, gender_acc, body_acc, up_cloth_acc]\n",
    "    if use_pedestrian:\n",
    "        metrics = [recall, precision, acc, gender_acc, body_acc, up_cloth_acc]\n",
    "    model.compile(loss=loss_detect, optimizer=optimizer, metrics=metrics)#[recall, precision, accuracy]\n",
    "    callbacks_list = [\n",
    "        tf.keras.callbacks.ModelCheckpoint(\n",
    "            filepath=path_weight,\n",
    "            monitor=\"loss\",\n",
    "            save_best_only=False,\n",
    "            save_weights_only=True,\n",
    "            verbose=0,\n",
    "            save_freq=200\n",
    "        )\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()#5,743,728"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    load_weight()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(input_list_train), len(input_list_test))#990 2857, 1170 3128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "resnet-34  81ms/step, resnet-24  53ms/step\n",
    "efficientB0  :      63ms/step - loss: 1.5916 - recall: 0.9155 - precision: 0.9330 - accuracy: 0.8751\n",
    "eff-D7 Freeze:     182ms/step - loss: 2.1849 - recall: 0.9470 - precision: 0.9660 - accuracy: 0.9055 - flip_accuracy: 0.0052\n",
    "eff-D2 finetu:  74s 74ms/step - loss: 4.5865 - recall: 0.8719 - precision: 0.8843 - acc: 0.9773\n",
    "eff-D3 finetu:  74s 74ms/step - loss: 1.5638 - recall: 0.8632 - precision: 0.8382 - acc: 0.8657\n",
    "        7s 75ms/step - loss: 3.3875 - recall: 0.8686 - precision: 0.8915 - acc: 0.8226\n",
    "       75s 75ms/step - loss: 1.8621 - recall: 0.9035 - precision: 0.8679 - acc: 0.9020\n",
    "       76s 76ms/step - loss: 1.8896 - recall: 0.9130 - precision: 0.8177 - acc: 0.9055\n",
    "       76s 76ms/step - loss: 1.9485 - recall: 0.9110 - precision: 0.7519 - acc: 0.9157 - gender_acc: 0.0000e+00 - body_acc: 0.0000e+00 - up_cloth_acc: 0.0000e+00\n",
    "       75s 75ms/step - loss: 1.6430 - recall: 0.9019 - precision: 0.8985 - acc: 0.9158\n",
    "''' \n",
    "out = model.evaluate(val_dataset.take(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "hist = model.fit(train_dataset, epochs=epochs,  callbacks=callbacks_list, verbose=1)\n",
    "'''\n",
    "effD2 Freeze:32s 474ms/step - loss: 14.7472 - recall: 0.4458 - precision: 0.6697 - accuracy: 0.0049\n",
    "effD2 fine :269s 544ms/step - loss: 9.3350 - recall: 0.9176 - precision: 0.9638 - acc: 0.9910\n",
    "effD3 Freeze : 171s 546ms/step - loss:15.9819 - recall: 0.8651 - precision: 0.6972 - acc: 0.9518 - gender_acc: 0.5927 - body_acc: 0.5747 - up_cloth_acc: 0.3175\n",
    "        fine : 110s 565ms/step - loss: 4.3338 - recall: 0.8358 - precision: 0.8073 - acc: 0.8943               \n",
    "                267s 558ms/step - loss: 8.0857 - recall: 0.9356 - precision: 0.9513 - acc: 0.9390 - gender_acc: 0.6069 - body_acc: 0.5565 - up_cloth_acc: 0.3009\n",
    "                1239s 521ms/step - loss: 2.2392 - recall: 0.8902 - precision: 0.9115 - acc: 0.9087\n",
    "effD4 Freeze : 123s 796ms/step - loss: 53.2804 - recall: 0.5714 - precision: 0.6422 - acc: 0.4137 - gender_acc: 0.5843 - body_acc: 0.5230 - up_cloth_acc: 0.2131\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(path_weight)\n",
    "path_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#image = tf.keras.Input(shape=[padded_image_shape[0], padded_image_shape[1], 3], name=\"image\")\n",
    "image = tf.keras.Input(shape=[None, None, 3], name=\"image\")\n",
    "predictions = model(image, training=False)\n",
    "\n",
    "detections = decodePredictions(image, predictions, confidence_threshold=0.5, nms_iou_threshold=0.2)\n",
    "inference_model = tf.keras.Model(inputs=image, outputs=detections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def visualize_detections(\n",
    "    image, boxes, classes, scores, figsize=(12, 10), linewidth=1, color=[0, 0, 1], \n",
    "    boxes_gt=None):\n",
    "    \n",
    "    image = np.array(image, dtype=np.uint8)\n",
    "    image = attach_crop_image(image, boxes, max_crop=200)        \n",
    "    \n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(image)\n",
    "    ax = plt.gca()        \n",
    "   \n",
    "    if boxes_gt is not None:\n",
    "        for box in boxes_gt:        \n",
    "            x1, y1, x2, y2 = box\n",
    "            w, h = x2 - x1, y2 - y1\n",
    "            patch = plt.Rectangle(\n",
    "                [x1, y1], w, h, fill=False, edgecolor=[0,1,0], linewidth=2\n",
    "            )\n",
    "            ax.add_patch(patch)\n",
    "            \n",
    "    for box, cls, score in zip(boxes, classes, scores):        \n",
    "        x1, y1, x2, y2 = box\n",
    "        w, h = x2 - x1, y2 - y1\n",
    "        \n",
    "        if cls == 3:\n",
    "            color = [1, 0, 0]\n",
    "        elif cls == 1:\n",
    "            color = [1, 1, 1]\n",
    "        else:\n",
    "            color = [0, 0, 1]\n",
    "            \n",
    "        color_text = color#edgecolors[cls]\n",
    "        \n",
    "        patch = plt.Rectangle(\n",
    "            [x1, y1], w, h, fill=False, edgecolor=color, linewidth=linewidth\n",
    "        )\n",
    "        ax.add_patch(patch)\n",
    "        if len(boxes) < 100:\n",
    "            score_txt = str.format('(%d)%.2f' %(cls, score))\n",
    "            ax.text(x1, y1, score_txt, bbox={\"facecolor\": color_text, \"alpha\": 0.4}, clip_box=ax.clipbox, clip_on=True,)\n",
    "          \n",
    "    plt.show()\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_test(test_datas, bboxs_label, step=1):\n",
    "    i = 0\n",
    "    for image, cbbox in test_datas: \n",
    "        if i%step==0:            \n",
    "            bbox_annotation = bboxs_label[i]\n",
    "            scale = np.array(image.shape[:2])[::-1]\n",
    "            scale = np.reshape(scale, [1, 2])\n",
    "            scale = np.concatenate((scale, scale), 1)\n",
    "            gt_bbox = bbox_annotation[ :, :4] * scale\n",
    "               \n",
    "            input_image, _, ratio = resize_and_pad_image(image, jitter=None)\n",
    "            input_image = tf.expand_dims(input_image, axis=0)\n",
    "            #input_image = tf.cast(input_image, tf.uint8)\n",
    "            detected_box = inference_model.predict(input_image)\n",
    "                        \n",
    "            print(input_image.shape, 'detected_box', detected_box.shape, scale, 'ratio',ratio)\n",
    "            #(1, 1024, 1920, 3) detected_box (7, 6) [[1920 1080 1920 1080]] ratio tf.Tensor(0.94814813, shape=(), dtype=float32)\n",
    "            cls_h = detected_box[:, 0].astype(np.int)\n",
    "            scores = detected_box[:, 1]\n",
    "            box = detected_box[:, 2:] / ratio\n",
    "                        \n",
    "            visualize_detections(\n",
    "                image,\n",
    "                box,\n",
    "                cls_h,\n",
    "                scores,\n",
    "                boxes_gt=gt_bbox\n",
    "            )\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "check_test(dataset_test, bbox_list_test, step=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#random indexing for train\n",
    "check_test(dataset, bbox_list_train, step=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_wrong(X, Y, annot):\n",
    "    i=0\n",
    "    for ann in annot:\n",
    "        image, y = X[i], Y[i]\n",
    "        #print('ann', ann)            \n",
    "        i+=1\n",
    "        \n",
    "        scale = np.array(image.shape[:2])[::-1]\n",
    "        scale = np.reshape(scale, [1, 2])\n",
    "        scale = np.concatenate((scale, scale), 1)\n",
    "        gt_bbox = y[ :, :4] * scale\n",
    "        #gt_bbox = y[:, 1:1+4]\n",
    "        input_image, _, ratio = resize_and_pad_image(image, jitter=None)\n",
    "        input_image = tf.expand_dims(input_image, axis=0)\n",
    "        #input_image = tf.cast(input_image, tf.uint8)\n",
    "        detected_box = inference_model.predict(input_image)\n",
    "        if len(detected_box)>len(y):\n",
    "            print('ann', ann)\n",
    "            print('detected_box', detected_box.shape, 'gt', len(y))\n",
    "            #(1, 1024, 1920, 3) detected_box (7, 6) [[1920 1080 1920 1080]] ratio tf.Tensor(0.94814813, shape=(), dtype=float32)\n",
    "            cls_h = detected_box[:, 0].astype(np.int)\n",
    "            scores = detected_box[:, 1]\n",
    "            box = detected_box[:, 2:] / ratio\n",
    "\n",
    "            visualize_detections(\n",
    "                image,\n",
    "                box,\n",
    "                cls_h,\n",
    "                scores,\n",
    "                boxes_gt=gt_bbox\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "check_wrong(input_list, bbox_list, annotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_pad_32x_arr(image_arr):\n",
    "    #print('image_arr', image_arr.shape, image_arr.dtype)\n",
    "    stride = 32\n",
    "    img_h = image_arr.shape[0]\n",
    "    img_w = image_arr.shape[1]\n",
    "    img_c = image_arr.shape[2]\n",
    "    #print('add_pad_32x', image_arr.shape, img_h, img_w)\n",
    "    pad_h = (stride - (img_h % stride)) % stride\n",
    "    pad_w = (stride - (img_w % stride)) % stride\n",
    "    padded_h = img_h + pad_h\n",
    "    padded_w = img_w + pad_w\n",
    "    #print('pad_h', pad_h, 'pad_w', pad_w)\n",
    "    image_padded = np.zeros((padded_h, padded_w, img_c), dtype=np.uint8)\n",
    "    image_padded[:img_h, :img_w] = image_arr\n",
    "        \n",
    "    return image_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_bg(image, is_display=True, is_save=False, save_path=''):\n",
    "    scale = np.array(image.shape[:2])[::-1]\n",
    "    scale = np.reshape(scale, [1, 2])\n",
    "    scale = np.concatenate((scale, scale), 1)\n",
    "\n",
    "    #input_image, _, ratio = resize_and_pad_image(image, jitter=None)\n",
    "    input_image = add_pad_32x_arr(image)     \n",
    "    ratio = 1\n",
    "    input_image = tf.expand_dims(input_image, axis=0)\n",
    "\n",
    "    detected_box = inference_model.predict(input_image)        \n",
    "    if len(detected_box) > 0:\n",
    "        #print(input_image.shape, 'detected_box', detected_box.shape, scale, 'ratio',ratio)\n",
    "        #(1, 1024, 1920, 3) detected_box (7, 6) [[1920 1080 1920 1080]] ratio tf.Tensor(0.94814813, shape=(), dtype=float32)\n",
    "        if is_display:\n",
    "            cls_h = detected_box[:, 0].astype(np.int)\n",
    "            scores = detected_box[:, 1]\n",
    "            box = detected_box[:, 2:] / ratio\n",
    "\n",
    "            visualize_detections(\n",
    "                image,\n",
    "                box,\n",
    "                cls_h,\n",
    "                scores                \n",
    "            )\n",
    "        if is_save:\n",
    "            img = Image.fromarray(image)\n",
    "            img.save(save_path +'.jpg')\n",
    "        else:\n",
    "            pass\n",
    "    return int(len(detected_box)>0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_jpg = glob(folder_water_bg + '*')\n",
    "print(len(list_jpg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong = 0\n",
    "for i in range(0, len(list_jpg)//1):    \n",
    "    jpg = list_jpg[i]\n",
    "    #print('i', i, jpg.split(os.sep)[-1])\n",
    "    img = Image.open(jpg)\n",
    "    arr = np.array(img)\n",
    "    \n",
    "    n = check_bg(arr, is_display=True)\n",
    "    if n>0:\n",
    "        wrong += n\n",
    "        print(wrong, i, jpg)#414/815, 348/544, 6/242, 43/1522, 59/2043, 64/2375"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## video inference, save result\n",
    "1. split image from video\n",
    "1. get target image paths\n",
    "1. inference, save result to list\n",
    "1. save as csv format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import time\n",
    "def video_to_frames(input_loc, stride=1):\n",
    "    \"\"\"Function to extract frames from input video file\n",
    "    and save them as separate frames in an output directory.\n",
    "    Args:\n",
    "        input_loc: Input video file.\n",
    "        output_loc: Output directory to save the frames.\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "        \n",
    "    folder_split = input_loc.split(os.sep)\n",
    "    file_name = folder_split[-1]\n",
    "    file_name = file_name.split('.')[0]\n",
    "    output_loc = os.sep.join(folder_split[:-1]) + os.sep + file_name + os.sep    \n",
    "    print('output_loc', output_loc)\n",
    "        \n",
    "    try:\n",
    "        os.mkdir(output_loc)\n",
    "    except OSError:\n",
    "        pass\n",
    "    # Log the time\n",
    "    time_start = time.time()\n",
    "    # Start capturing the feed\n",
    "    cap = cv2.VideoCapture(input_loc)\n",
    "    # Find the number of frames\n",
    "    \n",
    "    video_length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) - 1\n",
    "    print (\"Number of frames: \", video_length)\n",
    "    count = 0\n",
    "    save_count = 0\n",
    "    print (\"Converting video..\\n\")\n",
    "    # Start converting the video\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        # Extract the frame\n",
    "        ret, frame = cap.read()\n",
    "        count = count + 1        \n",
    "        file_name = \"%#05d.jpg\" % (count+1)        \n",
    "        dst_name = output_loc + file_name\n",
    "        \n",
    "        try:\n",
    "            cv2.imwrite(dst_name, frame)\n",
    "            #b, g, r = np.split(frame, 3, -1)\n",
    "            #rgb = np.concatenate((r,g,b), -1)        \n",
    "            save_count += 1\n",
    "        except:\n",
    "            print('except', cap, ret, frame, dst_name)\n",
    "            break\n",
    "        \n",
    "        # If there are no more frames left\n",
    "        if count%100==0:\n",
    "            print('count', video_length, count, save_count)\n",
    "       \n",
    "    time_end = time.time()\n",
    "    # Release the feed\n",
    "    cap.release()\n",
    "    # Print stats\n",
    "    print (\"Done extracting frames.\\n%d frames extracted\" % count)\n",
    "    print (\"It took %d seconds to save %d for conversion .\" % (time_end-time_start, save_count))\n",
    "    return output_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_avi = '/home/mvlab/Videos/미수_스마트시티_통영대교_미수방향1(Ch 01)_[20201217]140000-[20201217]141000(20201217_140001).avi'\n",
    "os.path.isfile(path_avi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output_loc = video_to_frames(path_avi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#                                미수_스마트시티_통영대교_미수방향1(Ch 01)_[20201217]135000-[20201217]140000(20201217_135001)\n",
    "output_loc = '/home/mvlab/Videos/미수방향1_2/'\n",
    "os.path.isdir(output_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#paths_test_img = glob('/media/mvlab/46FA9CA2FA9C8FB3/dataset/pedestrian/Tongyoung/event/*미수방향1*/*.jpg')\n",
    "paths_test_img = glob(output_loc + '*')\n",
    "print(len(paths_test_img), output_loc)\n",
    "paths_test_img = np.sort(paths_test_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_label_list_to_df(list_output):\n",
    "    print('convert_label_list_to_df')\n",
    "    df = pd.DataFrame(columns=['path', 'cls', 'x0', 'y0', 'x1', 'y1', 'confidence'])\n",
    "    \n",
    "    ratio = 0.9481481481481482\n",
    "    for output in list_output:\n",
    "        path_img = output[0]\n",
    "        detected_box = output[1]\n",
    "\n",
    "        for box in detected_box:\n",
    "            cls_h = box[0].astype(np.int)\n",
    "            scores = (box[1]*100).astype(np.int)\n",
    "            box_coord = box[2:2+6]/ratio\n",
    "            x0 = box_coord[0].astype(np.int)\n",
    "            y0 = box_coord[1].astype(np.int)\n",
    "            x1 = box_coord[2].astype(np.int)\n",
    "            y1 = box_coord[3].astype(np.int)\n",
    "            row = {'path':path_img, 'cls':cls_h, 'x0':x0, 'y0':y0, 'x1':x1, 'y1':y1, 'confidence':scores}\n",
    "            df = df.append(row, ignore_index=True)\n",
    "            \n",
    "    return df\n",
    "\n",
    "def save_inference_result(paths_img):\n",
    "    \n",
    "    print('len', len(paths_img))\n",
    "    \n",
    "    list_output = []\n",
    "    for i in range(len(paths_img)):\n",
    "        path_img = paths_img[i]\n",
    "        if i % 100 == 0:\n",
    "            print(len(paths_img), i, path_img.split(os.sep)[-1])\n",
    "        \n",
    "        img = Image.open(path_img)\n",
    "        img_arr = np.array(img)\n",
    "        input_image, _, ratio = resize_and_pad_image(img_arr, jitter=None)\n",
    "        input_image = tf.expand_dims(input_image, axis=0)        \n",
    "        detected_box = inference_model.predict(input_image)\n",
    "        \n",
    "        if len(detected_box) > 0:            \n",
    "            list_output.append([path_img, detected_box])\n",
    "        \n",
    "        if len(list_output) > 0 and len(list_output)%1000==0:\n",
    "            df = convert_label_list_to_df(list_output)            \n",
    "            df.to_csv('predict_temp.csv')\n",
    "    \n",
    "    df = convert_label_list_to_df(list_output)            \n",
    "    df.to_csv('predict.csv')\n",
    "    print('finished')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_inference_result(paths_test_img[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Freeze Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Convert Keras model to ConcreteFunction\n",
    "full_model = tf.function(lambda x: inference_model(x))\n",
    "full_model = full_model.get_concrete_function(\n",
    "    x=tf.TensorSpec(inference_model.inputs[0].shape, inference_model.inputs[0].dtype))\n",
    "\n",
    "# Get frozen ConcreteFunction\n",
    "frozen_func = convert_variables_to_constants_v2(full_model)\n",
    "frozen_func.graph.as_graph_def()\n",
    "\n",
    "layers = [op.name for op in frozen_func.graph.get_operations()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(type(frozen_func.outputs))\n",
    "frozen_list = frozen_func.outputs\n",
    "print(frozen_list)\n",
    "print(len(frozen_list))\n",
    "\n",
    "print(type(frozen_func.inputs))\n",
    "frozen_list = frozen_func.inputs\n",
    "print(frozen_list)\n",
    "print(len(frozen_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.io.write_graph(graph_or_graph_def=frozen_func.graph,\n",
    "                  logdir=\"./frozen_models\",\n",
    "                  name=\"pedestrian_efficientDet-D2_frozen_graph.pb\",\n",
    "                  as_text=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_error(X, Y, Path, step=1):\n",
    "    \n",
    "    for i in range(len(X)): \n",
    "        image = X[i]\n",
    "        bbox_annotation = Y[i]\n",
    "        path = Path[i]\n",
    "        \n",
    "        scale = np.array(image.shape[:2])[::-1]\n",
    "        scale = np.reshape(scale, [1, 2])\n",
    "        scale = np.concatenate((scale, scale), 1)\n",
    "        gt_bbox = bbox_annotation[:, 1:] * scale\n",
    "\n",
    "        input_image, _, ratio = resize_and_pad_image(image, jitter=None)\n",
    "        input_image = tf.expand_dims(input_image, axis=0)\n",
    "\n",
    "        detected_box = inference_model.predict(input_image)\n",
    "        detect_k = len(detected_box)\n",
    "        if detect_k!= len(bbox_annotation):\n",
    "            print(path, input_image.shape, 'detected_box', detected_box.shape, scale, 'ratio',ratio.numpy())\n",
    "            #(1, 1024, 1920, 3) detected_box (7, 6) [[1920 1080 1920 1080]] ratio tf.Tensor(0.94814813, shape=(), dtype=float32)\n",
    "            cls_h = detected_box[:, 0].astype(np.int)\n",
    "            scores = detected_box[:, 1]\n",
    "            box = detected_box[:, 2:] / ratio\n",
    "            #print('box', box)\n",
    "\n",
    "            visualize_detections(\n",
    "                image,\n",
    "                box,\n",
    "                cls_h,\n",
    "                scores,\n",
    "                boxes_gt=gt_bbox\n",
    "            )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 1000\n",
    "end = start + 100\n",
    "check_error(input_list[start:end], bbox_list[start:end], path_list[start:end])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load model from .pb file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model_pb = './pedestrian_efficientDet-D2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_model.save(saved_model_pb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_objects = {'recall':recall,'precision':precision}\n",
    "model_loaded = keras.models.load_model(saved_model_pb, custom_objects=custom_objects, compile=False)\n",
    "#model_loaded.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_loaded.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for image, cbbox in val_dataset: \n",
    "    \n",
    "    detected_box = model_loaded.predict(image)\n",
    "    print('detected_box', detected_box.shape)\n",
    "    if len(detected_box) > 0:\n",
    "        cls_h = detected_box[:, 0].astype(np.int)\n",
    "        scores = detected_box[:, 1]\n",
    "        box = detected_box[:, 2:]\n",
    "\n",
    "        visualize_detections(\n",
    "            image[0],\n",
    "            box,\n",
    "            cls_h,\n",
    "            scores\n",
    "        )    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_pb)\n",
    "#converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tflite_model = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the TF Lite model.\n",
    "with tf.io.gfile.GFile('model.tflite', 'wb') as f:\n",
    "  f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls *.tflite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Convert the model.\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(inference_model)\n",
    "tflite_model = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the TF Lite model.\n",
    "with tf.io.gfile.GFile('model.tflite', 'wb') as f:\n",
    "  f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load TFLite model and allocate tensors.\n",
    "interpreter = tf.lite.Interpreter(model_content=tflite_model)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensors.\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Test the TensorFlow Lite model on random input data.\n",
    "input_shape = input_details[0]['shape']\n",
    "input_details, output_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = input_list_train[0]\n",
    "input_data = (np.expand_dims(input_data, 0)/255).astype(np.float32)\n",
    "input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = input_data[:, :padded_image_shape[0], :padded_image_shape[1]]\n",
    "input_data.shape, np.max(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)\n",
    "print('input_data', input_data.shape)\n",
    "interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "\n",
    "interpreter.invoke()\n",
    "tflite_results = interpreter.get_tensor(output_details[0]['index'])\n",
    "tflite_results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tflite_results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detected_box = tflite_results\n",
    "cls_h = detected_box[:, 0].astype(np.int)\n",
    "scores = detected_box[:, 1]\n",
    "box = detected_box[:, 2:] / ratio\n",
    "#print('box', box)\n",
    "\n",
    "visualize_detections(\n",
    "    input_data[0]*255,\n",
    "    box,\n",
    "    cls_h,\n",
    "    scores,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glob('/home/mvlab/Downloads/dataset/통영/label_noise/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
